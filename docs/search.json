[
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Module 3 - R Coding Exercise\n\nDone by: Shaun van den Hurk\nGetting started and set up with packages and retrieving dataset.\n\n#Installing required packages (\"dslabs\", \"tidyverse\")\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\")) #Attempting to address issue with downloading dslabs script at render\ninstall.packages(\"dslabs\") #for dataset\n\nInstalling package into 'C:/Users/shaun/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'dslabs' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\shaun\\AppData\\Local\\Temp\\Rtmp0sfl5M\\downloaded_packages\n\ninstall.packages(\"tidyverse\") #for tools for analysis\n\nInstalling package into 'C:/Users/shaun/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'tidyverse' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\shaun\\AppData\\Local\\Temp\\Rtmp0sfl5M\\downloaded_packages\n\n#load required libraries (\"dslabs\", \"tidyverse\", \"ggplot2\", \"here\")\nlibrary(\"dslabs\")\n\nWarning: package 'dslabs' was built under R version 4.4.2\n\nlibrary(\"tidyverse\")\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"ggplot2\") #for generated plots\n\n#Getting an overview of the data\n#Look at the gapminder data help file help(gapminder)\n\n#Get an overview of the gapminder data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#Get a summary of the gapminder data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n\n#Determine the type of the object gapminder\nclass(gapminder)\n\n\n#Getting started with data processing\n#Filter out data from African countries by filtering by continent\n\n#Filter out African data\nafricadata &lt;- gapminder |&gt; filter(continent ==\"Africa\")\n#View the newly filtered data\nView(africadata)\n\n#From the filtered Africa data, create two new objects, one with the two columns “infant_mortality” and “life_expectancy”, and second object with the two columns “population” and “life_expectancy”.\n#Creating the first object named “africa_1” and looking at the overview and summary of the newly created object\n\n#Using the \"select\" function to select the columns from the previous \"africadata\" dataset\nafrica_1 &lt;- africadata |&gt; select(infant_mortality, life_expectancy)\n#get an overview of the new africa_1 datset\nstr(africa_1)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\n#see a summary of the new africa_1 dataset\nsummary(africa_1)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n#View the resulting object more closely\nview(africa_1)\n\n#Creating the second object named “africa_2” with the columns “population” and “life_expectancy” following the same approach as before\n\n#Using the \"select\" function to select the columns from the previous \"africadata\" dataset\nafrica_2 &lt;- africadata |&gt; select(population, life_expectancy)\n#get an overview of the new africa_1 datset\nstr(africa_2)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n#see a summary of the new africa_1 dataset\nsummary(africa_2)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n#View the resulting object more closely\nview(africa_2)\n\n#Create two different plots to evaluate the data from the objects arica_1 and africa_2 using the ggplot2 package - previously loaded.\n#Generate a plot demonstrating life expectancy as a function of infant mortality.\n\n#Generate the plot as a scatter plot with the variables associated with the approiate axes, reduce point size to 1, set colour to blue, label different areas of the graph\nggplot(africa_1, aes(x = infant_mortality, y = life_expectancy)) +   \n  geom_point(size = 1, color = \"blue\") +\n  labs(title = \"Infant mortality vs Life expectancy\", x = \"Infant mortality\", y = \"Life expectancy\" ) \n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n#Save the generated plot\nggsave(\"africa_1_Life_expectancy_vs_infant_mortality.png\", width = 8, height =6)\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nThe data shows shows a negative correlation between infant mortality and life expectancy. This is intuitive.\n#Generate a second plot demonstrating life expectancy as a function of population size, where the x-axis (population) is plotted on a log scale\n\n#Generate the plot as a scatter plot with the variables associated with the approiate axes,set the x-axis (population) to be in the log-scale, reduce point size to 1, set colour to red, label different areas of the graph\nggplot(africa_2, aes(x = population, y = life_expectancy)) +   \n  geom_point(size = 1, color = \"red\") +\n  scale_x_log10() +\n  labs(title = \"Population vs Life expectancy\", x = \"Population (Log-scale)\", y = \"Life expectancy\" ) \n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n#Save the generated plot\nggsave(\"africa_2_Life_expectancy_vs_population_log_scale.png\", width = 8, height =6)\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nThe plots generated are a bit messy and difficult to interpret as they currently are. The strange appearance is most likely a result of the repition of the data (collected data) that has occured by reporting the variables from 1962 to 2016. Therefore, there is a great deal of overlap and it is hard to see any real relationships or trends. This is amplified by the fact that the life expectancy and certainly the populaion changes each year and so the points are being shifted. It might be best to focus on a particular country or region over a period of time, or to focus on one particular year to reduce this overlap. T\n#Search for years with missing (NA) data points for infant mortality.\n\n#I used the textbook and ChatGPT to help me generate and correct my code. I tried the select function with & first which is not apprpriate for rows\nmissing_infant_mortality &lt;- africadata |&gt; #Assigning new variable and searching in the dataset\nfilter(is.na(infant_mortality)) |&gt;  #continuing with pipe and filtering for where infant mortality is \"NA\"\nselect(year)  #continuing pipe and keeping the years where infant mortality is NA\"\n\n#Create a new object “africa_3_y2000” with the extracted data from the year 2000 from “africadata” and view the summary of the data\n\n#Use the filter function to select the columns where the year is 2000\nafrica_3_y2000 &lt;- africadata |&gt; filter(year==2000)\n#view the data structure of the new object africa_3_y2000\nstr(africa_3_y2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n#view a summary of the new object africa_3_y2000\nsummary(africa_3_y2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\n#Make new plots for the data from the year 2000 following the same approach and similar code as before\n#Generate a plot to view infant mortality vs life expectancy from the year 2000 by using the filtered object created for the year 2000 from the Africa dataset that was originally created.\n\n#Generate the plot as a scatter plot with the variables associated with the approiate axes, reduce point size to 1, set colour to black, label different areas of the graph\nggplot(africa_3_y2000, aes(x = infant_mortality, y = life_expectancy)) +   \n  geom_point(size = 1, color = \"black\") +\n  labs(title = \"Infant mortality vs Life expectancy from the year 2000\", x = \"Infant mortality\", y = \"Life expectancy\" ) \n\n\n\n\n\n\n\n#Save the generated plot\nggsave(\"africa_3_y2000_Life_expectancy_vs_infant_mortality.png\", width = 8, height =6)\n\n#Generate a simialr plot to view population (on a log-scale) vs life expectancy from the year 2000 by using the filtered object created for the year 2000 from the Africa dataset that was originally created.\n\n#Generate the plot as a scatter plot with the variables associated with the approiate axes,set the x-axis (population) to be in the log-scale, reduce point size to 1, set colour to red, label different areas of the graph\nggplot(africa_3_y2000, aes(x = population, y = life_expectancy)) +   \n  geom_point(size = 1, color = \"red\") +\n  scale_x_log10() +\n  labs(title = \"Population vs Life expectancy for the year 2000\", x = \"Population (Log-scale)\", y = \"Life expectancy\" ) \n\n\n\n\n\n\n\n#Save the generated plot\nggsave(\"africa_3_y2000_Life_expectancy_vs_population_log_scale.png\", width = 8, height =6)\n\n#Fit a linear model to the data to help with further interpretation. Use the “lm” function to fit life expectancy as the outcome, with infant mortality as the predictor. This is based on the data from the year 2000 in Africa only.\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africa_3_y2000)\n#View the summary of the model\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africa_3_y2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\nFrom this fit it appears that infant moratlity has a strong and highly significant association with life expectancy (p value is 2.83e-08). We see that higher infant mortality is associated with a lower life expectancy.\n#Generate a similar linear fit with life expectancy as the outcome and population size as the predictor. This is based on the data from the year 2000 in Africa only.\n\nfit2 &lt;- lm(life_expectancy ~ population, data = africa_3_y2000)\n#View the summary of the model\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africa_3_y2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nThere does not appear to be any statistical significance in the relationship between population size and life expectancy in this dataset modelled. The p-value was 0.6159, which is not significant. This indicates that population size is not an effective predictor of life expectancy in this dataset.\n\n\n…………………………………………………………………………………….\n\n\n\n\n\n\n\n\nThis section contributed by Asmith Joseph\n\n# Taking a look at the dslabs to identify which dataset I want to choose for the assignment\nlibrary(dslabs)\ndata(package = \"dslabs\")\n\n\n# I chose us_contagious_diseases dataset\n# Loading the  us_contagious_diseases datasetthe Dataset\nlibrary(dslabs)\ndata(\"us_contagious_diseases\")\n\n\n#The dataset contains the following columns:\n\n#disease: The name of the disease (e.g., \"Measles,\" \"Polio\").\n#state: The U.S. state where the data was recorded.\n#year: The year the data was reported.\n#weeks_reporting: The number of weeks during the year in which the state reported data.\n#count: The number of reported cases of the disease.\n#population: The population of the state in the respective year.\n#rate: The number of disease cases per 10,000 people\n\n\n#Exploring the datasets to identify variables and so on \nhead(us_contagious_diseases)\n\n      disease   state year weeks_reporting count population\n1 Hepatitis A Alabama 1966              50   321    3345787\n2 Hepatitis A Alabama 1967              49   291    3364130\n3 Hepatitis A Alabama 1968              52   314    3386068\n4 Hepatitis A Alabama 1969              49   380    3412450\n5 Hepatitis A Alabama 1970              51   413    3444165\n6 Hepatitis A Alabama 1971              51   378    3481798\n\n#checking the structure \nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n# Exploring the summary \nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n\n\n#Processing and cleaning the Data. First I am filtering out diseases with missing data, mostly focusing on one specific diseases, Measles. \n### Data Processing I focus on measles data for this analysis. The dataset is cleaned to remove rows with missing values, and a new variable, `rate_per_100k`, is calculated to represent cases per 100,000 people.\n\n\n# filtering out the data for measles\nmeasles &lt;- us_contagious_diseases %&gt;%\n  filter(disease == \"Measles\") %&gt;%\n  drop_na()  # Remove rows with missing values\n\n# Adding a column for the cases per 100,000 population\nmeasles &lt;- measles %&gt;%\n  mutate(rate_per_100k = (count / population) * 100000)\n\n# Previewing the cleaned dataset\nhead(measles)\n\n  disease   state year weeks_reporting count population rate_per_100k\n1 Measles Alabama 1928              52  8843    2589923     341.43872\n2 Measles Alabama 1929              49  2959    2619131     112.97640\n3 Measles Alabama 1930              52  4156    2646248     157.05255\n4 Measles Alabama 1931              49  8934    2670818     334.50426\n5 Measles Alabama 1932              41   270    2693027      10.02589\n6 Measles Alabama 1933              51  1735    2713243      63.94562\n\n# Checking for missing values\nsum(is.na(measles))\n\n[1] 0\n\n\n\n### In the next part, I am doing exploratory Figures by visualizing the trends of measles cases over time and across states. 1) figure shows the number of cases over the years, 2) second figure is a heatmap of cases by state and year.\n\n\n#Creating exploratory figures, such as Visualize trends, distributions, or summaries.\n\n# Plot the total number of measles cases over time\nggplot(measles, aes(x = year, y = count)) +\n  geom_line(color = \"pink\") +\n  labs(title = \"Measles Cases Over Time\",\n       x = \"Year\",\n       y = \"Number of Cases\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n#  2nd figure Measles Cases by State (Heatmap)\n# Creating a heatmap of cases by state and year\nmeasles_heatmap &lt;- measles %&gt;%\n  group_by(state, year) %&gt;%\n  summarize(total_cases = sum(count, na.rm = TRUE))\n\n`summarise()` has grouped output by 'state'. You can override using the\n`.groups` argument.\n\nggplot(measles_heatmap, aes(x = year, y = reorder(state, total_cases), fill = total_cases)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  labs(title = \"Measles Cases Heatmap by State and Year\",\n       x = \"Year\",\n       y = \"State\",\n       fill = \"Total Cases\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNext Part, I am focusing on Statistical Model. fitting a simple linear regression model to examine the trend of measles cases over time. The model uses year as the predictor and count (number of cases) as the outcome variable.\n\n# Fitting a linear model to examine the trend of measles cases over time\nmeasles_lm &lt;- lm(count ~ year, data = measles)\n\n# Summarizing the model\nsummary(measles_lm)\n\n\nCall:\nlm(formula = count ~ year, data = measles)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12080  -4342  -1783    846 122169 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 379731.549  15514.361   24.48   &lt;2e-16 ***\nyear          -190.690      7.893  -24.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10460 on 3759 degrees of freedom\nMultiple R-squared:  0.1344,    Adjusted R-squared:  0.1342 \nF-statistic: 583.6 on 1 and 3759 DF,  p-value: &lt; 2.2e-16\n\n\n\n### Results from the Linear Model\n#The summary of the linear regression model shows the following:\n# 1) The slope coefficient for `year` is negative, indicating a decline in measles cases over time, 2) The model's p-value suggests that this decline is statistically significant."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Who am I?\nI am a second year PhD student in the Comparative Biomedical Sciences program in the College of Veterinary Medicine. I am a veterinarian from South Africa and I lived and worked in South Africa until 2023, when I came to UGA!\n\n\nMy background and hopes for the course:\nMy professional background is mainly as a clinical veterinarian, with particular interests in cattle production and medicine (I taught these topics at the University of Pretoria in South Africa previously), and so programming and data analysis is quite new to me.\nAs a veterinarian we get taught some basics in epidemiology but I did not learn anything about real data anaylsis and application. I am interested in learning more about data analysis and statistics to apply this to infectious disease studies in animals (particularly cattle) and my current research projects focus on bovine respiratory disease complex agents. I hope that this course will give me exposure and hands-on experience with working woth different tools (box is already getting ticked with this exercise) and performing data analysis on real-world data.\n\n\nSomething interesting about myself:\nAs you might expect, I love animals and wildlife and my favourite holiday destination is the Kruger National Park in South Africa which is one of the biggest nature reserves in the world. I aim to stay in every camp and to do every bush trail offered, I have big plans!\nThis picture is a typical view of the landscape.\n\n\n\n\n\n\n\nSomething interesting related to data analysis:\nI am sharing a link to a research study that was published by other researchers investigating bovine respiratory disease complex.\n“https://www.nature.com/articles/s41598-021-02343-7”\nThis study is very cool as the researchers are looking at genes and regulatroy mechanisms that are associated with bovine respiratory disease, and they made use of machine learning models for this. I hope to develop my skills with data analysis and machine learning to process similar data and research questions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shaun’s website and data analysis portfolio",
    "section": "",
    "text": "Hello everyone!\n\nI am Shaun van den Hurk.\nWelcome to my website and data analysis portfolio for the Modern Applied Data Analysis (MADA) course at UGA\n\nPlease use the Menu Bar above to look around.\n\nHave fun and stay warm!\nShaun"
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/murph/Documents/Github/ShaunvandenHurk-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.2\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.4.2\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/murph/Documents/Github/ShaunvandenHurk-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nWarning: package 'knitr' was built under R version 4.4.2"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nThe data contains a combination of numeric and categorical data related to people. There are four columns, these look into height, gender, minutes of daily exercise, and star/zodiac sign."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nStar sign\n0\n1\n5\n9\n0\n7\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nDaily Exercise minutes\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n18.77778\n18.06085\n0\n7\n15\n22\n60\n▇▆▂▁▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\nFigure 2 shows a scatterplot figure evaluating weight and daily exercise minutes as produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 2: Weight and daily exercise minutes.\n\n\n\n\n\nThe figure seems to demonstrate a negative correlation between daily exercise and weight, as daily exercise minutes increase, weight decreases.\nFigure 3 shows a boxplot figure evaluating star sign and height as produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 3: Star sign and height.\n\n\n\n\n\nThe box plot does not indicate much information regarding a relationship between star (zodiac) sign and height, and this is likely because there are too few data points to evaluate this effectively. We do see that most participants (except for one) were over 155cm in height and that Virgo was the most common star sign in the dataset."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\nExample Table 3 shows a summary of a linear model fit for height as an outcome with daily exercise minutes and star sign as predictors.\n\n\n\n\nTable 3: Linear model fit table.\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n139.341463\n6.0014622\n23.217919\n0.0274024\n\n\nDaily Exercise minutes\n-1.268293\n0.4146341\n-3.058823\n0.2011529\n\n\nStar signAries\n51.451220\n8.2693269\n6.221936\n0.1014510\n\n\nStar signCapricorn\n16.658537\n8.2302528\n2.024061\n0.2921329\n\n\nStar signLibra\n66.024390\n10.1055112\n6.533503\n0.0966889\n\n\nStar signPisces\n52.536585\n8.0079155\n6.560582\n0.0962959\n\n\nStar signScorpio\n102.756098\n24.1557754\n4.253894\n0.1469870\n\n\nStar signVirgo\n55.195122\n10.0115044\n5.513170\n0.1142306\n\n\n\n\n\n\n\n\nThere does not seem to be a statistically significant association between these factors."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Shaun van den Hurk's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.4.2\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/murph/Documents/Github/ShaunvandenHurk-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name`        `Variable Definition`                 `Allowed Values` \n  &lt;chr&gt;                  &lt;chr&gt;                                 &lt;chr&gt;            \n1 Height                 height in centimeters                 numeric value &gt;0…\n2 Weight                 weight in kilograms                   numeric value &gt;0…\n3 Gender                 identified gender (male/female/other) M/F/O/NA         \n4 Daily exercise minutes Average minutes exercised per day     numeric value &gt;o…\n5 Star sign              Zodiac sign based on birth month      Aries/Taurus/Gem…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height                   &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"15…\n$ Weight                   &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA…\n$ Gender                   &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\",…\n$ `Daily Exercise minutes` &lt;chr&gt; \"20\", \"10\", \"35\", \"15\", \"33\", \"7\", \"0\", \"22\",…\n$ `Star sign`              &lt;chr&gt; \"Libra\", \"Aries\", \"Taurus\", \"Virgo\", \"Libra\",…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Daily Exercise minutes\n Length:14          Min.   :  45.0   Length:14          Length:14             \n Class :character   1st Qu.:  55.0   Class :character   Class :character      \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character      \n                    Mean   : 602.7                                            \n                    3rd Qu.:  90.0                                            \n                    Max.   :7000.0                                            \n                    NA's   :1                                                 \n  Star sign        \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Daily Exercise minutes` `Star sign`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                    &lt;chr&gt;      \n1 180        80 M      20                       Libra      \n2 175        70 O      10                       Aries      \n3 sixty      60 F      35                       Taurus     \n4 178        76 F      15                       Virgo      \n5 192        90 NA     33                       Libra      \n6 6          55 F      7                        Pisces     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nDaily Exercise minutes\n0\n1\n1\n2\n0\n14\n0\n\n\nStar sign\n0\n1\n3\n11\n0\n11\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nDaily Exercise minutes\n0\n1\n1\n2\n0\n13\n0\n\n\nStar sign\n0\n1\n3\n11\n0\n10\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nDaily Exercise minutes\n0\n1\n1\n2\n0\n13\n0\n\n\nStar sign\n0\n1\n3\n11\n0\n10\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nDaily Exercise minutes\n0\n1\n1\n2\n0\n11\n0\n\n\nStar sign\n0\n1\n5\n9\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\nsummary(d3)\n\n     Height          Weight          Gender          Daily Exercise minutes\n Min.   :133.0   Min.   : 45.00   Length:11          Length:11             \n 1st Qu.:155.5   1st Qu.: 54.50   Class :character   Class :character      \n Median :166.0   Median : 70.00   Mode  :character   Mode  :character      \n Mean   :167.1   Mean   : 70.45                                            \n 3rd Qu.:179.0   3rd Qu.: 85.00                                            \n Max.   :192.0   Max.   :110.00                                            \n  Star sign        \n Length:11         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nclass(d3$Gender)\n\n[1] \"character\"\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nDaily Exercise minutes\n0\n1\n1\n2\n0\n11\n0\n\n\nStar sign\n0\n1\n5\n9\n0\n8\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nDaily Exercise minutes\n0\n1\n1\n2\n0\n9\n0\n\n\nStar sign\n0\n1\n5\n9\n0\n7\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nM.Y: The column Daily Exercise minutes is read as character by R. Here I will convert it back to integer.\n\nd4$`Daily Exercise minutes` &lt;- as.integer(d4$`Daily Exercise minutes`)  \nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nStar sign\n0\n1\n5\n9\n0\n7\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nDaily Exercise minutes\n0\n1\n18.78\n18.06\n0\n7\n15\n22\n60\n▇▆▂▁▂\n\n\n\n\nclass(d4$`Daily Exercise minutes`)\n\n[1] \"integer\"\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/murph/Documents/Github/ShaunvandenHurk-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.2\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Star sign             0             1   5   9     0        7          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable          n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100\n1 Height                         0             1 166.  16.0 133 156 166 178  183\n2 Weight                         0             1  70.1 21.2  45  55  70  80  110\n3 Daily Exercise minutes         0             1  18.8 18.1   0   7  15  22   60\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▇▆▂▁▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nM.Y: Here is the scattter plot with weight on the x-axis and daily exercise minutes on the y-axis.\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y= `Daily Exercise minutes`)) + geom_point() + geom_smooth(method='lm')\nplot(p5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-daily exercise minutes.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nM.Y: This is a Boxplot with star sign plotted against Height with star sign on the x-axis and height on the y-axis.\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=`Star sign`, y= Height)) + geom_boxplot() + geom_smooth(method='lm')\nplot(p6)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"star sign-height.png\")\nggsave(filename = figure_file, plot=p6)\n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html",
    "href": "data-exercise/data-exercise.qmd.html",
    "title": "Data Exercise",
    "section": "",
    "text": "Load the required packages:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nWe are looking to generate a synthetic dataset that evaluates the relationship between vaccination status and history of disease within and to compare thi between different cattle breeds (beef vs dairy breeds). We are going to generate a dataset with 330 individual animals from the age of 6 months to 3 years. Since females are found in higher numbers in cattle production systems we are generating the dataset with 70% females (30% males). We are looking at vaccination as yes or no without specifying specific diseases or vaccines. We will generate columns for history of respiratory disease and history of diarrheal disease (either yes or no).\nWe first need to set the seed to that our random generation of variables is repeatable\n\nset.seed(123)\n\nWe will start to generate our dataset as described:\n\n#Start by assigning the number of cattle (observations)\nn &lt;- 330\n\n#We will assign ages to the animals between 6 months and 3 years (18 months). Age can only be integers (whole numbers in months)\nage &lt;- sample(6:36, n, replace = TRUE)\n\n#we will assign the sex with 70% as females as mentioned\nsex &lt;- sample(c(\"Female\", \"Male\"), n, replace= TRUE, prob = c(0.7, 0.3))\n\n#We will assign the cattle breeds with an equal split using Angus as the beef breed and Holstein as the dairy breed\nbreed &lt;- sample(c(\"Angus\", \"Holstein\"), n, replace = TRUE)\n\n#We will classify these breeds to help with further interpretation and analysis downstream. This would be particularly helpful if we had multiple different breeds for each category in the dataset. We will use the if else function in R.\nbreed_type &lt;- ifelse(breed==\"Angus\", \"Beef\", \"Dairy\")\n\n#Double check that our breed data worked\nView(breed_type)\nprint(breed_type)\n\n  [1] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\"\n [10] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\"\n [19] \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\"\n [28] \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\" \n [37] \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\"\n [46] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\"\n [55] \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\" \n [64] \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\"\n [73] \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\"\n [82] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\"\n [91] \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\"\n[100] \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[109] \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\"\n[118] \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[127] \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[136] \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\"\n[145] \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\" \n[154] \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\"\n[163] \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\" \n[172] \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[181] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\" \n[190] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\"\n[199] \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[208] \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\" \n[217] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[226] \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[235] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[244] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\" \n[253] \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\"\n[262] \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\"\n[271] \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[280] \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[289] \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[298] \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[307] \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\" \n[316] \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\" \n[325] \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\" \n\n\nWe must now generate the vaccination and disease history data before we can produce our dataframe.\nWe will make the incidence of respiratory disease slightly higher in beef animals than in dairy animals, and the opposite (slightly higher incidence of diarrheal disease in dairy animals than beef animals).\n\n#We will assign vaccination status (yes/no) randomly\nvaccination &lt;- sample (c(\"Yes\",\"No\"), n, replace = TRUE)\n\n#Assign history of diseases based on our assumptions\n#Start by generating a vector for the history of disease - diarrhea or respiratory\nhistory_diarrhea &lt;- rep(\"No\", n)\nhistory_respiratory &lt;- rep(\"No\", n)\n\n#Using a loop function to change the incidence of the different diseases in beef and dairy breeds as mentioned above\n#I had chatGPT help me to generate the code for this\n\nfor (i in 1:n) {\n  if (breed[i] == \"Holstein\") {\n    if (runif(1) &lt; 0.35) history_diarrhea[i] &lt;- \"Yes\"\n    if (runif(1) &lt; 0.25) history_respiratory[i] &lt;- \"Yes\"\n  } else {\n    if (runif(1) &lt; 0.30) history_diarrhea[i] &lt;- \"Yes\"\n    if (runif(1) &lt; 0.35) history_respiratory[i] &lt;- \"Yes\"\n  }\n}\n\n#We are applying the conditions above if the breed is Holstein and if it is not then we apply the conditions below the else.\n\n\n#We now want to make sure that animals that are not vaccinated have a higher incidence of disease (vaccination is correlated with freedom from or reduced incidence of disease)\n#We will use another loop funciton for this.\n# I had ChatGPT assist with this code for the loop\n\nfor (i in 1:n) {\n  if (vaccination[i] == \"No\" && history_diarrhea[i] == \"No\" && history_respiratory[i] == \"No\") {\n    if (runif(1) &lt; 0.5) {\n      history_diarrhea[i] &lt;- \"Yes\"\n    } else {\n      history_respiratory[i] &lt;- \"Yes\"\n    }\n  }\n}\n\nWe want to make four different categories that will help us to evaluate the different disease states that are possible. Different states are “Respiratory disease”, “Diarrhea disease”, “Both diseases”, and “No disease”. We will create an additional column for this to make it easier for us to call back to this for later models and plots. This could have been done as a second or separate dataframe without including it in the main datframe if we preferred. For ease it has been done now for less separate steps.\nCreate disease history classification:\n\ndisease_history &lt;- case_when(history_respiratory == \"Yes\" & history_diarrhea == \"No\" ~ \"Respiratory Only\", history_respiratory == \"No\" & history_diarrhea == \"Yes\" ~ \"Diarrhea Only\", history_respiratory == \"yes\" & history_diarrhea == \"Yes\" ~ \"Both Diseases\", TRUE ~ \"No Disease\")\n\nWe then create a dataframe with all of these variables that we have assigned. We will call this cattle_data. We change the font of the column names to help use define searches and functions more directly.\n\ncattle_data &lt;- data.frame(Age = age, Sex = sex, Breed = breed, Breed_Type = breed_type, Vaccination = vaccination, History_Diarrhea = history_diarrhea, History_Respiratory = history_respiratory, Disease_History = factor(disease_history, levels = c(\"No Disease\", \"Respiratory Only\", \"Diarrhea Only\", \"Both Diseases\")))\n\nsummary(cattle_data)\n\n      Age            Sex               Breed            Breed_Type       \n Min.   : 6.00   Length:330         Length:330         Length:330        \n 1st Qu.:14.00   Class :character   Class :character   Class :character  \n Median :22.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :21.84                                                           \n 3rd Qu.:30.00                                                           \n Max.   :36.00                                                           \n Vaccination        History_Diarrhea   History_Respiratory\n Length:330         Length:330         Length:330         \n Class :character   Class :character   Class :character   \n Mode  :character   Mode  :character   Mode  :character   \n                                                          \n                                                          \n                                                          \n         Disease_History\n No Disease      :111   \n Respiratory Only:102   \n Diarrhea Only   :117   \n Both Diseases   :  0   \n                        \n                        \n\nprint(cattle_data)\n\n    Age    Sex    Breed Breed_Type Vaccination History_Diarrhea\n1    36 Female    Angus       Beef          No              Yes\n2    20 Female    Angus       Beef          No               No\n3    24 Female    Angus       Beef         Yes              Yes\n4    19 Female Holstein      Dairy          No              Yes\n5     8 Female Holstein      Dairy         Yes               No\n6    15   Male Holstein      Dairy         Yes               No\n7    23 Female Holstein      Dairy          No              Yes\n8    27 Female    Angus       Beef          No               No\n9    16 Female Holstein      Dairy         Yes               No\n10   10 Female    Angus       Beef         Yes               No\n11   25   Male    Angus       Beef          No              Yes\n12   19 Female    Angus       Beef          No              Yes\n13   27 Female Holstein      Dairy         Yes               No\n14   30 Female    Angus       Beef          No               No\n15   31   Male Holstein      Dairy         Yes               No\n16   32 Female Holstein      Dairy         Yes              Yes\n17   10 Female    Angus       Beef         Yes               No\n18   24 Female Holstein      Dairy          No              Yes\n19   32   Male    Angus       Beef          No               No\n20   30 Female Holstein      Dairy         Yes              Yes\n21   33 Female    Angus       Beef         Yes              Yes\n22   30   Male    Angus       Beef          No              Yes\n23   14 Female Holstein      Dairy         Yes               No\n24   34 Female Holstein      Dairy          No               No\n25    8   Male Holstein      Dairy          No               No\n26   13 Female Holstein      Dairy          No              Yes\n27   31 Female Holstein      Dairy         Yes              Yes\n28   12 Female Holstein      Dairy         Yes              Yes\n29   15 Female Holstein      Dairy         Yes               No\n30   14 Female Holstein      Dairy          No              Yes\n31   35 Female    Angus       Beef          No               No\n32   24   Male    Angus       Beef          No              Yes\n33    9 Female    Angus       Beef         Yes               No\n34   19 Female Holstein      Dairy          No               No\n35   22   Male    Angus       Beef          No              Yes\n36   16   Male    Angus       Beef         Yes              Yes\n37   12 Female    Angus       Beef         Yes               No\n38   26 Female Holstein      Dairy         Yes               No\n39   17   Male Holstein      Dairy         Yes               No\n40   20 Female    Angus       Beef         Yes               No\n41   15   Male Holstein      Dairy         Yes               No\n42   18 Female    Angus       Beef         Yes              Yes\n43   12   Male Holstein      Dairy          No              Yes\n44   14 Female    Angus       Beef          No              Yes\n45   14   Male Holstein      Dairy          No              Yes\n46   15 Female Holstein      Dairy          No              Yes\n47   28 Female Holstein      Dairy          No              Yes\n48   32 Female    Angus       Beef         Yes              Yes\n49   33 Female    Angus       Beef         Yes               No\n50   26   Male Holstein      Dairy          No               No\n51   12 Female Holstein      Dairy          No              Yes\n52   26   Male Holstein      Dairy          No              Yes\n53   32   Male Holstein      Dairy         Yes               No\n54   11 Female Holstein      Dairy          No               No\n55   30 Female    Angus       Beef         Yes               No\n56    7 Female    Angus       Beef          No               No\n57   34 Female Holstein      Dairy          No              Yes\n58   10 Female    Angus       Beef          No               No\n59   13   Male Holstein      Dairy          No              Yes\n60   17   Male    Angus       Beef          No              Yes\n61   36 Female    Angus       Beef         Yes               No\n62   18   Male    Angus       Beef         Yes              Yes\n63   23 Female    Angus       Beef         Yes               No\n64    6 Female    Angus       Beef          No              Yes\n65   30 Female Holstein      Dairy          No              Yes\n66   32   Male Holstein      Dairy          No              Yes\n67   30 Female    Angus       Beef         Yes              Yes\n68   11 Female Holstein      Dairy         Yes               No\n69   26 Female    Angus       Beef         Yes              Yes\n70   20 Female    Angus       Beef          No               No\n71   14   Male    Angus       Beef         Yes              Yes\n72   20 Female Holstein      Dairy          No              Yes\n73   31 Female    Angus       Beef         Yes               No\n74   33 Female    Angus       Beef         Yes               No\n75   36 Female    Angus       Beef          No               No\n76   21   Male    Angus       Beef          No              Yes\n77   25 Female Holstein      Dairy         Yes               No\n78   35 Female    Angus       Beef         Yes              Yes\n79   11 Female Holstein      Dairy          No               No\n80   16 Female Holstein      Dairy          No               No\n81   13 Female Holstein      Dairy         Yes              Yes\n82   27 Female    Angus       Beef         Yes               No\n83   27 Female    Angus       Beef         Yes               No\n84   12   Male    Angus       Beef          No              Yes\n85   36 Female Holstein      Dairy         Yes               No\n86   21 Female Holstein      Dairy         Yes               No\n87   22 Female    Angus       Beef         Yes              Yes\n88   27 Female Holstein      Dairy         Yes               No\n89   23   Male    Angus       Beef          No               No\n90   22   Male Holstein      Dairy         Yes               No\n91    7 Female    Angus       Beef          No              Yes\n92    9 Female Holstein      Dairy         Yes              Yes\n93   18   Male Holstein      Dairy          No               No\n94   10 Female Holstein      Dairy         Yes               No\n95   27 Female Holstein      Dairy          No              Yes\n96   36 Female Holstein      Dairy          No              Yes\n97   24 Female Holstein      Dairy         Yes               No\n98   30 Female Holstein      Dairy          No              Yes\n99   25 Female Holstein      Dairy          No               No\n100  27 Female    Angus       Beef         Yes              Yes\n101  30 Female    Angus       Beef         Yes               No\n102  19   Male Holstein      Dairy         Yes               No\n103  30 Female    Angus       Beef         Yes              Yes\n104  28   Male    Angus       Beef          No              Yes\n105   8   Male Holstein      Dairy          No               No\n106  13   Male    Angus       Beef         Yes               No\n107  21 Female    Angus       Beef         Yes               No\n108  35 Female Holstein      Dairy         Yes              Yes\n109  17 Female Holstein      Dairy         Yes               No\n110  36 Female    Angus       Beef          No              Yes\n111  30 Female    Angus       Beef         Yes              Yes\n112  19 Female    Angus       Beef          No              Yes\n113  35 Female    Angus       Beef          No              Yes\n114   8 Female    Angus       Beef          No              Yes\n115  19   Male    Angus       Beef         Yes               No\n116  34   Male Holstein      Dairy          No              Yes\n117  12   Male Holstein      Dairy          No              Yes\n118   8 Female Holstein      Dairy          No              Yes\n119  28 Female Holstein      Dairy          No               No\n120  27   Male    Angus       Beef         Yes               No\n121  31 Female Holstein      Dairy          No              Yes\n122  20 Female    Angus       Beef         Yes               No\n123  26 Female Holstein      Dairy         Yes               No\n124  10 Female    Angus       Beef          No               No\n125  13 Female    Angus       Beef          No               No\n126  24 Female    Angus       Beef         Yes              Yes\n127  15 Female Holstein      Dairy         Yes              Yes\n128  23 Female Holstein      Dairy         Yes               No\n129  15   Male Holstein      Dairy         Yes               No\n130  17 Female    Angus       Beef         Yes               No\n131   7   Male    Angus       Beef         Yes               No\n132  15 Female Holstein      Dairy         Yes               No\n133  27   Male Holstein      Dairy          No              Yes\n134  36 Female    Angus       Beef          No              Yes\n135  17 Female    Angus       Beef         Yes              Yes\n136  25 Female    Angus       Beef         Yes              Yes\n137  19 Female    Angus       Beef          No              Yes\n138  22 Female    Angus       Beef         Yes              Yes\n139  35   Male    Angus       Beef         Yes              Yes\n140  19 Female    Angus       Beef         Yes               No\n141  27   Male    Angus       Beef          No               No\n142   8 Female    Angus       Beef         Yes               No\n143  13 Female    Angus       Beef          No              Yes\n144  19   Male Holstein      Dairy          No              Yes\n145  24 Female    Angus       Beef          No              Yes\n146  35 Female Holstein      Dairy          No              Yes\n147  20 Female    Angus       Beef          No              Yes\n148  29 Female Holstein      Dairy          No              Yes\n149  22   Male Holstein      Dairy          No              Yes\n150  28   Male    Angus       Beef         Yes               No\n151  16 Female Holstein      Dairy          No               No\n152  12 Female Holstein      Dairy         Yes               No\n153  34   Male    Angus       Beef         Yes               No\n154  20 Female    Angus       Beef          No              Yes\n155  28   Male Holstein      Dairy          No               No\n156  31 Female Holstein      Dairy         Yes               No\n157  11 Female    Angus       Beef          No               No\n158  19 Female Holstein      Dairy         Yes               No\n159  36   Male    Angus       Beef         Yes               No\n160  12 Female Holstein      Dairy          No              Yes\n161  32 Female Holstein      Dairy          No              Yes\n162  15 Female Holstein      Dairy         Yes               No\n163  10 Female    Angus       Beef          No               No\n164  11 Female    Angus       Beef         Yes               No\n165  21 Female Holstein      Dairy          No               No\n166  29 Female Holstein      Dairy         Yes              Yes\n167  26 Female Holstein      Dairy         Yes               No\n168  28   Male    Angus       Beef          No               No\n169  16 Female    Angus       Beef          No               No\n170   9 Female Holstein      Dairy          No              Yes\n171  17 Female    Angus       Beef          No               No\n172  19   Male Holstein      Dairy          No               No\n173  33 Female Holstein      Dairy          No               No\n174  34   Male Holstein      Dairy          No              Yes\n175  24 Female Holstein      Dairy          No              Yes\n176  30 Female    Angus       Beef          No              Yes\n177  12   Male Holstein      Dairy          No              Yes\n178  27 Female Holstein      Dairy         Yes              Yes\n179  31   Male    Angus       Beef         Yes               No\n180  14 Female    Angus       Beef         Yes               No\n181  12 Female Holstein      Dairy         Yes              Yes\n182   7 Female Holstein      Dairy         Yes               No\n183  21 Female    Angus       Beef         Yes               No\n184  18 Female    Angus       Beef         Yes               No\n185  24 Female    Angus       Beef          No              Yes\n186  29   Male    Angus       Beef          No              Yes\n187  25 Female    Angus       Beef          No               No\n188  20   Male    Angus       Beef         Yes               No\n189  12 Female    Angus       Beef         Yes               No\n190   9   Male    Angus       Beef          No               No\n191   6 Female    Angus       Beef          No              Yes\n192  13 Female    Angus       Beef         Yes               No\n193  35   Male Holstein      Dairy          No               No\n194  35 Female    Angus       Beef         Yes               No\n195  25   Male    Angus       Beef         Yes              Yes\n196  30 Female    Angus       Beef         Yes               No\n197  21   Male Holstein      Dairy          No              Yes\n198  29 Female Holstein      Dairy         Yes               No\n199  27 Female Holstein      Dairy         Yes               No\n200  16   Male Holstein      Dairy         Yes              Yes\n201  21 Female Holstein      Dairy          No               No\n202  25   Male Holstein      Dairy          No               No\n203  13 Female Holstein      Dairy         Yes               No\n204   8   Male Holstein      Dairy         Yes               No\n205  34   Male    Angus       Beef         Yes               No\n206   9 Female    Angus       Beef          No               No\n207  25   Male Holstein      Dairy         Yes              Yes\n208  17   Male Holstein      Dairy         Yes              Yes\n209  27 Female    Angus       Beef         Yes               No\n210  22 Female Holstein      Dairy         Yes               No\n211  15 Female    Angus       Beef         Yes               No\n212  32 Female    Angus       Beef          No               No\n213  25   Male    Angus       Beef         Yes               No\n214  16 Female    Angus       Beef          No              Yes\n215  30 Female    Angus       Beef         Yes               No\n216  28 Female    Angus       Beef         Yes              Yes\n217  13 Female Holstein      Dairy          No              Yes\n218  19 Female Holstein      Dairy          No              Yes\n219  26 Female    Angus       Beef          No               No\n220  18 Female    Angus       Beef          No               No\n221   7   Male Holstein      Dairy          No               No\n222  16   Male Holstein      Dairy          No               No\n223  18 Female    Angus       Beef          No               No\n224  19 Female    Angus       Beef         Yes              Yes\n225  11 Female    Angus       Beef         Yes               No\n226  30 Female Holstein      Dairy          No              Yes\n227  13   Male    Angus       Beef          No               No\n228  17 Female    Angus       Beef         Yes              Yes\n229  31   Male Holstein      Dairy          No               No\n230   9 Female Holstein      Dairy          No               No\n231  18   Male Holstein      Dairy         Yes              Yes\n232  34 Female    Angus       Beef         Yes               No\n233  19 Female    Angus       Beef          No              Yes\n234  26   Male Holstein      Dairy          No              Yes\n235  21   Male Holstein      Dairy          No               No\n236  28 Female Holstein      Dairy          No               No\n237  34 Female    Angus       Beef          No              Yes\n238   6 Female    Angus       Beef          No              Yes\n239  13 Female Holstein      Dairy         Yes               No\n240  13   Male Holstein      Dairy         Yes               No\n241  15   Male    Angus       Beef          No               No\n242  30   Male    Angus       Beef          No              Yes\n243  13 Female    Angus       Beef          No               No\n244  23   Male Holstein      Dairy         Yes               No\n245  34 Female Holstein      Dairy          No              Yes\n246  26 Female    Angus       Beef         Yes               No\n247  14 Female    Angus       Beef         Yes               No\n248  12   Male Holstein      Dairy         Yes               No\n249  12 Female    Angus       Beef         Yes               No\n250  31 Female    Angus       Beef          No              Yes\n251  34 Female Holstein      Dairy         Yes               No\n252  33   Male    Angus       Beef         Yes              Yes\n253  15 Female Holstein      Dairy          No              Yes\n254  29 Female    Angus       Beef         Yes               No\n255  36   Male    Angus       Beef         Yes               No\n256  27 Female Holstein      Dairy         Yes               No\n257  28 Female Holstein      Dairy         Yes               No\n258  31   Male    Angus       Beef         Yes              Yes\n259  16 Female Holstein      Dairy         Yes              Yes\n260   6 Female Holstein      Dairy         Yes              Yes\n261  30 Female Holstein      Dairy          No               No\n262  34 Female Holstein      Dairy          No              Yes\n263  34 Female Holstein      Dairy         Yes               No\n264  24 Female Holstein      Dairy         Yes              Yes\n265  15   Male Holstein      Dairy         Yes               No\n266  26 Female    Angus       Beef         Yes               No\n267  27   Male    Angus       Beef         Yes               No\n268  18 Female Holstein      Dairy         Yes              Yes\n269  16 Female    Angus       Beef          No              Yes\n270  16 Female Holstein      Dairy         Yes               No\n271  30 Female Holstein      Dairy         Yes               No\n272  25 Female    Angus       Beef         Yes               No\n273  31   Male    Angus       Beef          No               No\n274  12 Female Holstein      Dairy          No              Yes\n275  30   Male    Angus       Beef          No               No\n276  28   Male Holstein      Dairy         Yes              Yes\n277  31   Male Holstein      Dairy          No              Yes\n278  25   Male    Angus       Beef          No               No\n279  34 Female    Angus       Beef         Yes               No\n280  29   Male    Angus       Beef         Yes               No\n281  30   Male Holstein      Dairy          No               No\n282  14   Male    Angus       Beef          No               No\n283  14 Female Holstein      Dairy          No              Yes\n284  10 Female Holstein      Dairy          No               No\n285  28 Female Holstein      Dairy          No               No\n286  19 Female    Angus       Beef          No              Yes\n287  19   Male    Angus       Beef          No               No\n288  11 Female    Angus       Beef          No               No\n289  32 Female    Angus       Beef         Yes               No\n290   6   Male    Angus       Beef         Yes               No\n291  31   Male    Angus       Beef          No              Yes\n292  15 Female    Angus       Beef         Yes               No\n293  22   Male Holstein      Dairy          No              Yes\n294  31 Female Holstein      Dairy         Yes               No\n295  22 Female Holstein      Dairy         Yes              Yes\n296  34   Male    Angus       Beef          No              Yes\n297  31   Male    Angus       Beef          No              Yes\n298  32   Male    Angus       Beef         Yes               No\n299  26 Female    Angus       Beef          No              Yes\n300  12 Female Holstein      Dairy         Yes               No\n301  33   Male    Angus       Beef         Yes               No\n302  26   Male Holstein      Dairy          No              Yes\n303  31 Female Holstein      Dairy          No              Yes\n304  14 Female    Angus       Beef          No               No\n305  25 Female    Angus       Beef          No              Yes\n306  11   Male Holstein      Dairy          No              Yes\n307  23 Female    Angus       Beef          No               No\n308  35 Female Holstein      Dairy         Yes               No\n309  36 Female    Angus       Beef         Yes               No\n310  34   Male Holstein      Dairy          No              Yes\n311  22   Male Holstein      Dairy         Yes               No\n312  10 Female Holstein      Dairy         Yes              Yes\n313  30 Female    Angus       Beef          No               No\n314  25   Male Holstein      Dairy         Yes               No\n315   8   Male    Angus       Beef         Yes              Yes\n316  30   Male    Angus       Beef          No              Yes\n317  19 Female Holstein      Dairy         Yes              Yes\n318   6 Female Holstein      Dairy         Yes               No\n319   7 Female Holstein      Dairy          No              Yes\n320   9   Male    Angus       Beef          No               No\n321  15   Male Holstein      Dairy         Yes               No\n322   6 Female    Angus       Beef          No               No\n323  10   Male Holstein      Dairy          No               No\n324  26 Female    Angus       Beef          No              Yes\n325  30 Female    Angus       Beef         Yes              Yes\n326  13   Male Holstein      Dairy          No               No\n327  28 Female Holstein      Dairy          No              Yes\n328  30 Female Holstein      Dairy          No              Yes\n329  26 Female Holstein      Dairy          No               No\n330  18 Female    Angus       Beef         Yes              Yes\n    History_Respiratory  Disease_History\n1                    No    Diarrhea Only\n2                   Yes Respiratory Only\n3                    No    Diarrhea Only\n4                    No    Diarrhea Only\n5                    No       No Disease\n6                    No       No Disease\n7                    No    Diarrhea Only\n8                   Yes Respiratory Only\n9                    No       No Disease\n10                  Yes Respiratory Only\n11                  Yes       No Disease\n12                  Yes       No Disease\n13                  Yes Respiratory Only\n14                  Yes Respiratory Only\n15                   No       No Disease\n16                   No    Diarrhea Only\n17                  Yes Respiratory Only\n18                  Yes       No Disease\n19                  Yes Respiratory Only\n20                   No    Diarrhea Only\n21                   No    Diarrhea Only\n22                   No    Diarrhea Only\n23                   No       No Disease\n24                  Yes Respiratory Only\n25                  Yes Respiratory Only\n26                   No    Diarrhea Only\n27                   No    Diarrhea Only\n28                  Yes       No Disease\n29                  Yes Respiratory Only\n30                   No    Diarrhea Only\n31                  Yes Respiratory Only\n32                   No    Diarrhea Only\n33                   No       No Disease\n34                  Yes Respiratory Only\n35                  Yes       No Disease\n36                  Yes       No Disease\n37                  Yes Respiratory Only\n38                   No       No Disease\n39                  Yes Respiratory Only\n40                   No       No Disease\n41                   No       No Disease\n42                  Yes       No Disease\n43                   No    Diarrhea Only\n44                   No    Diarrhea Only\n45                   No    Diarrhea Only\n46                   No    Diarrhea Only\n47                   No    Diarrhea Only\n48                   No    Diarrhea Only\n49                   No       No Disease\n50                  Yes Respiratory Only\n51                   No    Diarrhea Only\n52                   No    Diarrhea Only\n53                   No       No Disease\n54                  Yes Respiratory Only\n55                  Yes Respiratory Only\n56                  Yes Respiratory Only\n57                   No    Diarrhea Only\n58                  Yes Respiratory Only\n59                   No    Diarrhea Only\n60                   No    Diarrhea Only\n61                   No       No Disease\n62                   No    Diarrhea Only\n63                  Yes Respiratory Only\n64                   No    Diarrhea Only\n65                   No    Diarrhea Only\n66                   No    Diarrhea Only\n67                  Yes       No Disease\n68                   No       No Disease\n69                   No    Diarrhea Only\n70                  Yes Respiratory Only\n71                   No    Diarrhea Only\n72                   No    Diarrhea Only\n73                   No       No Disease\n74                   No       No Disease\n75                  Yes Respiratory Only\n76                   No    Diarrhea Only\n77                   No       No Disease\n78                   No    Diarrhea Only\n79                  Yes Respiratory Only\n80                  Yes Respiratory Only\n81                   No    Diarrhea Only\n82                  Yes Respiratory Only\n83                   No       No Disease\n84                   No    Diarrhea Only\n85                   No       No Disease\n86                   No       No Disease\n87                   No    Diarrhea Only\n88                   No       No Disease\n89                  Yes Respiratory Only\n90                   No       No Disease\n91                   No    Diarrhea Only\n92                   No    Diarrhea Only\n93                  Yes Respiratory Only\n94                   No       No Disease\n95                   No    Diarrhea Only\n96                   No    Diarrhea Only\n97                   No       No Disease\n98                  Yes       No Disease\n99                  Yes Respiratory Only\n100                  No    Diarrhea Only\n101                  No       No Disease\n102                  No       No Disease\n103                  No    Diarrhea Only\n104                  No    Diarrhea Only\n105                 Yes Respiratory Only\n106                 Yes Respiratory Only\n107                  No       No Disease\n108                 Yes       No Disease\n109                  No       No Disease\n110                  No    Diarrhea Only\n111                  No    Diarrhea Only\n112                  No    Diarrhea Only\n113                  No    Diarrhea Only\n114                  No    Diarrhea Only\n115                  No       No Disease\n116                  No    Diarrhea Only\n117                  No    Diarrhea Only\n118                 Yes       No Disease\n119                 Yes Respiratory Only\n120                 Yes Respiratory Only\n121                  No    Diarrhea Only\n122                 Yes Respiratory Only\n123                 Yes Respiratory Only\n124                 Yes Respiratory Only\n125                 Yes Respiratory Only\n126                 Yes       No Disease\n127                  No    Diarrhea Only\n128                  No       No Disease\n129                  No       No Disease\n130                  No       No Disease\n131                  No       No Disease\n132                  No       No Disease\n133                  No    Diarrhea Only\n134                  No    Diarrhea Only\n135                  No    Diarrhea Only\n136                 Yes       No Disease\n137                  No    Diarrhea Only\n138                  No    Diarrhea Only\n139                  No    Diarrhea Only\n140                  No       No Disease\n141                 Yes Respiratory Only\n142                  No       No Disease\n143                  No    Diarrhea Only\n144                  No    Diarrhea Only\n145                 Yes       No Disease\n146                 Yes       No Disease\n147                  No    Diarrhea Only\n148                  No    Diarrhea Only\n149                  No    Diarrhea Only\n150                  No       No Disease\n151                 Yes Respiratory Only\n152                  No       No Disease\n153                 Yes Respiratory Only\n154                 Yes       No Disease\n155                 Yes Respiratory Only\n156                  No       No Disease\n157                 Yes Respiratory Only\n158                  No       No Disease\n159                  No       No Disease\n160                  No    Diarrhea Only\n161                  No    Diarrhea Only\n162                 Yes Respiratory Only\n163                 Yes Respiratory Only\n164                 Yes Respiratory Only\n165                 Yes Respiratory Only\n166                  No    Diarrhea Only\n167                  No       No Disease\n168                 Yes Respiratory Only\n169                 Yes Respiratory Only\n170                  No    Diarrhea Only\n171                 Yes Respiratory Only\n172                 Yes Respiratory Only\n173                 Yes Respiratory Only\n174                  No    Diarrhea Only\n175                 Yes       No Disease\n176                  No    Diarrhea Only\n177                  No    Diarrhea Only\n178                  No    Diarrhea Only\n179                  No       No Disease\n180                  No       No Disease\n181                  No    Diarrhea Only\n182                  No       No Disease\n183                  No       No Disease\n184                  No       No Disease\n185                  No    Diarrhea Only\n186                  No    Diarrhea Only\n187                 Yes Respiratory Only\n188                 Yes Respiratory Only\n189                  No       No Disease\n190                 Yes Respiratory Only\n191                 Yes       No Disease\n192                  No       No Disease\n193                 Yes Respiratory Only\n194                  No       No Disease\n195                  No    Diarrhea Only\n196                  No       No Disease\n197                 Yes       No Disease\n198                 Yes Respiratory Only\n199                  No       No Disease\n200                  No    Diarrhea Only\n201                 Yes Respiratory Only\n202                 Yes Respiratory Only\n203                  No       No Disease\n204                  No       No Disease\n205                 Yes Respiratory Only\n206                 Yes Respiratory Only\n207                 Yes       No Disease\n208                  No    Diarrhea Only\n209                  No       No Disease\n210                  No       No Disease\n211                  No       No Disease\n212                 Yes Respiratory Only\n213                  No       No Disease\n214                  No    Diarrhea Only\n215                  No       No Disease\n216                 Yes       No Disease\n217                  No    Diarrhea Only\n218                  No    Diarrhea Only\n219                 Yes Respiratory Only\n220                 Yes Respiratory Only\n221                 Yes Respiratory Only\n222                 Yes Respiratory Only\n223                 Yes Respiratory Only\n224                  No    Diarrhea Only\n225                 Yes Respiratory Only\n226                  No    Diarrhea Only\n227                 Yes Respiratory Only\n228                  No    Diarrhea Only\n229                 Yes Respiratory Only\n230                 Yes Respiratory Only\n231                  No    Diarrhea Only\n232                  No       No Disease\n233                  No    Diarrhea Only\n234                  No    Diarrhea Only\n235                 Yes Respiratory Only\n236                 Yes Respiratory Only\n237                  No    Diarrhea Only\n238                  No    Diarrhea Only\n239                  No       No Disease\n240                  No       No Disease\n241                 Yes Respiratory Only\n242                  No    Diarrhea Only\n243                 Yes Respiratory Only\n244                  No       No Disease\n245                  No    Diarrhea Only\n246                 Yes Respiratory Only\n247                  No       No Disease\n248                 Yes Respiratory Only\n249                  No       No Disease\n250                  No    Diarrhea Only\n251                  No       No Disease\n252                 Yes       No Disease\n253                  No    Diarrhea Only\n254                  No       No Disease\n255                 Yes Respiratory Only\n256                  No       No Disease\n257                  No       No Disease\n258                  No    Diarrhea Only\n259                  No    Diarrhea Only\n260                  No    Diarrhea Only\n261                 Yes Respiratory Only\n262                  No    Diarrhea Only\n263                  No       No Disease\n264                  No    Diarrhea Only\n265                  No       No Disease\n266                 Yes Respiratory Only\n267                  No       No Disease\n268                  No    Diarrhea Only\n269                  No    Diarrhea Only\n270                  No       No Disease\n271                 Yes Respiratory Only\n272                 Yes Respiratory Only\n273                 Yes Respiratory Only\n274                  No    Diarrhea Only\n275                 Yes Respiratory Only\n276                  No    Diarrhea Only\n277                 Yes       No Disease\n278                 Yes Respiratory Only\n279                 Yes Respiratory Only\n280                  No       No Disease\n281                 Yes Respiratory Only\n282                 Yes Respiratory Only\n283                  No    Diarrhea Only\n284                 Yes Respiratory Only\n285                 Yes Respiratory Only\n286                 Yes       No Disease\n287                 Yes Respiratory Only\n288                 Yes Respiratory Only\n289                  No       No Disease\n290                 Yes Respiratory Only\n291                  No    Diarrhea Only\n292                  No       No Disease\n293                 Yes       No Disease\n294                  No       No Disease\n295                  No    Diarrhea Only\n296                  No    Diarrhea Only\n297                  No    Diarrhea Only\n298                  No       No Disease\n299                  No    Diarrhea Only\n300                  No       No Disease\n301                 Yes Respiratory Only\n302                 Yes       No Disease\n303                  No    Diarrhea Only\n304                 Yes Respiratory Only\n305                  No    Diarrhea Only\n306                 Yes       No Disease\n307                 Yes Respiratory Only\n308                 Yes Respiratory Only\n309                  No       No Disease\n310                 Yes       No Disease\n311                 Yes Respiratory Only\n312                  No    Diarrhea Only\n313                 Yes Respiratory Only\n314                  No       No Disease\n315                  No    Diarrhea Only\n316                 Yes       No Disease\n317                  No    Diarrhea Only\n318                  No       No Disease\n319                  No    Diarrhea Only\n320                 Yes Respiratory Only\n321                  No       No Disease\n322                 Yes Respiratory Only\n323                 Yes Respiratory Only\n324                  No    Diarrhea Only\n325                  No    Diarrhea Only\n326                 Yes Respiratory Only\n327                  No    Diarrhea Only\n328                  No    Diarrhea Only\n329                 Yes Respiratory Only\n330                  No    Diarrhea Only\n\n\nWe want to changed the categorical variables (most of our variables) to factors\n\ncattle_data$Vaccination &lt;- as.factor(cattle_data$Vaccination)\ncattle_data$History_Diarrhea &lt;- as.factor(cattle_data$History_Diarrhea)\ncattle_data$History_Respiratory &lt;- as.factor(cattle_data$History_Respiratory)\ncattle_data$Breed &lt;- as.factor(cattle_data$Breed)\n\nWe will now use logistic regression models to evaluate our data and hopefully see the patterns that we expect. We will also produce some plots to evaluate the data.\nFirst model: geometric logistic regression to test the association between vaccination and disease history (our main association of interest)\n\nmodel1 &lt;- glm(Vaccination ~ Disease_History + Breed, data = cattle_data, family = binomial)\nsummary(model1)\n\n\nCall:\nglm(formula = Vaccination ~ Disease_History + Breed, family = binomial, \n    data = cattle_data)\n\nCoefficients:\n                                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       1.7733     0.2925   6.062 1.35e-09 ***\nDisease_HistoryRespiratory Only  -2.4618     0.3363  -7.321 2.47e-13 ***\nDisease_HistoryDiarrhea Only     -2.2069     0.3193  -6.911 4.82e-12 ***\nBreedHolstein                    -0.3552     0.2543  -1.397    0.162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 457.47  on 329  degrees of freedom\nResidual deviance: 376.50  on 326  degrees of freedom\nAIC: 384.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nFrom the results of this model we can see that the absence of vaccination is strongly and statistically associated with the presence of both respiratory and diarrheal disease in caatle in the dataset. However there is not a statistically significant association/effect between breed and vaccination.\nSecond model: geometric logistic regression to test the effect of breed on respiratory disease (we expect beef breeds to have a higher incidence than dairy)\nLogistic regression testing the effect of breed on disease history. We would need to use a multinomial regression here if we want to test disease history with both respiratory and diarrheal disease in the same model, so instead we will evaluate diarrheal disease and respiratory disease separately using binomial models.\nSecond model: Geometric logistic regression testing the effect of breed on respiratory disease history (we expect respiratory disease to be slightly higher in beef breeds).\n\nmodel2 &lt;- glm(History_Respiratory ~ Breed + Age, data = cattle_data, family = binomial)\nsummary(model2)\n\n\nCall:\nglm(formula = History_Respiratory ~ Breed + Age, family = binomial, \n    data = cattle_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)    0.08072    0.33356   0.242   0.8088  \nBreedHolstein -0.44122    0.22784  -1.937   0.0528 .\nAge           -0.01319    0.01319  -1.000   0.3172  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 443.36  on 329  degrees of freedom\nResidual deviance: 438.85  on 327  degrees of freedom\nAIC: 444.85\n\nNumber of Fisher Scoring iterations: 4\n\n\nThird model: geometric logistic regression to test the effect of breed on diarrhea disease (we expect dairy breeds to have a higher incidence than beef)\n\nmodel3 &lt;- glm(History_Diarrhea ~ Breed + Age, data = cattle_data, family = binomial)\nsummary(model3)\n\n\nCall:\nglm(formula = History_Diarrhea ~ Breed + Age, family = binomial, \n    data = cattle_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   -0.79869    0.33562  -2.380   0.0173 *\nBreedHolstein  0.24110    0.22365   1.078   0.2810  \nAge            0.02044    0.01304   1.568   0.1169  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 453.09  on 329  degrees of freedom\nResidual deviance: 449.69  on 327  degrees of freedom\nAIC: 455.69\n\nNumber of Fisher Scoring iterations: 4\n\n\nFourth model: linear logistic regression to test age differences across vaccination status (we do not expect correlation in our dataset)\n\nmodel4 &lt;- lm(Age ~ Vaccination, data = cattle_data)\nsummary(model4)\n\n\nCall:\nlm(formula = Age ~ Vaccination, data = cattle_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.1402  -7.5482   0.1558   7.8598  14.4518 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     21.5482     0.6701  32.155   &lt;2e-16 ***\nVaccinationYes   0.5921     0.9506   0.623    0.534    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.634 on 328 degrees of freedom\nMultiple R-squared:  0.001181,  Adjusted R-squared:  -0.001864 \nF-statistic: 0.3879 on 1 and 328 DF,  p-value: 0.5338\n\n\nFifth model: linear logistic regression to test age differences across disease history (we do not expect correlation in our dataset)\n\nmodel5 &lt;- lm(Age ~ Disease_History, data = cattle_data)\nsummary(model5)\n\n\nCall:\nlm(formula = Age ~ Disease_History, data = cattle_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.4615  -7.4615   0.2255   7.5385  15.2255 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      22.1712     0.8183  27.093   &lt;2e-16 ***\nDisease_HistoryRespiratory Only  -1.3967     1.1825  -1.181    0.238    \nDisease_HistoryDiarrhea Only      0.2904     1.1424   0.254    0.800    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.622 on 327 degrees of freedom\nMultiple R-squared:  0.007074,  Adjusted R-squared:  0.001001 \nF-statistic: 1.165 on 2 and 327 DF,  p-value: 0.3133\n\n\nPlotting our graphs to see how they compare with our associations seen in the linear regression models\nFirst plot: Violin plot to compare disease history with the vaccination status per breed\n\np1 &lt;- ggplot(cattle_data, aes(x = Disease_History, y = Vaccination, fill = Breed)) +\n  geom_violin() +\n  labs(x = \"Disease History\", y = \"Vaccination Status\", fill = \"Breed\") +\n  theme_minimal()\n\nprint(p1)\n\n\n\n\n\n\n\n\nThis violin plot doesn’t really display this data well. We will produce a bar plot instead.\nGenerate a bar plot to evaluate the relationship between vaccination status and disease history by comparing the proportions of animals with history of disease and vaccination status. We are working with proportion instead of count to make interpretation easier.\n\np2 &lt;- ggplot(cattle_data, aes(x = Disease_History, fill = Vaccination)) +\n    geom_bar(position = \"fill\") +  \n    labs(x = \"Disease History\", y = \"Proportion\", fill = \"Vaccination Status\") +\n    theme_minimal()\n\nprint(p2)\n\n\n\n\n\n\n\n\nThird plot: Scatter plot of age vs vaccination status\n\np3 &lt;- ggplot(cattle_data, aes(x = Vaccination, y = Age, color = Vaccination)) +\n  geom_jitter(width = 0.2, alpha = 0.6) +\n  labs(x = \"Vaccination Status\", y = \"Age (Months)\") +\n  theme_minimal()\n\nprint(p3)\n\n\n\n\n\n\n\n\nVaccination status does not show any obvious relationship with age.\nFourth plot: Scatter plot of age vs History of infection (respiratory and diarrheal)\n\np4 &lt;- ggplot(cattle_data, aes(x = Disease_History, y = Age, color = Disease_History)) +\n  geom_jitter(width = 0.2, alpha = 0.6) +\n  labs(x = \"History of Disease\", y = \"Age (Months)\", color = \"Disease History\") +\n  theme_minimal()\n\nprint(p4)\n\n\n\n\n\n\n\n\nFifth plot: Bar plot comparing disease history across all breeds\n\np5 &lt;- ggplot(cattle_data, aes(x = Breed, fill = Disease_History)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Breed\", y = \"Count\", fill = \"History of Disease\") +\n  theme_minimal()\n\nprint(p5)\n\n\n\n\n\n\n\n\nFrom the graphs we once again see that there is no obvious relationship between disease status and age (as we created in the synthetic data, which might differ from real life evaluations). We do see that there is slightly higher incidence of respiratory disease in beef animals, and diarrhea disease in dairy animals.\nIn summary, our models used for evalaution work well and we produce the expected results from this synthetic dataset. There are some aspects of the synthetic dataset that could be worked on further to make it more closely resemble and model a real-world situation, and then we might need more evaluation. However, as a proof of concept this is helpful at this stage."
  },
  {
    "objectID": "data-exercise/data-exercise.qmd.html#data-exercise-to-generate-synthetic-data.",
    "href": "data-exercise/data-exercise.qmd.html#data-exercise-to-generate-synthetic-data.",
    "title": "Data Exercise",
    "section": "",
    "text": "Load the required packages:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nWe are looking to generate a synthetic dataset that evaluates the relationship between vaccination status and history of disease within and to compare thi between different cattle breeds (beef vs dairy breeds). We are going to generate a dataset with 330 individual animals from the age of 6 months to 3 years. Since females are found in higher numbers in cattle production systems we are generating the dataset with 70% females (30% males). We are looking at vaccination as yes or no without specifying specific diseases or vaccines. We will generate columns for history of respiratory disease and history of diarrheal disease (either yes or no).\nWe first need to set the seed to that our random generation of variables is repeatable\n\nset.seed(123)\n\nWe will start to generate our dataset as described:\n\n#Start by assigning the number of cattle (observations)\nn &lt;- 330\n\n#We will assign ages to the animals between 6 months and 3 years (18 months). Age can only be integers (whole numbers in months)\nage &lt;- sample(6:36, n, replace = TRUE)\n\n#we will assign the sex with 70% as females as mentioned\nsex &lt;- sample(c(\"Female\", \"Male\"), n, replace= TRUE, prob = c(0.7, 0.3))\n\n#We will assign the cattle breeds with an equal split using Angus as the beef breed and Holstein as the dairy breed\nbreed &lt;- sample(c(\"Angus\", \"Holstein\"), n, replace = TRUE)\n\n#We will classify these breeds to help with further interpretation and analysis downstream. This would be particularly helpful if we had multiple different breeds for each category in the dataset. We will use the if else function in R.\nbreed_type &lt;- ifelse(breed==\"Angus\", \"Beef\", \"Dairy\")\n\n#Double check that our breed data worked\nView(breed_type)\nprint(breed_type)\n\n  [1] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\"\n [10] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\"\n [19] \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\"\n [28] \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\" \n [37] \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\"\n [46] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\"\n [55] \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\" \n [64] \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\"\n [73] \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\"\n [82] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\"\n [91] \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\"\n[100] \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[109] \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\"\n[118] \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[127] \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[136] \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\"\n[145] \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\" \n[154] \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\"\n[163] \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\" \n[172] \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[181] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\" \n[190] \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\"\n[199] \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[208] \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\" \n[217] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[226] \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[235] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[244] \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\" \n[253] \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\"\n[262] \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\"\n[271] \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[280] \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Beef\" \n[289] \"Beef\"  \"Beef\"  \"Beef\"  \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\" \n[298] \"Beef\"  \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Beef\"  \"Beef\"  \"Dairy\"\n[307] \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\" \n[316] \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\"  \"Dairy\" \"Beef\" \n[325] \"Beef\"  \"Dairy\" \"Dairy\" \"Dairy\" \"Dairy\" \"Beef\" \n\n\nWe must now generate the vaccination and disease history data before we can produce our dataframe.\nWe will make the incidence of respiratory disease slightly higher in beef animals than in dairy animals, and the opposite (slightly higher incidence of diarrheal disease in dairy animals than beef animals).\n\n#We will assign vaccination status (yes/no) randomly\nvaccination &lt;- sample (c(\"Yes\",\"No\"), n, replace = TRUE)\n\n#Assign history of diseases based on our assumptions\n#Start by generating a vector for the history of disease - diarrhea or respiratory\nhistory_diarrhea &lt;- rep(\"No\", n)\nhistory_respiratory &lt;- rep(\"No\", n)\n\n#Using a loop function to change the incidence of the different diseases in beef and dairy breeds as mentioned above\n#I had chatGPT help me to generate the code for this\n\nfor (i in 1:n) {\n  if (breed[i] == \"Holstein\") {\n    if (runif(1) &lt; 0.35) history_diarrhea[i] &lt;- \"Yes\"\n    if (runif(1) &lt; 0.25) history_respiratory[i] &lt;- \"Yes\"\n  } else {\n    if (runif(1) &lt; 0.30) history_diarrhea[i] &lt;- \"Yes\"\n    if (runif(1) &lt; 0.35) history_respiratory[i] &lt;- \"Yes\"\n  }\n}\n\n#We are applying the conditions above if the breed is Holstein and if it is not then we apply the conditions below the else.\n\n\n#We now want to make sure that animals that are not vaccinated have a higher incidence of disease (vaccination is correlated with freedom from or reduced incidence of disease)\n#We will use another loop funciton for this.\n# I had ChatGPT assist with this code for the loop\n\nfor (i in 1:n) {\n  if (vaccination[i] == \"No\" && history_diarrhea[i] == \"No\" && history_respiratory[i] == \"No\") {\n    if (runif(1) &lt; 0.5) {\n      history_diarrhea[i] &lt;- \"Yes\"\n    } else {\n      history_respiratory[i] &lt;- \"Yes\"\n    }\n  }\n}\n\nWe want to make four different categories that will help us to evaluate the different disease states that are possible. Different states are “Respiratory disease”, “Diarrhea disease”, “Both diseases”, and “No disease”. We will create an additional column for this to make it easier for us to call back to this for later models and plots. This could have been done as a second or separate dataframe without including it in the main datframe if we preferred. For ease it has been done now for less separate steps.\nCreate disease history classification:\n\ndisease_history &lt;- case_when(history_respiratory == \"Yes\" & history_diarrhea == \"No\" ~ \"Respiratory Only\", history_respiratory == \"No\" & history_diarrhea == \"Yes\" ~ \"Diarrhea Only\", history_respiratory == \"yes\" & history_diarrhea == \"Yes\" ~ \"Both Diseases\", TRUE ~ \"No Disease\")\n\nWe then create a dataframe with all of these variables that we have assigned. We will call this cattle_data. We change the font of the column names to help use define searches and functions more directly.\n\ncattle_data &lt;- data.frame(Age = age, Sex = sex, Breed = breed, Breed_Type = breed_type, Vaccination = vaccination, History_Diarrhea = history_diarrhea, History_Respiratory = history_respiratory, Disease_History = factor(disease_history, levels = c(\"No Disease\", \"Respiratory Only\", \"Diarrhea Only\", \"Both Diseases\")))\n\nsummary(cattle_data)\n\n      Age            Sex               Breed            Breed_Type       \n Min.   : 6.00   Length:330         Length:330         Length:330        \n 1st Qu.:14.00   Class :character   Class :character   Class :character  \n Median :22.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :21.84                                                           \n 3rd Qu.:30.00                                                           \n Max.   :36.00                                                           \n Vaccination        History_Diarrhea   History_Respiratory\n Length:330         Length:330         Length:330         \n Class :character   Class :character   Class :character   \n Mode  :character   Mode  :character   Mode  :character   \n                                                          \n                                                          \n                                                          \n         Disease_History\n No Disease      :111   \n Respiratory Only:102   \n Diarrhea Only   :117   \n Both Diseases   :  0   \n                        \n                        \n\nprint(cattle_data)\n\n    Age    Sex    Breed Breed_Type Vaccination History_Diarrhea\n1    36 Female    Angus       Beef          No              Yes\n2    20 Female    Angus       Beef          No               No\n3    24 Female    Angus       Beef         Yes              Yes\n4    19 Female Holstein      Dairy          No              Yes\n5     8 Female Holstein      Dairy         Yes               No\n6    15   Male Holstein      Dairy         Yes               No\n7    23 Female Holstein      Dairy          No              Yes\n8    27 Female    Angus       Beef          No               No\n9    16 Female Holstein      Dairy         Yes               No\n10   10 Female    Angus       Beef         Yes               No\n11   25   Male    Angus       Beef          No              Yes\n12   19 Female    Angus       Beef          No              Yes\n13   27 Female Holstein      Dairy         Yes               No\n14   30 Female    Angus       Beef          No               No\n15   31   Male Holstein      Dairy         Yes               No\n16   32 Female Holstein      Dairy         Yes              Yes\n17   10 Female    Angus       Beef         Yes               No\n18   24 Female Holstein      Dairy          No              Yes\n19   32   Male    Angus       Beef          No               No\n20   30 Female Holstein      Dairy         Yes              Yes\n21   33 Female    Angus       Beef         Yes              Yes\n22   30   Male    Angus       Beef          No              Yes\n23   14 Female Holstein      Dairy         Yes               No\n24   34 Female Holstein      Dairy          No               No\n25    8   Male Holstein      Dairy          No               No\n26   13 Female Holstein      Dairy          No              Yes\n27   31 Female Holstein      Dairy         Yes              Yes\n28   12 Female Holstein      Dairy         Yes              Yes\n29   15 Female Holstein      Dairy         Yes               No\n30   14 Female Holstein      Dairy          No              Yes\n31   35 Female    Angus       Beef          No               No\n32   24   Male    Angus       Beef          No              Yes\n33    9 Female    Angus       Beef         Yes               No\n34   19 Female Holstein      Dairy          No               No\n35   22   Male    Angus       Beef          No              Yes\n36   16   Male    Angus       Beef         Yes              Yes\n37   12 Female    Angus       Beef         Yes               No\n38   26 Female Holstein      Dairy         Yes               No\n39   17   Male Holstein      Dairy         Yes               No\n40   20 Female    Angus       Beef         Yes               No\n41   15   Male Holstein      Dairy         Yes               No\n42   18 Female    Angus       Beef         Yes              Yes\n43   12   Male Holstein      Dairy          No              Yes\n44   14 Female    Angus       Beef          No              Yes\n45   14   Male Holstein      Dairy          No              Yes\n46   15 Female Holstein      Dairy          No              Yes\n47   28 Female Holstein      Dairy          No              Yes\n48   32 Female    Angus       Beef         Yes              Yes\n49   33 Female    Angus       Beef         Yes               No\n50   26   Male Holstein      Dairy          No               No\n51   12 Female Holstein      Dairy          No              Yes\n52   26   Male Holstein      Dairy          No              Yes\n53   32   Male Holstein      Dairy         Yes               No\n54   11 Female Holstein      Dairy          No               No\n55   30 Female    Angus       Beef         Yes               No\n56    7 Female    Angus       Beef          No               No\n57   34 Female Holstein      Dairy          No              Yes\n58   10 Female    Angus       Beef          No               No\n59   13   Male Holstein      Dairy          No              Yes\n60   17   Male    Angus       Beef          No              Yes\n61   36 Female    Angus       Beef         Yes               No\n62   18   Male    Angus       Beef         Yes              Yes\n63   23 Female    Angus       Beef         Yes               No\n64    6 Female    Angus       Beef          No              Yes\n65   30 Female Holstein      Dairy          No              Yes\n66   32   Male Holstein      Dairy          No              Yes\n67   30 Female    Angus       Beef         Yes              Yes\n68   11 Female Holstein      Dairy         Yes               No\n69   26 Female    Angus       Beef         Yes              Yes\n70   20 Female    Angus       Beef          No               No\n71   14   Male    Angus       Beef         Yes              Yes\n72   20 Female Holstein      Dairy          No              Yes\n73   31 Female    Angus       Beef         Yes               No\n74   33 Female    Angus       Beef         Yes               No\n75   36 Female    Angus       Beef          No               No\n76   21   Male    Angus       Beef          No              Yes\n77   25 Female Holstein      Dairy         Yes               No\n78   35 Female    Angus       Beef         Yes              Yes\n79   11 Female Holstein      Dairy          No               No\n80   16 Female Holstein      Dairy          No               No\n81   13 Female Holstein      Dairy         Yes              Yes\n82   27 Female    Angus       Beef         Yes               No\n83   27 Female    Angus       Beef         Yes               No\n84   12   Male    Angus       Beef          No              Yes\n85   36 Female Holstein      Dairy         Yes               No\n86   21 Female Holstein      Dairy         Yes               No\n87   22 Female    Angus       Beef         Yes              Yes\n88   27 Female Holstein      Dairy         Yes               No\n89   23   Male    Angus       Beef          No               No\n90   22   Male Holstein      Dairy         Yes               No\n91    7 Female    Angus       Beef          No              Yes\n92    9 Female Holstein      Dairy         Yes              Yes\n93   18   Male Holstein      Dairy          No               No\n94   10 Female Holstein      Dairy         Yes               No\n95   27 Female Holstein      Dairy          No              Yes\n96   36 Female Holstein      Dairy          No              Yes\n97   24 Female Holstein      Dairy         Yes               No\n98   30 Female Holstein      Dairy          No              Yes\n99   25 Female Holstein      Dairy          No               No\n100  27 Female    Angus       Beef         Yes              Yes\n101  30 Female    Angus       Beef         Yes               No\n102  19   Male Holstein      Dairy         Yes               No\n103  30 Female    Angus       Beef         Yes              Yes\n104  28   Male    Angus       Beef          No              Yes\n105   8   Male Holstein      Dairy          No               No\n106  13   Male    Angus       Beef         Yes               No\n107  21 Female    Angus       Beef         Yes               No\n108  35 Female Holstein      Dairy         Yes              Yes\n109  17 Female Holstein      Dairy         Yes               No\n110  36 Female    Angus       Beef          No              Yes\n111  30 Female    Angus       Beef         Yes              Yes\n112  19 Female    Angus       Beef          No              Yes\n113  35 Female    Angus       Beef          No              Yes\n114   8 Female    Angus       Beef          No              Yes\n115  19   Male    Angus       Beef         Yes               No\n116  34   Male Holstein      Dairy          No              Yes\n117  12   Male Holstein      Dairy          No              Yes\n118   8 Female Holstein      Dairy          No              Yes\n119  28 Female Holstein      Dairy          No               No\n120  27   Male    Angus       Beef         Yes               No\n121  31 Female Holstein      Dairy          No              Yes\n122  20 Female    Angus       Beef         Yes               No\n123  26 Female Holstein      Dairy         Yes               No\n124  10 Female    Angus       Beef          No               No\n125  13 Female    Angus       Beef          No               No\n126  24 Female    Angus       Beef         Yes              Yes\n127  15 Female Holstein      Dairy         Yes              Yes\n128  23 Female Holstein      Dairy         Yes               No\n129  15   Male Holstein      Dairy         Yes               No\n130  17 Female    Angus       Beef         Yes               No\n131   7   Male    Angus       Beef         Yes               No\n132  15 Female Holstein      Dairy         Yes               No\n133  27   Male Holstein      Dairy          No              Yes\n134  36 Female    Angus       Beef          No              Yes\n135  17 Female    Angus       Beef         Yes              Yes\n136  25 Female    Angus       Beef         Yes              Yes\n137  19 Female    Angus       Beef          No              Yes\n138  22 Female    Angus       Beef         Yes              Yes\n139  35   Male    Angus       Beef         Yes              Yes\n140  19 Female    Angus       Beef         Yes               No\n141  27   Male    Angus       Beef          No               No\n142   8 Female    Angus       Beef         Yes               No\n143  13 Female    Angus       Beef          No              Yes\n144  19   Male Holstein      Dairy          No              Yes\n145  24 Female    Angus       Beef          No              Yes\n146  35 Female Holstein      Dairy          No              Yes\n147  20 Female    Angus       Beef          No              Yes\n148  29 Female Holstein      Dairy          No              Yes\n149  22   Male Holstein      Dairy          No              Yes\n150  28   Male    Angus       Beef         Yes               No\n151  16 Female Holstein      Dairy          No               No\n152  12 Female Holstein      Dairy         Yes               No\n153  34   Male    Angus       Beef         Yes               No\n154  20 Female    Angus       Beef          No              Yes\n155  28   Male Holstein      Dairy          No               No\n156  31 Female Holstein      Dairy         Yes               No\n157  11 Female    Angus       Beef          No               No\n158  19 Female Holstein      Dairy         Yes               No\n159  36   Male    Angus       Beef         Yes               No\n160  12 Female Holstein      Dairy          No              Yes\n161  32 Female Holstein      Dairy          No              Yes\n162  15 Female Holstein      Dairy         Yes               No\n163  10 Female    Angus       Beef          No               No\n164  11 Female    Angus       Beef         Yes               No\n165  21 Female Holstein      Dairy          No               No\n166  29 Female Holstein      Dairy         Yes              Yes\n167  26 Female Holstein      Dairy         Yes               No\n168  28   Male    Angus       Beef          No               No\n169  16 Female    Angus       Beef          No               No\n170   9 Female Holstein      Dairy          No              Yes\n171  17 Female    Angus       Beef          No               No\n172  19   Male Holstein      Dairy          No               No\n173  33 Female Holstein      Dairy          No               No\n174  34   Male Holstein      Dairy          No              Yes\n175  24 Female Holstein      Dairy          No              Yes\n176  30 Female    Angus       Beef          No              Yes\n177  12   Male Holstein      Dairy          No              Yes\n178  27 Female Holstein      Dairy         Yes              Yes\n179  31   Male    Angus       Beef         Yes               No\n180  14 Female    Angus       Beef         Yes               No\n181  12 Female Holstein      Dairy         Yes              Yes\n182   7 Female Holstein      Dairy         Yes               No\n183  21 Female    Angus       Beef         Yes               No\n184  18 Female    Angus       Beef         Yes               No\n185  24 Female    Angus       Beef          No              Yes\n186  29   Male    Angus       Beef          No              Yes\n187  25 Female    Angus       Beef          No               No\n188  20   Male    Angus       Beef         Yes               No\n189  12 Female    Angus       Beef         Yes               No\n190   9   Male    Angus       Beef          No               No\n191   6 Female    Angus       Beef          No              Yes\n192  13 Female    Angus       Beef         Yes               No\n193  35   Male Holstein      Dairy          No               No\n194  35 Female    Angus       Beef         Yes               No\n195  25   Male    Angus       Beef         Yes              Yes\n196  30 Female    Angus       Beef         Yes               No\n197  21   Male Holstein      Dairy          No              Yes\n198  29 Female Holstein      Dairy         Yes               No\n199  27 Female Holstein      Dairy         Yes               No\n200  16   Male Holstein      Dairy         Yes              Yes\n201  21 Female Holstein      Dairy          No               No\n202  25   Male Holstein      Dairy          No               No\n203  13 Female Holstein      Dairy         Yes               No\n204   8   Male Holstein      Dairy         Yes               No\n205  34   Male    Angus       Beef         Yes               No\n206   9 Female    Angus       Beef          No               No\n207  25   Male Holstein      Dairy         Yes              Yes\n208  17   Male Holstein      Dairy         Yes              Yes\n209  27 Female    Angus       Beef         Yes               No\n210  22 Female Holstein      Dairy         Yes               No\n211  15 Female    Angus       Beef         Yes               No\n212  32 Female    Angus       Beef          No               No\n213  25   Male    Angus       Beef         Yes               No\n214  16 Female    Angus       Beef          No              Yes\n215  30 Female    Angus       Beef         Yes               No\n216  28 Female    Angus       Beef         Yes              Yes\n217  13 Female Holstein      Dairy          No              Yes\n218  19 Female Holstein      Dairy          No              Yes\n219  26 Female    Angus       Beef          No               No\n220  18 Female    Angus       Beef          No               No\n221   7   Male Holstein      Dairy          No               No\n222  16   Male Holstein      Dairy          No               No\n223  18 Female    Angus       Beef          No               No\n224  19 Female    Angus       Beef         Yes              Yes\n225  11 Female    Angus       Beef         Yes               No\n226  30 Female Holstein      Dairy          No              Yes\n227  13   Male    Angus       Beef          No               No\n228  17 Female    Angus       Beef         Yes              Yes\n229  31   Male Holstein      Dairy          No               No\n230   9 Female Holstein      Dairy          No               No\n231  18   Male Holstein      Dairy         Yes              Yes\n232  34 Female    Angus       Beef         Yes               No\n233  19 Female    Angus       Beef          No              Yes\n234  26   Male Holstein      Dairy          No              Yes\n235  21   Male Holstein      Dairy          No               No\n236  28 Female Holstein      Dairy          No               No\n237  34 Female    Angus       Beef          No              Yes\n238   6 Female    Angus       Beef          No              Yes\n239  13 Female Holstein      Dairy         Yes               No\n240  13   Male Holstein      Dairy         Yes               No\n241  15   Male    Angus       Beef          No               No\n242  30   Male    Angus       Beef          No              Yes\n243  13 Female    Angus       Beef          No               No\n244  23   Male Holstein      Dairy         Yes               No\n245  34 Female Holstein      Dairy          No              Yes\n246  26 Female    Angus       Beef         Yes               No\n247  14 Female    Angus       Beef         Yes               No\n248  12   Male Holstein      Dairy         Yes               No\n249  12 Female    Angus       Beef         Yes               No\n250  31 Female    Angus       Beef          No              Yes\n251  34 Female Holstein      Dairy         Yes               No\n252  33   Male    Angus       Beef         Yes              Yes\n253  15 Female Holstein      Dairy          No              Yes\n254  29 Female    Angus       Beef         Yes               No\n255  36   Male    Angus       Beef         Yes               No\n256  27 Female Holstein      Dairy         Yes               No\n257  28 Female Holstein      Dairy         Yes               No\n258  31   Male    Angus       Beef         Yes              Yes\n259  16 Female Holstein      Dairy         Yes              Yes\n260   6 Female Holstein      Dairy         Yes              Yes\n261  30 Female Holstein      Dairy          No               No\n262  34 Female Holstein      Dairy          No              Yes\n263  34 Female Holstein      Dairy         Yes               No\n264  24 Female Holstein      Dairy         Yes              Yes\n265  15   Male Holstein      Dairy         Yes               No\n266  26 Female    Angus       Beef         Yes               No\n267  27   Male    Angus       Beef         Yes               No\n268  18 Female Holstein      Dairy         Yes              Yes\n269  16 Female    Angus       Beef          No              Yes\n270  16 Female Holstein      Dairy         Yes               No\n271  30 Female Holstein      Dairy         Yes               No\n272  25 Female    Angus       Beef         Yes               No\n273  31   Male    Angus       Beef          No               No\n274  12 Female Holstein      Dairy          No              Yes\n275  30   Male    Angus       Beef          No               No\n276  28   Male Holstein      Dairy         Yes              Yes\n277  31   Male Holstein      Dairy          No              Yes\n278  25   Male    Angus       Beef          No               No\n279  34 Female    Angus       Beef         Yes               No\n280  29   Male    Angus       Beef         Yes               No\n281  30   Male Holstein      Dairy          No               No\n282  14   Male    Angus       Beef          No               No\n283  14 Female Holstein      Dairy          No              Yes\n284  10 Female Holstein      Dairy          No               No\n285  28 Female Holstein      Dairy          No               No\n286  19 Female    Angus       Beef          No              Yes\n287  19   Male    Angus       Beef          No               No\n288  11 Female    Angus       Beef          No               No\n289  32 Female    Angus       Beef         Yes               No\n290   6   Male    Angus       Beef         Yes               No\n291  31   Male    Angus       Beef          No              Yes\n292  15 Female    Angus       Beef         Yes               No\n293  22   Male Holstein      Dairy          No              Yes\n294  31 Female Holstein      Dairy         Yes               No\n295  22 Female Holstein      Dairy         Yes              Yes\n296  34   Male    Angus       Beef          No              Yes\n297  31   Male    Angus       Beef          No              Yes\n298  32   Male    Angus       Beef         Yes               No\n299  26 Female    Angus       Beef          No              Yes\n300  12 Female Holstein      Dairy         Yes               No\n301  33   Male    Angus       Beef         Yes               No\n302  26   Male Holstein      Dairy          No              Yes\n303  31 Female Holstein      Dairy          No              Yes\n304  14 Female    Angus       Beef          No               No\n305  25 Female    Angus       Beef          No              Yes\n306  11   Male Holstein      Dairy          No              Yes\n307  23 Female    Angus       Beef          No               No\n308  35 Female Holstein      Dairy         Yes               No\n309  36 Female    Angus       Beef         Yes               No\n310  34   Male Holstein      Dairy          No              Yes\n311  22   Male Holstein      Dairy         Yes               No\n312  10 Female Holstein      Dairy         Yes              Yes\n313  30 Female    Angus       Beef          No               No\n314  25   Male Holstein      Dairy         Yes               No\n315   8   Male    Angus       Beef         Yes              Yes\n316  30   Male    Angus       Beef          No              Yes\n317  19 Female Holstein      Dairy         Yes              Yes\n318   6 Female Holstein      Dairy         Yes               No\n319   7 Female Holstein      Dairy          No              Yes\n320   9   Male    Angus       Beef          No               No\n321  15   Male Holstein      Dairy         Yes               No\n322   6 Female    Angus       Beef          No               No\n323  10   Male Holstein      Dairy          No               No\n324  26 Female    Angus       Beef          No              Yes\n325  30 Female    Angus       Beef         Yes              Yes\n326  13   Male Holstein      Dairy          No               No\n327  28 Female Holstein      Dairy          No              Yes\n328  30 Female Holstein      Dairy          No              Yes\n329  26 Female Holstein      Dairy          No               No\n330  18 Female    Angus       Beef         Yes              Yes\n    History_Respiratory  Disease_History\n1                    No    Diarrhea Only\n2                   Yes Respiratory Only\n3                    No    Diarrhea Only\n4                    No    Diarrhea Only\n5                    No       No Disease\n6                    No       No Disease\n7                    No    Diarrhea Only\n8                   Yes Respiratory Only\n9                    No       No Disease\n10                  Yes Respiratory Only\n11                  Yes       No Disease\n12                  Yes       No Disease\n13                  Yes Respiratory Only\n14                  Yes Respiratory Only\n15                   No       No Disease\n16                   No    Diarrhea Only\n17                  Yes Respiratory Only\n18                  Yes       No Disease\n19                  Yes Respiratory Only\n20                   No    Diarrhea Only\n21                   No    Diarrhea Only\n22                   No    Diarrhea Only\n23                   No       No Disease\n24                  Yes Respiratory Only\n25                  Yes Respiratory Only\n26                   No    Diarrhea Only\n27                   No    Diarrhea Only\n28                  Yes       No Disease\n29                  Yes Respiratory Only\n30                   No    Diarrhea Only\n31                  Yes Respiratory Only\n32                   No    Diarrhea Only\n33                   No       No Disease\n34                  Yes Respiratory Only\n35                  Yes       No Disease\n36                  Yes       No Disease\n37                  Yes Respiratory Only\n38                   No       No Disease\n39                  Yes Respiratory Only\n40                   No       No Disease\n41                   No       No Disease\n42                  Yes       No Disease\n43                   No    Diarrhea Only\n44                   No    Diarrhea Only\n45                   No    Diarrhea Only\n46                   No    Diarrhea Only\n47                   No    Diarrhea Only\n48                   No    Diarrhea Only\n49                   No       No Disease\n50                  Yes Respiratory Only\n51                   No    Diarrhea Only\n52                   No    Diarrhea Only\n53                   No       No Disease\n54                  Yes Respiratory Only\n55                  Yes Respiratory Only\n56                  Yes Respiratory Only\n57                   No    Diarrhea Only\n58                  Yes Respiratory Only\n59                   No    Diarrhea Only\n60                   No    Diarrhea Only\n61                   No       No Disease\n62                   No    Diarrhea Only\n63                  Yes Respiratory Only\n64                   No    Diarrhea Only\n65                   No    Diarrhea Only\n66                   No    Diarrhea Only\n67                  Yes       No Disease\n68                   No       No Disease\n69                   No    Diarrhea Only\n70                  Yes Respiratory Only\n71                   No    Diarrhea Only\n72                   No    Diarrhea Only\n73                   No       No Disease\n74                   No       No Disease\n75                  Yes Respiratory Only\n76                   No    Diarrhea Only\n77                   No       No Disease\n78                   No    Diarrhea Only\n79                  Yes Respiratory Only\n80                  Yes Respiratory Only\n81                   No    Diarrhea Only\n82                  Yes Respiratory Only\n83                   No       No Disease\n84                   No    Diarrhea Only\n85                   No       No Disease\n86                   No       No Disease\n87                   No    Diarrhea Only\n88                   No       No Disease\n89                  Yes Respiratory Only\n90                   No       No Disease\n91                   No    Diarrhea Only\n92                   No    Diarrhea Only\n93                  Yes Respiratory Only\n94                   No       No Disease\n95                   No    Diarrhea Only\n96                   No    Diarrhea Only\n97                   No       No Disease\n98                  Yes       No Disease\n99                  Yes Respiratory Only\n100                  No    Diarrhea Only\n101                  No       No Disease\n102                  No       No Disease\n103                  No    Diarrhea Only\n104                  No    Diarrhea Only\n105                 Yes Respiratory Only\n106                 Yes Respiratory Only\n107                  No       No Disease\n108                 Yes       No Disease\n109                  No       No Disease\n110                  No    Diarrhea Only\n111                  No    Diarrhea Only\n112                  No    Diarrhea Only\n113                  No    Diarrhea Only\n114                  No    Diarrhea Only\n115                  No       No Disease\n116                  No    Diarrhea Only\n117                  No    Diarrhea Only\n118                 Yes       No Disease\n119                 Yes Respiratory Only\n120                 Yes Respiratory Only\n121                  No    Diarrhea Only\n122                 Yes Respiratory Only\n123                 Yes Respiratory Only\n124                 Yes Respiratory Only\n125                 Yes Respiratory Only\n126                 Yes       No Disease\n127                  No    Diarrhea Only\n128                  No       No Disease\n129                  No       No Disease\n130                  No       No Disease\n131                  No       No Disease\n132                  No       No Disease\n133                  No    Diarrhea Only\n134                  No    Diarrhea Only\n135                  No    Diarrhea Only\n136                 Yes       No Disease\n137                  No    Diarrhea Only\n138                  No    Diarrhea Only\n139                  No    Diarrhea Only\n140                  No       No Disease\n141                 Yes Respiratory Only\n142                  No       No Disease\n143                  No    Diarrhea Only\n144                  No    Diarrhea Only\n145                 Yes       No Disease\n146                 Yes       No Disease\n147                  No    Diarrhea Only\n148                  No    Diarrhea Only\n149                  No    Diarrhea Only\n150                  No       No Disease\n151                 Yes Respiratory Only\n152                  No       No Disease\n153                 Yes Respiratory Only\n154                 Yes       No Disease\n155                 Yes Respiratory Only\n156                  No       No Disease\n157                 Yes Respiratory Only\n158                  No       No Disease\n159                  No       No Disease\n160                  No    Diarrhea Only\n161                  No    Diarrhea Only\n162                 Yes Respiratory Only\n163                 Yes Respiratory Only\n164                 Yes Respiratory Only\n165                 Yes Respiratory Only\n166                  No    Diarrhea Only\n167                  No       No Disease\n168                 Yes Respiratory Only\n169                 Yes Respiratory Only\n170                  No    Diarrhea Only\n171                 Yes Respiratory Only\n172                 Yes Respiratory Only\n173                 Yes Respiratory Only\n174                  No    Diarrhea Only\n175                 Yes       No Disease\n176                  No    Diarrhea Only\n177                  No    Diarrhea Only\n178                  No    Diarrhea Only\n179                  No       No Disease\n180                  No       No Disease\n181                  No    Diarrhea Only\n182                  No       No Disease\n183                  No       No Disease\n184                  No       No Disease\n185                  No    Diarrhea Only\n186                  No    Diarrhea Only\n187                 Yes Respiratory Only\n188                 Yes Respiratory Only\n189                  No       No Disease\n190                 Yes Respiratory Only\n191                 Yes       No Disease\n192                  No       No Disease\n193                 Yes Respiratory Only\n194                  No       No Disease\n195                  No    Diarrhea Only\n196                  No       No Disease\n197                 Yes       No Disease\n198                 Yes Respiratory Only\n199                  No       No Disease\n200                  No    Diarrhea Only\n201                 Yes Respiratory Only\n202                 Yes Respiratory Only\n203                  No       No Disease\n204                  No       No Disease\n205                 Yes Respiratory Only\n206                 Yes Respiratory Only\n207                 Yes       No Disease\n208                  No    Diarrhea Only\n209                  No       No Disease\n210                  No       No Disease\n211                  No       No Disease\n212                 Yes Respiratory Only\n213                  No       No Disease\n214                  No    Diarrhea Only\n215                  No       No Disease\n216                 Yes       No Disease\n217                  No    Diarrhea Only\n218                  No    Diarrhea Only\n219                 Yes Respiratory Only\n220                 Yes Respiratory Only\n221                 Yes Respiratory Only\n222                 Yes Respiratory Only\n223                 Yes Respiratory Only\n224                  No    Diarrhea Only\n225                 Yes Respiratory Only\n226                  No    Diarrhea Only\n227                 Yes Respiratory Only\n228                  No    Diarrhea Only\n229                 Yes Respiratory Only\n230                 Yes Respiratory Only\n231                  No    Diarrhea Only\n232                  No       No Disease\n233                  No    Diarrhea Only\n234                  No    Diarrhea Only\n235                 Yes Respiratory Only\n236                 Yes Respiratory Only\n237                  No    Diarrhea Only\n238                  No    Diarrhea Only\n239                  No       No Disease\n240                  No       No Disease\n241                 Yes Respiratory Only\n242                  No    Diarrhea Only\n243                 Yes Respiratory Only\n244                  No       No Disease\n245                  No    Diarrhea Only\n246                 Yes Respiratory Only\n247                  No       No Disease\n248                 Yes Respiratory Only\n249                  No       No Disease\n250                  No    Diarrhea Only\n251                  No       No Disease\n252                 Yes       No Disease\n253                  No    Diarrhea Only\n254                  No       No Disease\n255                 Yes Respiratory Only\n256                  No       No Disease\n257                  No       No Disease\n258                  No    Diarrhea Only\n259                  No    Diarrhea Only\n260                  No    Diarrhea Only\n261                 Yes Respiratory Only\n262                  No    Diarrhea Only\n263                  No       No Disease\n264                  No    Diarrhea Only\n265                  No       No Disease\n266                 Yes Respiratory Only\n267                  No       No Disease\n268                  No    Diarrhea Only\n269                  No    Diarrhea Only\n270                  No       No Disease\n271                 Yes Respiratory Only\n272                 Yes Respiratory Only\n273                 Yes Respiratory Only\n274                  No    Diarrhea Only\n275                 Yes Respiratory Only\n276                  No    Diarrhea Only\n277                 Yes       No Disease\n278                 Yes Respiratory Only\n279                 Yes Respiratory Only\n280                  No       No Disease\n281                 Yes Respiratory Only\n282                 Yes Respiratory Only\n283                  No    Diarrhea Only\n284                 Yes Respiratory Only\n285                 Yes Respiratory Only\n286                 Yes       No Disease\n287                 Yes Respiratory Only\n288                 Yes Respiratory Only\n289                  No       No Disease\n290                 Yes Respiratory Only\n291                  No    Diarrhea Only\n292                  No       No Disease\n293                 Yes       No Disease\n294                  No       No Disease\n295                  No    Diarrhea Only\n296                  No    Diarrhea Only\n297                  No    Diarrhea Only\n298                  No       No Disease\n299                  No    Diarrhea Only\n300                  No       No Disease\n301                 Yes Respiratory Only\n302                 Yes       No Disease\n303                  No    Diarrhea Only\n304                 Yes Respiratory Only\n305                  No    Diarrhea Only\n306                 Yes       No Disease\n307                 Yes Respiratory Only\n308                 Yes Respiratory Only\n309                  No       No Disease\n310                 Yes       No Disease\n311                 Yes Respiratory Only\n312                  No    Diarrhea Only\n313                 Yes Respiratory Only\n314                  No       No Disease\n315                  No    Diarrhea Only\n316                 Yes       No Disease\n317                  No    Diarrhea Only\n318                  No       No Disease\n319                  No    Diarrhea Only\n320                 Yes Respiratory Only\n321                  No       No Disease\n322                 Yes Respiratory Only\n323                 Yes Respiratory Only\n324                  No    Diarrhea Only\n325                  No    Diarrhea Only\n326                 Yes Respiratory Only\n327                  No    Diarrhea Only\n328                  No    Diarrhea Only\n329                 Yes Respiratory Only\n330                  No    Diarrhea Only\n\n\nWe want to changed the categorical variables (most of our variables) to factors\n\ncattle_data$Vaccination &lt;- as.factor(cattle_data$Vaccination)\ncattle_data$History_Diarrhea &lt;- as.factor(cattle_data$History_Diarrhea)\ncattle_data$History_Respiratory &lt;- as.factor(cattle_data$History_Respiratory)\ncattle_data$Breed &lt;- as.factor(cattle_data$Breed)\n\nWe will now use logistic regression models to evaluate our data and hopefully see the patterns that we expect. We will also produce some plots to evaluate the data.\nFirst model: geometric logistic regression to test the association between vaccination and disease history (our main association of interest)\n\nmodel1 &lt;- glm(Vaccination ~ Disease_History + Breed, data = cattle_data, family = binomial)\nsummary(model1)\n\n\nCall:\nglm(formula = Vaccination ~ Disease_History + Breed, family = binomial, \n    data = cattle_data)\n\nCoefficients:\n                                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       1.7733     0.2925   6.062 1.35e-09 ***\nDisease_HistoryRespiratory Only  -2.4618     0.3363  -7.321 2.47e-13 ***\nDisease_HistoryDiarrhea Only     -2.2069     0.3193  -6.911 4.82e-12 ***\nBreedHolstein                    -0.3552     0.2543  -1.397    0.162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 457.47  on 329  degrees of freedom\nResidual deviance: 376.50  on 326  degrees of freedom\nAIC: 384.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nFrom the results of this model we can see that the absence of vaccination is strongly and statistically associated with the presence of both respiratory and diarrheal disease in caatle in the dataset. However there is not a statistically significant association/effect between breed and vaccination.\nSecond model: geometric logistic regression to test the effect of breed on respiratory disease (we expect beef breeds to have a higher incidence than dairy)\nLogistic regression testing the effect of breed on disease history. We would need to use a multinomial regression here if we want to test disease history with both respiratory and diarrheal disease in the same model, so instead we will evaluate diarrheal disease and respiratory disease separately using binomial models.\nSecond model: Geometric logistic regression testing the effect of breed on respiratory disease history (we expect respiratory disease to be slightly higher in beef breeds).\n\nmodel2 &lt;- glm(History_Respiratory ~ Breed + Age, data = cattle_data, family = binomial)\nsummary(model2)\n\n\nCall:\nglm(formula = History_Respiratory ~ Breed + Age, family = binomial, \n    data = cattle_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)    0.08072    0.33356   0.242   0.8088  \nBreedHolstein -0.44122    0.22784  -1.937   0.0528 .\nAge           -0.01319    0.01319  -1.000   0.3172  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 443.36  on 329  degrees of freedom\nResidual deviance: 438.85  on 327  degrees of freedom\nAIC: 444.85\n\nNumber of Fisher Scoring iterations: 4\n\n\nThird model: geometric logistic regression to test the effect of breed on diarrhea disease (we expect dairy breeds to have a higher incidence than beef)\n\nmodel3 &lt;- glm(History_Diarrhea ~ Breed + Age, data = cattle_data, family = binomial)\nsummary(model3)\n\n\nCall:\nglm(formula = History_Diarrhea ~ Breed + Age, family = binomial, \n    data = cattle_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   -0.79869    0.33562  -2.380   0.0173 *\nBreedHolstein  0.24110    0.22365   1.078   0.2810  \nAge            0.02044    0.01304   1.568   0.1169  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 453.09  on 329  degrees of freedom\nResidual deviance: 449.69  on 327  degrees of freedom\nAIC: 455.69\n\nNumber of Fisher Scoring iterations: 4\n\n\nFourth model: linear logistic regression to test age differences across vaccination status (we do not expect correlation in our dataset)\n\nmodel4 &lt;- lm(Age ~ Vaccination, data = cattle_data)\nsummary(model4)\n\n\nCall:\nlm(formula = Age ~ Vaccination, data = cattle_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.1402  -7.5482   0.1558   7.8598  14.4518 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     21.5482     0.6701  32.155   &lt;2e-16 ***\nVaccinationYes   0.5921     0.9506   0.623    0.534    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.634 on 328 degrees of freedom\nMultiple R-squared:  0.001181,  Adjusted R-squared:  -0.001864 \nF-statistic: 0.3879 on 1 and 328 DF,  p-value: 0.5338\n\n\nFifth model: linear logistic regression to test age differences across disease history (we do not expect correlation in our dataset)\n\nmodel5 &lt;- lm(Age ~ Disease_History, data = cattle_data)\nsummary(model5)\n\n\nCall:\nlm(formula = Age ~ Disease_History, data = cattle_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.4615  -7.4615   0.2255   7.5385  15.2255 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      22.1712     0.8183  27.093   &lt;2e-16 ***\nDisease_HistoryRespiratory Only  -1.3967     1.1825  -1.181    0.238    \nDisease_HistoryDiarrhea Only      0.2904     1.1424   0.254    0.800    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.622 on 327 degrees of freedom\nMultiple R-squared:  0.007074,  Adjusted R-squared:  0.001001 \nF-statistic: 1.165 on 2 and 327 DF,  p-value: 0.3133\n\n\nPlotting our graphs to see how they compare with our associations seen in the linear regression models\nFirst plot: Violin plot to compare disease history with the vaccination status per breed\n\np1 &lt;- ggplot(cattle_data, aes(x = Disease_History, y = Vaccination, fill = Breed)) +\n  geom_violin() +\n  labs(x = \"Disease History\", y = \"Vaccination Status\", fill = \"Breed\") +\n  theme_minimal()\n\nprint(p1)\n\n\n\n\n\n\n\n\nThis violin plot doesn’t really display this data well. We will produce a bar plot instead.\nGenerate a bar plot to evaluate the relationship between vaccination status and disease history by comparing the proportions of animals with history of disease and vaccination status. We are working with proportion instead of count to make interpretation easier.\n\np2 &lt;- ggplot(cattle_data, aes(x = Disease_History, fill = Vaccination)) +\n    geom_bar(position = \"fill\") +  \n    labs(x = \"Disease History\", y = \"Proportion\", fill = \"Vaccination Status\") +\n    theme_minimal()\n\nprint(p2)\n\n\n\n\n\n\n\n\nThird plot: Scatter plot of age vs vaccination status\n\np3 &lt;- ggplot(cattle_data, aes(x = Vaccination, y = Age, color = Vaccination)) +\n  geom_jitter(width = 0.2, alpha = 0.6) +\n  labs(x = \"Vaccination Status\", y = \"Age (Months)\") +\n  theme_minimal()\n\nprint(p3)\n\n\n\n\n\n\n\n\nVaccination status does not show any obvious relationship with age.\nFourth plot: Scatter plot of age vs History of infection (respiratory and diarrheal)\n\np4 &lt;- ggplot(cattle_data, aes(x = Disease_History, y = Age, color = Disease_History)) +\n  geom_jitter(width = 0.2, alpha = 0.6) +\n  labs(x = \"History of Disease\", y = \"Age (Months)\", color = \"Disease History\") +\n  theme_minimal()\n\nprint(p4)\n\n\n\n\n\n\n\n\nFifth plot: Bar plot comparing disease history across all breeds\n\np5 &lt;- ggplot(cattle_data, aes(x = Breed, fill = Disease_History)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Breed\", y = \"Count\", fill = \"History of Disease\") +\n  theme_minimal()\n\nprint(p5)\n\n\n\n\n\n\n\n\nFrom the graphs we once again see that there is no obvious relationship between disease status and age (as we created in the synthetic data, which might differ from real life evaluations). We do see that there is slightly higher incidence of respiratory disease in beef animals, and diarrhea disease in dairy animals.\nIn summary, our models used for evalaution work well and we produce the expected results from this synthetic dataset. There are some aspects of the synthetic dataset that could be worked on further to make it more closely resemble and model a real-world situation, and then we might need more evaluation. However, as a proof of concept this is helpful at this stage."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "This dataset was sourced from the data repositories on the CDC website. This data evaluates the weekly hospitalization metrics for viral respiratory infections by jurisdiction in the United States from the mandatory reporting periods of August 1, 2020 to April 30, 2024, and then some voluntary reporting from May 1, 2024 - October 31, 2024. The data focuses on viral respiratory hospitalizations, primarily COVID-19 and influenza, and is derived from the National Healthcare Safety Network (NHSN).\nThe data is reported by week and contains information on the number of hospitals reporting adult COVID-19 adminssions, pediatric COVID-19 admissions, and influenza admissions. It then also provides a number of other variables related to hospital admissions and occupancies (including inpatient and ICU bed occupancies), and this is reported by geographic aggregation (State) and icludes the number of hospitals reporting in that time frame (including the hospital and patient reporting days).\nThe data can be found at this URL: https://data.cdc.gov/Public-Health-Surveillance/Weekly-United-States-Hospitalization-Metrics-by-Ju/aemt-mg7g/about_data\nThe citation for this data is:\nCenters for Disease Control and Prevention, Division of Healthcare Quality Promotion, National Healthcare Safety Network (NHSN), Weekly United States Viral Respiratory Hospitalization, Bed Occupancy, and Bed Capacity Metrics by Jurisdiction, During the Mandatory Hospital Reporting Period from August 1, 2020 to April 30, 2024, and for Data Reported Voluntarily Beginning May 1, 2024 (version date: November 1, 2024).\n\n\nFirst let’s load the required packages for our data analysis\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(\"readr\")\nlibrary(ggplot2)\nlibrary(patchwork)\n\nLet us load and get a summary of the CDC dataset. Note that the name of the file was changed at download to have a more user friendly name. Change the name at download to the same name used below or change the name of the file when loading the data to the name We will name this full dataset as cdc_datsset in R and work with that variable name.\n\ncdc_dataset &lt;- read_csv(here::here(\n  \"cdcdata-exercise/cdc_data_weekly_respiratory_hospitalizations_2020-2024.csv\")\n  ) # MJ: updated to use \"here\" package, useful for collaboration work \n\nRows: 12597 Columns: 82\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): Geographic aggregation\ndbl  (80): Hospital Reporting Days, Percent Hospital Reporting Days, Number ...\ndate  (1): Week Ending Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Viewing the dataset and getting a summary\nsummary(cdc_dataset)\n\n Week Ending Date     Geographic aggregation Hospital Reporting Days\n Min.   :2020-08-08   Length:12597           Min.   :    0.0        \n 1st Qu.:2021-08-28   Class :character       1st Qu.:  204.2        \n Median :2022-09-17   Mode  :character       Median :  474.0        \n Mean   :2022-09-17                          Mean   : 1101.5        \n 3rd Qu.:2023-10-07                          3rd Qu.:  797.0        \n Max.   :2024-10-26                          Max.   :33925.0        \n                                             NA's   :171            \n Percent Hospital Reporting Days\n Min.   :0.0000                 \n 1st Qu.:0.9500                 \n Median :0.9800                 \n Mean   :0.8996                 \n 3rd Qu.:1.0000                 \n Max.   :1.0000                 \n NA's   :171                    \n Number Hospitals Reporting Adult COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  28.0                                      \n Median :  64.0                                      \n Mean   : 157.4                                      \n 3rd Qu.: 114.0                                      \n Max.   :4872.0                                      \n                                                     \n Number Hospitals Reporting Pediatric COVID-19 Admissions\n Min.   :   0.0                                          \n 1st Qu.:  27.0                                          \n Median :  63.0                                          \n Mean   : 156.4                                          \n 3rd Qu.: 114.0                                          \n Max.   :4872.0                                          \n                                                         \n Number Hospitals Reporting Influenza Admissions\n Min.   :   0.0                                 \n 1st Qu.:  24.0                                 \n Median :  58.0                                 \n Mean   : 144.9                                 \n 3rd Qu.: 112.0                                 \n Max.   :4872.0                                 \n                                                \n Number Hospitals Reporting Prevalent Influenza Hospitalizations\n Min.   :   0.0                                                 \n 1st Qu.:  24.0                                                 \n Median :  58.0                                                 \n Mean   : 144.9                                                 \n 3rd Qu.: 112.0                                                 \n Max.   :4872.0                                                 \n                                                                \n Number Hospitals Reporting Hospitalized Influenza ICU Patients\n Min.   :   0.0                                                \n 1st Qu.:  24.0                                                \n Median :  58.0                                                \n Mean   : 144.9                                                \n 3rd Qu.: 112.0                                                \n Max.   :4872.0                                                \n                                                               \n Number Hospitals Reporting Inpatient Beds Number Hospitals Reporting ICU Beds\n Min.   :   0.0                            Min.   :   0.0                     \n 1st Qu.:  28.0                            1st Qu.:  28.0                     \n Median :  63.0                            Median :  63.0                     \n Mean   : 156.7                            Mean   : 157.1                     \n 3rd Qu.: 114.0                            3rd Qu.: 114.0                     \n Max.   :4868.0                            Max.   :4872.0                     \n                                                                              \n Number Hospitals Reporting Inpatient Beds Occupied\n Min.   :   0.0                                    \n 1st Qu.:  28.0                                    \n Median :  63.0                                    \n Mean   : 156.7                                    \n 3rd Qu.: 114.0                                    \n Max.   :4868.0                                    \n                                                   \n Number Hospitals Reporting ICU Beds Occupied\n Min.   :   0.0                              \n 1st Qu.:  27.0                              \n Median :  63.0                              \n Mean   : 155.8                              \n 3rd Qu.: 114.0                              \n Max.   :4864.0                              \n                                             \n Number Hospitals Reporting Percent Inpatient Bed Occupancy\n Min.   :   0.0                                            \n 1st Qu.:  28.0                                            \n Median :  63.0                                            \n Mean   : 156.7                                            \n 3rd Qu.: 114.0                                            \n Max.   :4868.0                                            \n                                                           \n Number Hospitals Reporting Percent ICU Bed Occupancy\n Min.   :   0.0                                      \n 1st Qu.:  27.0                                      \n Median :  63.0                                      \n Mean   : 155.8                                      \n 3rd Qu.: 114.0                                      \n Max.   :4864.0                                      \n                                                     \n Number Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\n Min.   :   0.0                                                     \n 1st Qu.:  27.0                                                     \n Median :  63.0                                                     \n Mean   : 156.3                                                     \n 3rd Qu.: 114.0                                                     \n Max.   :4868.0                                                     \n                                                                    \n Number Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\n Min.   :   0.0                                                      \n 1st Qu.:  24.0                                                      \n Median :  57.0                                                      \n Mean   : 144.4                                                      \n 3rd Qu.: 112.0                                                      \n Max.   :4868.0                                                      \n                                                                     \n Number Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\n Min.   :   0.0                                               \n 1st Qu.:  27.0                                               \n Median :  63.0                                               \n Mean   : 156.3                                               \n 3rd Qu.: 114.0                                               \n Max.   :4872.0                                               \n                                                              \n Number Hospitals Reporting Percent Influenza ICU Bed Occupancy\n Min.   :   0.0                                                \n 1st Qu.:  24.0                                                \n Median :  57.0                                                \n Mean   : 144.7                                                \n 3rd Qu.: 112.0                                                \n Max.   :4872.0                                                \n                                                               \n Number Hospitals Reporting Total COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  28.0                                      \n Median :  64.0                                      \n Mean   : 157.4                                      \n 3rd Qu.: 114.0                                      \n Max.   :4872.0                                      \n                                                     \n Number Hospitals Reporting Prevalent COVID-19 Hospitalizations\n Min.   :   0                                                  \n 1st Qu.:  27                                                  \n Median :  63                                                  \n Mean   : 157                                                  \n 3rd Qu.: 114                                                  \n Max.   :4872                                                  \n                                                               \n Number Hospitals Reporting Hospitalized COVID-19 ICU Patients\n Min.   :   0.0                                               \n 1st Qu.:  27.0                                               \n Median :  63.0                                               \n Mean   : 156.4                                               \n 3rd Qu.: 114.0                                               \n Max.   :4872.0                                               \n                                                              \n Weekly Average Adult COVID-19 Admissions\n Min.   :    0.00                        \n 1st Qu.:    8.71                        \n Median :   28.86                        \n Mean   :  154.91                        \n 3rd Qu.:   86.82                        \n Max.   :20271.57                        \n NA's   :263                             \n Weekly Total Adult COVID-19 Admissions\n Min.   :     0.0                      \n 1st Qu.:    61.0                      \n Median :   202.0                      \n Mean   :  1084.3                      \n 3rd Qu.:   607.8                      \n Max.   :141901.0                      \n NA's   :263                           \n Weekly Average Pediatric COVID-19 Admissions\n Min.   :  0.000                             \n 1st Qu.:  0.290                             \n Median :  1.140                             \n Mean   :  5.356                             \n 3rd Qu.:  3.140                             \n Max.   :907.140                             \n NA's   :272                                 \n Weekly Total Pediatric COVID-19 Admissions Weekly Average COVID-19 Admissions\n Min.   :   0.00                            Min.   :    0.00                  \n 1st Qu.:   2.00                            1st Qu.:    9.43                  \n Median :   8.00                            Median :   30.14                  \n Mean   :  37.49                            Mean   :  160.27                  \n 3rd Qu.:  22.00                            3rd Qu.:   89.57                  \n Max.   :6350.00                            Max.   :21178.71                  \n NA's   :272                                NA's   :263                       \n Weekly Total COVID-19 Admissions Weekly Average Influenza Admissions\n Min.   :     0                   Min.   :   0.00                    \n 1st Qu.:    66                   1st Qu.:   0.14                    \n Median :   211                   Median :   1.14                    \n Mean   :  1122                   Mean   :  13.44                    \n 3rd Qu.:   627                   3rd Qu.:   4.71                    \n Max.   :148251                   Max.   :3696.00                    \n NA's   :263                      NA's   :778                        \n Weekly Total Influenza Admissions\n Min.   :    0.00                 \n 1st Qu.:    1.00                 \n Median :    8.00                 \n Mean   :   94.09                 \n 3rd Qu.:   33.00                 \n Max.   :25872.00                 \n NA's   :778                      \n Weekly Average Prevalent COVID-19 Hospitalizations\n Min.   :     0.00                                 \n 1st Qu.:    55.86                                 \n Median :   193.64                                 \n Mean   :  1121.83                                 \n 3rd Qu.:   600.18                                 \n Max.   :143988.14                                 \n NA's   :263                                       \n Weekly Average Prevalent Influenza Hospitalizations\n Min.   :    0.00                                   \n 1st Qu.:    1.00                                   \n Median :    6.29                                   \n Mean   :   70.75                                   \n 3rd Qu.:   26.86                                   \n Max.   :15911.71                                   \n NA's   :786                                        \n Weekly Average Hospitalized COVID-19 ICU Patients\n Min.   :    0.00                                 \n 1st Qu.:    7.43                                 \n Median :   32.14                                 \n Mean   :  229.82                                 \n 3rd Qu.:  117.00                                 \n Max.   :26442.14                                 \n NA's   :274                                      \n Weekly Average Hospitalized Influenza ICU Patients\n Min.   :   0.00                                   \n 1st Qu.:   0.00                                   \n Median :   0.86                                   \n Mean   :  11.23                                   \n 3rd Qu.:   3.71                                   \n Max.   :2333.71                                   \n NA's   :778                                       \n Weekly Average Inpatient Beds Weekly Average ICU Beds\n Min.   :     7                Min.   :     0.0       \n 1st Qu.:  2590                1st Qu.:   332.3       \n Median :  7648                Median :  1052.9       \n Mean   : 22430                Mean   :  3363.7       \n 3rd Qu.: 15473                3rd Qu.:  2425.8       \n Max.   :710607                Max.   :113106.0       \n NA's   :263                   NA's   :263            \n Weekly Average Inpatient Beds Occupied Weekly Average ICU Beds Occupied\n Min.   :     0.3                       Min.   :    0.0                 \n 1st Qu.:  1891.2                       1st Qu.:  221.7                 \n Median :  5241.7                       Median :  747.6                 \n Mean   : 16707.7                       Mean   : 2370.4                 \n 3rd Qu.: 11438.7                       3rd Qu.: 1570.2                 \n Max.   :538750.1                       Max.   :82660.3                 \n NA's   :263                            NA's   :278                     \n Weekly Average Percent Inpatient Bed Occupancy\n Min.   :0.0400                                \n 1st Qu.:0.6700                                \n Median :0.7300                                \n Mean   :0.7187                                \n 3rd Qu.:0.7900                                \n Max.   :1.0900                                \n NA's   :263                                   \n Weekly Average Percent ICU Bed Occupancy\n Min.   :0.0000                          \n 1st Qu.:0.6300                          \n Median :0.7000                          \n Mean   :0.6823                          \n 3rd Qu.:0.7600                          \n Max.   :1.0000                          \n NA's   :309                             \n Weekly Average Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.00000                                        \n 1st Qu.:0.01000                                        \n Median :0.03000                                        \n Mean   :0.04221                                        \n 3rd Qu.:0.05000                                        \n Max.   :0.35000                                        \n NA's   :263                                            \n Weekly Average Percent Influenza Inpatient Bed Occupancy\n Min.   :0.0000                                          \n 1st Qu.:0.0000                                          \n Median :0.0000                                          \n Mean   :0.0023                                          \n 3rd Qu.:0.0000                                          \n Max.   :0.0700                                          \n NA's   :781                                             \n Weekly Average Percent COVID-19 ICU Bed Occupancy\n Min.   :0.00000                                  \n 1st Qu.:0.01000                                  \n Median :0.03000                                  \n Mean   :0.05781                                  \n 3rd Qu.:0.07000                                  \n Max.   :0.68000                                  \n NA's   :305                                      \n Weekly Average Percent Influenza ICU Bed Occupancy\n Min.   :0.0000                                    \n 1st Qu.:0.0000                                    \n Median :0.0000                                    \n Mean   :0.0026                                    \n 3rd Qu.:0.0000                                    \n Max.   :0.0900                                    \n NA's   :839                                       \n Percent Adult COVID-19 Admissions Percent Pediatric COVID-19 Admissions\n Min.   :0.0000                    Min.   :0.0000                       \n 1st Qu.:0.9500                    1st Qu.:0.0200                       \n Median :0.9700                    Median :0.0300                       \n Mean   :0.9537                    Mean   :0.0454                       \n 3rd Qu.:0.9800                    3rd Qu.:0.0500                       \n Max.   :1.0000                    Max.   :1.0000                       \n NA's   :574                       NA's   :574                          \n Percent Hospitals Reporting Adult COVID-19 Admissions\n Min.   :0.0000                                       \n 1st Qu.:0.9600                                       \n Median :0.9800                                       \n Mean   :0.8984                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Pediatric COVID-19 Admissions\n Min.   :0.0000                                           \n 1st Qu.:0.9600                                           \n Median :0.9800                                           \n Mean   :0.8927                                           \n 3rd Qu.:1.0000                                           \n Max.   :1.0000                                           \n                                                          \n Percent Hospitals Reporting Influenza Admissions\n Min.   :0.0000                                  \n 1st Qu.:0.9200                                  \n Median :0.9700                                  \n Mean   :0.8312                                  \n 3rd Qu.:1.0000                                  \n Max.   :1.0000                                  \n                                                 \n Percent Hospitals Reporting Prevalent Influenza Hospitalizations\n Min.   :0.0000                                                  \n 1st Qu.:0.9200                                                  \n Median :0.9700                                                  \n Mean   :0.8313                                                  \n 3rd Qu.:1.0000                                                  \n Max.   :1.0000                                                  \n                                                                 \n Percent Hospitals Reporting Hospitalized Influenza ICU Patients\n Min.   :0.0000                                                 \n 1st Qu.:0.9200                                                 \n Median :0.9700                                                 \n Mean   :0.8312                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Inpatient Beds\n Min.   :0.0000                            \n 1st Qu.:0.9500                            \n Median :0.9800                            \n Mean   :0.8945                            \n 3rd Qu.:1.0000                            \n Max.   :1.0000                            \n                                           \n Percent Hospitals Reporting ICU Beds\n Min.   :0.0000                      \n 1st Qu.:0.9600                      \n Median :0.9800                      \n Mean   :0.8969                      \n 3rd Qu.:1.0000                      \n Max.   :1.0000                      \n                                     \n Percent Hospitals Reporting Inpatient Beds Occupied\n Min.   :0.0000                                     \n 1st Qu.:0.9500                                     \n Median :0.9800                                     \n Mean   :0.8945                                     \n 3rd Qu.:1.0000                                     \n Max.   :1.0000                                     \n                                                    \n Percent Hospitals Reporting ICU Beds Occupied\n Min.   :0.0000                               \n 1st Qu.:0.9500                               \n Median :0.9800                               \n Mean   :0.8881                               \n 3rd Qu.:1.0000                               \n Max.   :1.0000                               \n                                              \n Percent Hospitals Reporting Percent Inpatient Bed Occupancy\n Min.   :0.0000                                             \n 1st Qu.:0.9500                                             \n Median :0.9800                                             \n Mean   :0.8945                                             \n 3rd Qu.:1.0000                                             \n Max.   :1.0000                                             \n                                                            \n Percent Hospitals Reporting Percent ICU Bed Occupancy\n Min.   :0.0000                                       \n 1st Qu.:0.9500                                       \n Median :0.9800                                       \n Mean   :0.8881                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.0000                                                      \n 1st Qu.:0.9500                                                      \n Median :0.9800                                                      \n Mean   :0.8926                                                      \n 3rd Qu.:1.0000                                                      \n Max.   :1.0000                                                      \n                                                                     \n Percent Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\n Min.   :0.0000                                                       \n 1st Qu.:0.9200                                                       \n Median :0.9700                                                       \n Mean   :0.8293                                                       \n 3rd Qu.:1.0000                                                       \n Max.   :1.0000                                                       \n                                                                      \n Percent Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\n Min.   :0.0000                                                \n 1st Qu.:0.9600                                                \n Median :0.9800                                                \n Mean   :0.8916                                                \n 3rd Qu.:1.0000                                                \n Max.   :1.0000                                                \n                                                               \n Percent Hospitals Reporting Percent Influenza ICU Bed Occupancy\n Min.   :0.0000                                                 \n 1st Qu.:0.9200                                                 \n Median :0.9700                                                 \n Mean   :0.8305                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Total COVID-19 Admissions\n Min.   :0.0000                                       \n 1st Qu.:0.9600                                       \n Median :0.9800                                       \n Mean   :0.8985                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Prevalent COVID-19 Hospitalizations\n Min.   :0.0000                                                 \n 1st Qu.:0.9600                                                 \n Median :0.9800                                                 \n Mean   :0.8963                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Hospitalized COVID-19 ICU Patients\n Min.   :0.0000                                                \n 1st Qu.:0.9600                                                \n Median :0.9800                                                \n Mean   :0.8925                                                \n 3rd Qu.:1.0000                                                \n Max.   :1.0000                                                \n                                                               \n Absolute Change in the Percent Hospitals Reporting Adult COVID-19 Admissions from Prior Week\n Min.   :-100.0000                                                                           \n 1st Qu.:   0.0000                                                                           \n Median :   0.0000                                                                           \n Mean   :  -0.2037                                                                           \n 3rd Qu.:   0.0000                                                                           \n Max.   : 100.0000                                                                           \n NA's   :57                                                                                  \n Absolute Change in the Percent Hospitals Reporting Pediatric COVID-19 Admissions from Prior Week\n Min.   :-100.0000                                                                               \n 1st Qu.:   0.0000                                                                               \n Median :   0.0000                                                                               \n Mean   :  -0.1778                                                                               \n 3rd Qu.:   0.0000                                                                               \n Max.   : 100.0000                                                                               \n NA's   :57                                                                                      \n Absolute Change in the Percent Hospitals Reporting Influenza Admissions from Prior Week\n Min.   :-100.0000                                                                      \n 1st Qu.:   0.0000                                                                      \n Median :   0.0000                                                                      \n Mean   :   0.1491                                                                      \n 3rd Qu.:   0.0000                                                                      \n Max.   : 100.0000                                                                      \n NA's   :57                                                                             \n Absolute Change in the Percent Hospitals Reporting Prevalent Influenza Hospitalizations from Prior Week\n Min.   :-100.0000                                                                                      \n 1st Qu.:   0.0000                                                                                      \n Median :   0.0000                                                                                      \n Mean   :   0.1487                                                                                      \n 3rd Qu.:   0.0000                                                                                      \n Max.   : 100.0000                                                                                      \n NA's   :57                                                                                             \n Absolute Change in the Percent Hospitals Reporting Hospitalized Influenza ICU Patients from Prior Week\n Min.   :-100.0000                                                                                     \n 1st Qu.:   0.0000                                                                                     \n Median :   0.0000                                                                                     \n Mean   :   0.1485                                                                                     \n 3rd Qu.:   0.0000                                                                                     \n Max.   : 100.0000                                                                                     \n NA's   :57                                                                                            \n Absolute Change in the Percent Hospitals Reporting Inpatient Beds from Prior Week\n Min.   :-100.0000                                                                \n 1st Qu.:   0.0000                                                                \n Median :   0.0000                                                                \n Mean   :  -0.2326                                                                \n 3rd Qu.:   0.0000                                                                \n Max.   : 100.0000                                                                \n NA's   :57                                                                       \n Absolute Change in the Percent Hospitals Reporting ICU Beds from Prior Week\n Min.   :-100.0000                                                          \n 1st Qu.:   0.0000                                                          \n Median :   0.0000                                                          \n Mean   :  -0.2561                                                          \n 3rd Qu.:   0.0000                                                          \n Max.   : 100.0000                                                          \n NA's   :57                                                                 \n Absolute Change in the Percent Hospitals Reporting Inpatient Beds Occupied from Prior Week\n Min.   :-100.0000                                                                         \n 1st Qu.:   0.0000                                                                         \n Median :   0.0000                                                                         \n Mean   :  -0.2326                                                                         \n 3rd Qu.:   0.0000                                                                         \n Max.   : 100.0000                                                                         \n NA's   :57                                                                                \n Absolute Change in the Percent Hospitals Reporting ICU Beds Occupied from Prior Week\n Min.   :-100.0000                                                                   \n 1st Qu.:   0.0000                                                                   \n Median :   0.0000                                                                   \n Mean   :  -0.1627                                                                   \n 3rd Qu.:   0.0000                                                                   \n Max.   : 100.0000                                                                   \n NA's   :57                                                                          \n Absolute Change in the Percent Hospitals Reporting Percent Inpatient Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                                 \n 1st Qu.:   0.0000                                                                                 \n Median :   0.0000                                                                                 \n Mean   :  -0.2326                                                                                 \n 3rd Qu.:   0.0000                                                                                 \n Max.   : 100.0000                                                                                 \n NA's   :57                                                                                        \n Absolute Change in the Percent Hospitals Reporting Percent ICU Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                           \n 1st Qu.:   0.0000                                                                           \n Median :   0.0000                                                                           \n Mean   :  -0.1627                                                                           \n 3rd Qu.:   0.0000                                                                           \n Max.   : 100.0000                                                                           \n NA's   :57                                                                                  \n Absolute Change in the Percent Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                                          \n 1st Qu.:   0.0000                                                                                          \n Median :   0.0000                                                                                          \n Mean   :  -0.1973                                                                                          \n 3rd Qu.:   0.0000                                                                                          \n Max.   : 100.0000                                                                                          \n NA's   :57                                                                                                 \n Absolute Change in the Percent Hospitals Reporting Percent Influenza Inpatient Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                                           \n 1st Qu.:   0.0000                                                                                           \n Median :   0.0000                                                                                           \n Mean   :   0.1487                                                                                           \n 3rd Qu.:   0.0000                                                                                           \n Max.   : 100.0000                                                                                           \n NA's   :57                                                                                                  \n Absolute Change in the Percent Hospitals Reporting Percent COVID-19 ICU Bed Occupancy from Prior Week\n Min.   :-100.000                                                                                     \n 1st Qu.:   0.000                                                                                     \n Median :   0.000                                                                                     \n Mean   :  -0.195                                                                                     \n 3rd Qu.:   0.000                                                                                     \n Max.   : 100.000                                                                                     \n NA's   :57                                                                                           \n Absolute Change in the Percent Hospitals Reporting Percent Influenza ICU Bed Occupancy from Prior Week\n Min.   :-100.000                                                                                      \n 1st Qu.:   0.000                                                                                      \n Median :   0.000                                                                                      \n Mean   :   0.142                                                                                      \n 3rd Qu.:   0.000                                                                                      \n Max.   : 100.000                                                                                      \n NA's   :57                                                                                            \n Absolute Change in the Percent Hospitals Reporting Total COVID-19 Admissions from Prior Week\n Min.   :-100.0000                                                                           \n 1st Qu.:   0.0000                                                                           \n Median :   0.0000                                                                           \n Mean   :  -0.2091                                                                           \n 3rd Qu.:   0.0000                                                                           \n Max.   : 100.0000                                                                           \n NA's   :57                                                                                  \n Absolute Change in the Percent Hospitals Reporting Prevalent COVID-19 Hospitalizations from Prior Week\n Min.   :-100.0000                                                                                     \n 1st Qu.:   0.0000                                                                                     \n Median :   0.0000                                                                                     \n Mean   :  -0.2261                                                                                     \n 3rd Qu.:   0.0000                                                                                     \n Max.   : 100.0000                                                                                     \n NA's   :57                                                                                            \n Absolute Change in the Percent Hospitals Reporting Hospitalized COVID-19 ICU Patients from Prior Week\n Min.   :-100.000                                                                                     \n 1st Qu.:   0.000                                                                                     \n Median :   0.000                                                                                     \n Mean   :  -0.201                                                                                     \n 3rd Qu.:   0.000                                                                                     \n Max.   : 100.000                                                                                     \n NA's   :57                                                                                           \n\nview(cdc_dataset)\nprint(cdc_dataset)\n\n# A tibble: 12,597 × 82\n   `Week Ending Date` `Geographic aggregation` `Hospital Reporting Days`\n   &lt;date&gt;             &lt;chr&gt;                                        &lt;dbl&gt;\n 1 2020-08-08         AK                                              98\n 2 2020-08-15         AK                                              98\n 3 2020-08-22         AK                                              98\n 4 2020-08-29         AK                                              97\n 5 2020-09-05         AK                                              97\n 6 2020-09-12         AK                                              98\n 7 2020-09-19         AK                                              98\n 8 2020-09-26         AK                                              96\n 9 2020-10-03         AK                                              97\n10 2020-10-10         AK                                              98\n# ℹ 12,587 more rows\n# ℹ 79 more variables: `Percent Hospital Reporting Days` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Adult COVID-19 Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Pediatric COVID-19 Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Influenza Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Prevalent Influenza Hospitalizations` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Hospitalized Influenza ICU Patients` &lt;dbl&gt;, …\n\n\nThe summary of the dataset is valuable but it helps to actually view and scroll through the data for a better overview and feel of the data.\nWe can see that this is a very large dataset. There are 12597 observations in this dataset and 82 columns. We have to do a lot of scrolling to see much, including our summary, which is not very helpful. This is very big for us to evaluate and so we might want to cut it down a bit before we spend time filtering through and cleaning all of the data that is not of interest to us.\nFor this evaluation we want to have an idea of how COVID-19 and infuenza affected the population by evaluating the hospitilization rates. We are interested in knowing how this affected the healthcare system so we will include the information related to how many hospitals reported data and the occupancy of these hospitals. From viewing the data there seems to be overlap and repetition of some of the data, as well as categories and definitions that are not necessary for us, so we will reduce our columns to those of greatest interest to us. We will focus on adult data as the reporting of pediatric data seems to be limited to COVID-19 and thus will limit any comparisons to influenza.\nWe are also not going to split up ICU vs general admissions for our evaluations, but this is something that could be broken down.\nWe will also cut down our observations by focusing on one State and by defining our time frame.\nThis part could be done in either order and the State and time frame could be adapted.\nCutting down our dataset: Since we know that the mandatory reporting period will have the best information reported, we will focus on this time frame August 1, 2020 - April 30, 2024. We will also start by focusing on the state of Georgia (region=GA). We will use the filter function to get the rows with these variables.\n\n#Using the filter function to select for our rows of interest. We will assign these to two new variables.\n#Filter by State = selecting GA\ncdc_data_georgia &lt;- cdc_dataset |&gt; filter(`Geographic aggregation` == \"GA\")\n\n#Filter further by date = August 1, 2020 - April 30, 2024\ncdc_georgia_mandatory_data &lt;- cdc_data_georgia |&gt; filter(`Week Ending Date`&gt;=\"2020-08-01\" & `Week Ending Date` &lt;= \"2024-04-30\")\n\nWe can see that we have filtered our observations down to 195 observations. We could do the same thing for multiple states (or have included multiple states) and compared the data between states.\nNow we will focus on selecting the columns that we are most interested in: We will use the select function. Columns of interest to be kept: - Week Ending Date - Geographic aggregation - Number Hospitals Reporting Adult COVID-19 Admissions - Number Hospitals Reporting Influenza Admissions - Weekly Total Adult COVID-19 Admissions - Weekly Total Influenza Admissions - Weekly Average Hospitalized COVID-19 ICU Patients - Weekly Average Hospitalized Influenza ICU Patients - Weekly Average Inpatient Beds - Weekly Average ICU Beds - Weekly Average Inpatient Beds Occupied - Weekly Average ICU Beds Occupied\nDefining our new dataset with columns of interest:\n\n#Using the select function to define our columns of interest for the GA dataset that we refined\ncdc_ga_mandatory_refined &lt;- cdc_georgia_mandatory_data |&gt; dplyr::select(`Week Ending Date`,\n`Geographic aggregation`, `Number Hospitals Reporting Adult COVID-19 Admissions`,\n`Number Hospitals Reporting Influenza Admissions`,\n`Weekly Total Adult COVID-19 Admissions`,\n`Weekly Total Influenza Admissions`,\n`Weekly Average Hospitalized COVID-19 ICU Patients`,\n`Weekly Average Hospitalized Influenza ICU Patients`,\n`Weekly Average Inpatient Beds`,\n`Weekly Average ICU Beds`,\n`Weekly Average Inpatient Beds Occupied`,\n`Weekly Average ICU Beds Occupied`)\n\n#View the new datset and a summary\nsummary(cdc_ga_mandatory_refined)\n\n Week Ending Date     Geographic aggregation\n Min.   :2020-08-08   Length:195            \n 1st Qu.:2021-07-13   Class :character      \n Median :2022-06-18   Mode  :character      \n Mean   :2022-06-18                         \n 3rd Qu.:2023-05-23                         \n Max.   :2024-04-27                         \n                                            \n Number Hospitals Reporting Adult COVID-19 Admissions\n Min.   : 87.0                                       \n 1st Qu.:137.0                                       \n Median :138.0                                       \n Mean   :137.5                                       \n 3rd Qu.:139.0                                       \n Max.   :140.0                                       \n                                                     \n Number Hospitals Reporting Influenza Admissions\n Min.   :  0.0                                  \n 1st Qu.:134.0                                  \n Median :137.0                                  \n Mean   :127.2                                  \n 3rd Qu.:139.0                                  \n Max.   :139.0                                  \n                                                \n Weekly Total Adult COVID-19 Admissions Weekly Total Influenza Admissions\n Min.   : 122.0                         Min.   :   0.00                  \n 1st Qu.: 410.5                         1st Qu.:  17.00                  \n Median : 816.0                         Median :  30.00                  \n Mean   :1259.5                         Mean   :  73.04                  \n 3rd Qu.:1446.5                         3rd Qu.:  54.75                  \n Max.   :5459.0                         Max.   :1123.00                  \n                                        NA's   :5                        \n Weekly Average Hospitalized COVID-19 ICU Patients\n Min.   :  28.57                                  \n 1st Qu.:  69.64                                  \n Median : 137.29                                  \n Mean   : 283.58                                  \n 3rd Qu.: 376.29                                  \n Max.   :1319.86                                  \n                                                  \n Weekly Average Hospitalized Influenza ICU Patients\n Min.   :  0.000                                   \n 1st Qu.:  2.895                                   \n Median : 31.570                                   \n Mean   : 45.580                                   \n 3rd Qu.: 71.395                                   \n Max.   :255.000                                   \n NA's   :5                                         \n Weekly Average Inpatient Beds Weekly Average ICU Beds\n Min.   :12754                 Min.   :2403           \n 1st Qu.:17263                 1st Qu.:3007           \n Median :17994                 Median :3043           \n Mean   :18361                 Mean   :3069           \n 3rd Qu.:20123                 3rd Qu.:3153           \n Max.   :20852                 Max.   :3422           \n                                                      \n Weekly Average Inpatient Beds Occupied Weekly Average ICU Beds Occupied\n Min.   :10065                          Min.   :1210                    \n 1st Qu.:13769                          1st Qu.:2315                    \n Median :14365                          Median :2395                    \n Mean   :14415                          Mean   :2377                    \n 3rd Qu.:15170                          3rd Qu.:2484                    \n Max.   :16660                          Max.   :2828                    \n                                                                        \n\nview(cdc_ga_mandatory_refined)\nprint(cdc_ga_mandatory_refined)\n\n# A tibble: 195 × 12\n   `Week Ending Date` `Geographic aggregation` Number Hospitals Reporting Adul…¹\n   &lt;date&gt;             &lt;chr&gt;                                                &lt;dbl&gt;\n 1 2020-08-08         GA                                                      87\n 2 2020-08-15         GA                                                     130\n 3 2020-08-22         GA                                                     130\n 4 2020-08-29         GA                                                     131\n 5 2020-09-05         GA                                                     133\n 6 2020-09-12         GA                                                     134\n 7 2020-09-19         GA                                                     129\n 8 2020-09-26         GA                                                     131\n 9 2020-10-03         GA                                                     139\n10 2020-10-10         GA                                                     139\n# ℹ 185 more rows\n# ℹ abbreviated name: ¹​`Number Hospitals Reporting Adult COVID-19 Admissions`\n# ℹ 9 more variables: `Number Hospitals Reporting Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Total Adult COVID-19 Admissions` &lt;dbl&gt;,\n#   `Weekly Total Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized COVID-19 ICU Patients` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized Influenza ICU Patients` &lt;dbl&gt;, …\n\n\nWe have reduced our datset to a much more manageable set of information that we can refine and evalaute. If we feel that we should incude an additional column (left something valuable out) we can go back and add it now. Our sumamry shows us that we have a few NA values in our datset.\nWe will look for these using the is.na function. We will create a specific variable from these NA values to make it easier to visualise without searching. In reality, with the summary and reduced size of the dataset we can scroll through and find these if we want. However, printing these specifically allows us to isolate them quickly.\n\ncdc_refined_ga_NA &lt;- cdc_ga_mandatory_refined |&gt; dplyr::filter(if_any(everything(), is.na))\n\nprint(cdc_refined_ga_NA)\n\n# A tibble: 5 × 12\n  `Week Ending Date` `Geographic aggregation` Number Hospitals Reporting Adult…¹\n  &lt;date&gt;             &lt;chr&gt;                                                 &lt;dbl&gt;\n1 2020-08-08         GA                                                       87\n2 2020-08-15         GA                                                      130\n3 2020-08-22         GA                                                      130\n4 2020-08-29         GA                                                      131\n5 2020-10-10         GA                                                      139\n# ℹ abbreviated name: ¹​`Number Hospitals Reporting Adult COVID-19 Admissions`\n# ℹ 9 more variables: `Number Hospitals Reporting Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Total Adult COVID-19 Admissions` &lt;dbl&gt;,\n#   `Weekly Total Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized COVID-19 ICU Patients` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized Influenza ICU Patients` &lt;dbl&gt;,\n#   `Weekly Average Inpatient Beds` &lt;dbl&gt;, `Weekly Average ICU Beds` &lt;dbl&gt;, …\n\n\nWe can see that these NA values are in the first four weeks of observations, and one week in October 2020, and that they are all in the influenza observations. We might be led to believe that perhaps the recording/reporting (or testing) of influenza tests was halted in the peak of the COVID-19 pandemic when resources were limited, or that the order to report influenza cases for this use only cae through later. This is something that we might be able to find on the CDC website if desired. We will not delete these observations because we would lose valuable information related to the COVID-19 pandemic from these weeks. We could potentially use synthetic data for these missing variables or we could work on date ranges that do not include these weeks (2020-08-08 - 2020-08-29 and 2020-10-10) if we want to compare COVID-19 to Influenza.\nWe will make a few plots of different variables. We will start by analyzing the COVID-19 data.\n\n#We are looking at the distribution of the total adult COVID-19 cases  \ncovid_adult_admissions_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Total Adult COVID-19 Admissions`)) + geom_histogram(bins=10,fill= \"green\", color = \"black\") + labs(title = \"Distribution of the weekly total adult COVID-19 Hospital Admissions\", x = \"Weekly Total Adult COVID-19 Admissions\", y = \"Week counts\")\n\nprint(covid_adult_admissions_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of this data (Distribution of the weekly total adult COVID-19 Hospital Admissions) and calculate the standard deviation\n\nsummary(cdc_ga_mandatory_refined$`Weekly Total Adult COVID-19 Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  122.0   410.5   816.0  1259.5  1446.5  5459.0 \n\n#We will calculate the standard deviation\nsd(cdc_ga_mandatory_refined$`Weekly Total Adult COVID-19 Admissions`)\n\n[1] 1267.792\n\n\n\n#We are looking at the distribution of the average hospitalized COVID-19 ICU patients  \ncovid_icu_patients_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average Hospitalized COVID-19 ICU Patients`)) + geom_histogram(bins=10,fill= \"red\", color = \"black\") + labs(title = \"Distribution of the average COVID-19 Hospital ICU Patients\", x = \"Weekly Average COVID-19 ICU Patients\", y = \"Week counts\")\n\nprint(covid_icu_patients_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of this data (Distribution of the average COVID-19 Hospital ICU Patients) and calculate the standard deviation\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average Hospitalized COVID-19 ICU Patients`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.57   69.64  137.29  283.58  376.29 1319.86 \n\n#We will calculate the standard deviation\nsd(cdc_ga_mandatory_refined$`Weekly Average Hospitalized COVID-19 ICU Patients`)\n\n[1] 315.2279\n\n\nWe see how many more patients were hospitalized than were in ICU in Georgia, as the max number for the ICU patients is quite close to mean value of the hospital admissions.\nLooking at the same distributions for the Influenza data:\n\n#We are looking at the distribution of the total Influenza cases  \ninfluenza_adult_admissions_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Total Influenza Admissions`)) + geom_histogram(bins=10,fill= \"blue\", color = \"black\") + labs(title = \"Distribution of the weekly Influenza Hospital Admissions\", x = \"Weekly Total Influenza Admissions\", y = \"Week counts\")\n\nprint(influenza_adult_admissions_plot)\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe receive a warning that some values were excluded (likely our NA values).\nWe will get a summary of this data (Weekly Total Influenza Admissions) and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Total Influenza Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   17.00   30.00   73.04   54.75 1123.00       5 \n\n#We will calculate the standard deviation and exclude the NA values in the influenza data\nsd(cdc_ga_mandatory_refined$`Weekly Total Influenza Admissions`, na.rm = TRUE)  \n\n[1] 137.6914\n\n\nThe mean value for admissions for the influenza cases is a great deal lower than that from the COVID-19 cases. We see that the mean of the COVID-19 hospital admissions is 17.2 times greater than the mean from the influenza admissions.\n\n#We are looking at the distribution of the average influenza ICU patients  \ninfluenza_icu_patients_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average Hospitalized Influenza ICU Patients`)) + geom_histogram(bins=10,fill= \"red\", color = \"black\") + labs(title = \"Distribution of the average Influenza Hospital ICU Patients\", x = \"Weekly Average Influenza ICU Patients\", y = \"Week counts\")\n\nprint(influenza_icu_patients_plot)\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe get the same warning as our previous plot for the influenza cases (regarding the missing NA values).\nWe will get a summary of this data (Weekly Average Influenza ICU Patients) and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average Hospitalized Influenza ICU Patients`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   2.895  31.570  45.580  71.395 255.000       5 \n\n#We will calculate the standard deviation and exclude the NA values in the influenza data\nsd(cdc_ga_mandatory_refined$`Weekly Average Hospitalized Influenza ICU Patients`, na.rm = TRUE)\n\n[1] 51.3058\n\n\nThe mean value for ICU patients for the influenza cases is also much lower than that from the COVID-19 cases (not quite as much compared to admissions). We see that the mean of the COVID-19 hospital admissions is 6.2 times greater than the mean from the influenza ICU patients.\nBy comparing the plots and summary data we generated for COVID-19 and Influenza cases we can already see the much greater number of hospital admissions and ICU cases from COVID-19 compared to influenza. We also see the wider range for the COVID-19 cases compared to Influenza. The pronounced impact of COVID-19 as a respiratory disease is obvious before we do further analysis.\nWe can evaluate the strain on the healthcare system by looking at the hospital beds that were occupied (we will just look at total numbers between inpatient and ICU and not as a percentage). This does not distinguish what caused the beds to be occupied.\nLooking at the distributions of occupied inptient beds:\n\n#We are looking at the distribution of the occupied inptient beds  \ncdc_inpatient_occupied_beds_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average Inpatient Beds Occupied`)) + geom_histogram(bins=10,fill= \"blue\", color = \"black\") + labs(title = \"Distribution of the weekly Average Occupied Inpatient Beds\", x = \"Weekly Inpatient Beds\", y = \"Week counts\")\n\nprint(cdc_inpatient_occupied_beds_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of the inpatient occupied beds data and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average Inpatient Beds Occupied`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10065   13769   14365   14415   15170   16660 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Weekly Average Inpatient Beds Occupied`)\n\n[1] 1097.536\n\n\n\n#We are looking at the distribution of the occupied ICU beds  \ncdc_icu_occupied_beds_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average ICU Beds Occupied`)) + geom_histogram(bins=10,fill= \"red\", color = \"black\") + labs(title = \"Distribution of the weekly Average Occupied ICU Beds\", x = \"Weekly Occupied ICU Beds\", y = \"Week counts\")\n\nprint(cdc_icu_occupied_beds_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of the occupied ICU beds data and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average ICU Beds Occupied`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1210    2315    2395    2377    2484    2828 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Weekly Average ICU Beds Occupied`)\n\n[1] 237.6443\n\n\nWe can look at the occupancy of the inpatient hosptial beds by week for the period evaluated, we will generate a line plot:\n\n#We are looking at the distribution of the occupied inptient beds over time \ncdc_inpatient_beds_time_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Week Ending Date`, y=`Weekly Average Inpatient Beds Occupied`)) + geom_line(color = \"black\") + labs(title = \"Average Occupancy of Inpatient Beds over time\", x = \"Week Evaluated\", y = \"Weekly Occupied Inpatient Beds\")\n\nprint(cdc_inpatient_beds_time_plot)\n\n\n\n\n\n\n\n\nWe will provide a summary of the number of hospitals reporting COVID-19 and influenza respectively without plotting a graph.\nWe will get a summary of the number of hospital reporting Adult COVID-19 Admissions and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Number Hospitals Reporting Adult COVID-19 Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   87.0   137.0   138.0   137.5   139.0   140.0 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Number Hospitals Reporting Adult COVID-19 Admissions`)\n\n[1] 4.080754\n\n\nWe will get a summary of the number of hospital reporting Influenza Admissions and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Number Hospitals Reporting Influenza Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   134.0   137.0   127.2   139.0   139.0 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Number Hospitals Reporting Influenza Admissions`)\n\n[1] 31.62036\n\n\nWe see very similar values for influenza and COVID-19, although we realize that the 0 values in the influenza cases might be an artifact carried by the original creators from the NA values. We will create a new variable looking at the number of hospitals reporting influenza cases without these entries.\n\n#We are using a loop function to first select for rows where there are no NA values, this is based on using one of the columns that we previously saw had an NA value. We then select the rows to only keep the influenza data\n\nfiltered_infuenza_cases&lt;- cdc_ga_mandatory_refined |&gt; dplyr::filter(!is.na(`Weekly Total Influenza Admissions`)) |&gt; \n  dplyr::select(`Week Ending Date`, `Number Hospitals Reporting Influenza Admissions`, `Weekly Total Influenza Admissions`,`Weekly Average Hospitalized Influenza ICU Patients`)\n\nWe can now run the previous summary of the influenza reported cases by hospitals again and compare results\n\nsummary(filtered_infuenza_cases$`Number Hospitals Reporting Influenza Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0   135.0   137.0   130.6   139.0   139.0 \n\n#We will calculate the standard deviation \nsd(filtered_infuenza_cases$`Number Hospitals Reporting Influenza Admissions`)\n\n[1] 24.22654\n\n\nWe see that the values did change, with the standard deviation decreasing by around 7, but it perhaps did not decrease by as much as expected because the minimum value is now 1 instead of 0."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#evaluating-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#evaluating-the-data",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "First let’s load the required packages for our data analysis\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(\"readr\")\nlibrary(ggplot2)\nlibrary(patchwork)\n\nLet us load and get a summary of the CDC dataset. Note that the name of the file was changed at download to have a more user friendly name. Change the name at download to the same name used below or change the name of the file when loading the data to the name We will name this full dataset as cdc_datsset in R and work with that variable name.\n\ncdc_dataset &lt;- read_csv(here::here(\n  \"cdcdata-exercise/cdc_data_weekly_respiratory_hospitalizations_2020-2024.csv\")\n  ) # MJ: updated to use \"here\" package, useful for collaboration work \n\nRows: 12597 Columns: 82\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): Geographic aggregation\ndbl  (80): Hospital Reporting Days, Percent Hospital Reporting Days, Number ...\ndate  (1): Week Ending Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Viewing the dataset and getting a summary\nsummary(cdc_dataset)\n\n Week Ending Date     Geographic aggregation Hospital Reporting Days\n Min.   :2020-08-08   Length:12597           Min.   :    0.0        \n 1st Qu.:2021-08-28   Class :character       1st Qu.:  204.2        \n Median :2022-09-17   Mode  :character       Median :  474.0        \n Mean   :2022-09-17                          Mean   : 1101.5        \n 3rd Qu.:2023-10-07                          3rd Qu.:  797.0        \n Max.   :2024-10-26                          Max.   :33925.0        \n                                             NA's   :171            \n Percent Hospital Reporting Days\n Min.   :0.0000                 \n 1st Qu.:0.9500                 \n Median :0.9800                 \n Mean   :0.8996                 \n 3rd Qu.:1.0000                 \n Max.   :1.0000                 \n NA's   :171                    \n Number Hospitals Reporting Adult COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  28.0                                      \n Median :  64.0                                      \n Mean   : 157.4                                      \n 3rd Qu.: 114.0                                      \n Max.   :4872.0                                      \n                                                     \n Number Hospitals Reporting Pediatric COVID-19 Admissions\n Min.   :   0.0                                          \n 1st Qu.:  27.0                                          \n Median :  63.0                                          \n Mean   : 156.4                                          \n 3rd Qu.: 114.0                                          \n Max.   :4872.0                                          \n                                                         \n Number Hospitals Reporting Influenza Admissions\n Min.   :   0.0                                 \n 1st Qu.:  24.0                                 \n Median :  58.0                                 \n Mean   : 144.9                                 \n 3rd Qu.: 112.0                                 \n Max.   :4872.0                                 \n                                                \n Number Hospitals Reporting Prevalent Influenza Hospitalizations\n Min.   :   0.0                                                 \n 1st Qu.:  24.0                                                 \n Median :  58.0                                                 \n Mean   : 144.9                                                 \n 3rd Qu.: 112.0                                                 \n Max.   :4872.0                                                 \n                                                                \n Number Hospitals Reporting Hospitalized Influenza ICU Patients\n Min.   :   0.0                                                \n 1st Qu.:  24.0                                                \n Median :  58.0                                                \n Mean   : 144.9                                                \n 3rd Qu.: 112.0                                                \n Max.   :4872.0                                                \n                                                               \n Number Hospitals Reporting Inpatient Beds Number Hospitals Reporting ICU Beds\n Min.   :   0.0                            Min.   :   0.0                     \n 1st Qu.:  28.0                            1st Qu.:  28.0                     \n Median :  63.0                            Median :  63.0                     \n Mean   : 156.7                            Mean   : 157.1                     \n 3rd Qu.: 114.0                            3rd Qu.: 114.0                     \n Max.   :4868.0                            Max.   :4872.0                     \n                                                                              \n Number Hospitals Reporting Inpatient Beds Occupied\n Min.   :   0.0                                    \n 1st Qu.:  28.0                                    \n Median :  63.0                                    \n Mean   : 156.7                                    \n 3rd Qu.: 114.0                                    \n Max.   :4868.0                                    \n                                                   \n Number Hospitals Reporting ICU Beds Occupied\n Min.   :   0.0                              \n 1st Qu.:  27.0                              \n Median :  63.0                              \n Mean   : 155.8                              \n 3rd Qu.: 114.0                              \n Max.   :4864.0                              \n                                             \n Number Hospitals Reporting Percent Inpatient Bed Occupancy\n Min.   :   0.0                                            \n 1st Qu.:  28.0                                            \n Median :  63.0                                            \n Mean   : 156.7                                            \n 3rd Qu.: 114.0                                            \n Max.   :4868.0                                            \n                                                           \n Number Hospitals Reporting Percent ICU Bed Occupancy\n Min.   :   0.0                                      \n 1st Qu.:  27.0                                      \n Median :  63.0                                      \n Mean   : 155.8                                      \n 3rd Qu.: 114.0                                      \n Max.   :4864.0                                      \n                                                     \n Number Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\n Min.   :   0.0                                                     \n 1st Qu.:  27.0                                                     \n Median :  63.0                                                     \n Mean   : 156.3                                                     \n 3rd Qu.: 114.0                                                     \n Max.   :4868.0                                                     \n                                                                    \n Number Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\n Min.   :   0.0                                                      \n 1st Qu.:  24.0                                                      \n Median :  57.0                                                      \n Mean   : 144.4                                                      \n 3rd Qu.: 112.0                                                      \n Max.   :4868.0                                                      \n                                                                     \n Number Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\n Min.   :   0.0                                               \n 1st Qu.:  27.0                                               \n Median :  63.0                                               \n Mean   : 156.3                                               \n 3rd Qu.: 114.0                                               \n Max.   :4872.0                                               \n                                                              \n Number Hospitals Reporting Percent Influenza ICU Bed Occupancy\n Min.   :   0.0                                                \n 1st Qu.:  24.0                                                \n Median :  57.0                                                \n Mean   : 144.7                                                \n 3rd Qu.: 112.0                                                \n Max.   :4872.0                                                \n                                                               \n Number Hospitals Reporting Total COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  28.0                                      \n Median :  64.0                                      \n Mean   : 157.4                                      \n 3rd Qu.: 114.0                                      \n Max.   :4872.0                                      \n                                                     \n Number Hospitals Reporting Prevalent COVID-19 Hospitalizations\n Min.   :   0                                                  \n 1st Qu.:  27                                                  \n Median :  63                                                  \n Mean   : 157                                                  \n 3rd Qu.: 114                                                  \n Max.   :4872                                                  \n                                                               \n Number Hospitals Reporting Hospitalized COVID-19 ICU Patients\n Min.   :   0.0                                               \n 1st Qu.:  27.0                                               \n Median :  63.0                                               \n Mean   : 156.4                                               \n 3rd Qu.: 114.0                                               \n Max.   :4872.0                                               \n                                                              \n Weekly Average Adult COVID-19 Admissions\n Min.   :    0.00                        \n 1st Qu.:    8.71                        \n Median :   28.86                        \n Mean   :  154.91                        \n 3rd Qu.:   86.82                        \n Max.   :20271.57                        \n NA's   :263                             \n Weekly Total Adult COVID-19 Admissions\n Min.   :     0.0                      \n 1st Qu.:    61.0                      \n Median :   202.0                      \n Mean   :  1084.3                      \n 3rd Qu.:   607.8                      \n Max.   :141901.0                      \n NA's   :263                           \n Weekly Average Pediatric COVID-19 Admissions\n Min.   :  0.000                             \n 1st Qu.:  0.290                             \n Median :  1.140                             \n Mean   :  5.356                             \n 3rd Qu.:  3.140                             \n Max.   :907.140                             \n NA's   :272                                 \n Weekly Total Pediatric COVID-19 Admissions Weekly Average COVID-19 Admissions\n Min.   :   0.00                            Min.   :    0.00                  \n 1st Qu.:   2.00                            1st Qu.:    9.43                  \n Median :   8.00                            Median :   30.14                  \n Mean   :  37.49                            Mean   :  160.27                  \n 3rd Qu.:  22.00                            3rd Qu.:   89.57                  \n Max.   :6350.00                            Max.   :21178.71                  \n NA's   :272                                NA's   :263                       \n Weekly Total COVID-19 Admissions Weekly Average Influenza Admissions\n Min.   :     0                   Min.   :   0.00                    \n 1st Qu.:    66                   1st Qu.:   0.14                    \n Median :   211                   Median :   1.14                    \n Mean   :  1122                   Mean   :  13.44                    \n 3rd Qu.:   627                   3rd Qu.:   4.71                    \n Max.   :148251                   Max.   :3696.00                    \n NA's   :263                      NA's   :778                        \n Weekly Total Influenza Admissions\n Min.   :    0.00                 \n 1st Qu.:    1.00                 \n Median :    8.00                 \n Mean   :   94.09                 \n 3rd Qu.:   33.00                 \n Max.   :25872.00                 \n NA's   :778                      \n Weekly Average Prevalent COVID-19 Hospitalizations\n Min.   :     0.00                                 \n 1st Qu.:    55.86                                 \n Median :   193.64                                 \n Mean   :  1121.83                                 \n 3rd Qu.:   600.18                                 \n Max.   :143988.14                                 \n NA's   :263                                       \n Weekly Average Prevalent Influenza Hospitalizations\n Min.   :    0.00                                   \n 1st Qu.:    1.00                                   \n Median :    6.29                                   \n Mean   :   70.75                                   \n 3rd Qu.:   26.86                                   \n Max.   :15911.71                                   \n NA's   :786                                        \n Weekly Average Hospitalized COVID-19 ICU Patients\n Min.   :    0.00                                 \n 1st Qu.:    7.43                                 \n Median :   32.14                                 \n Mean   :  229.82                                 \n 3rd Qu.:  117.00                                 \n Max.   :26442.14                                 \n NA's   :274                                      \n Weekly Average Hospitalized Influenza ICU Patients\n Min.   :   0.00                                   \n 1st Qu.:   0.00                                   \n Median :   0.86                                   \n Mean   :  11.23                                   \n 3rd Qu.:   3.71                                   \n Max.   :2333.71                                   \n NA's   :778                                       \n Weekly Average Inpatient Beds Weekly Average ICU Beds\n Min.   :     7                Min.   :     0.0       \n 1st Qu.:  2590                1st Qu.:   332.3       \n Median :  7648                Median :  1052.9       \n Mean   : 22430                Mean   :  3363.7       \n 3rd Qu.: 15473                3rd Qu.:  2425.8       \n Max.   :710607                Max.   :113106.0       \n NA's   :263                   NA's   :263            \n Weekly Average Inpatient Beds Occupied Weekly Average ICU Beds Occupied\n Min.   :     0.3                       Min.   :    0.0                 \n 1st Qu.:  1891.2                       1st Qu.:  221.7                 \n Median :  5241.7                       Median :  747.6                 \n Mean   : 16707.7                       Mean   : 2370.4                 \n 3rd Qu.: 11438.7                       3rd Qu.: 1570.2                 \n Max.   :538750.1                       Max.   :82660.3                 \n NA's   :263                            NA's   :278                     \n Weekly Average Percent Inpatient Bed Occupancy\n Min.   :0.0400                                \n 1st Qu.:0.6700                                \n Median :0.7300                                \n Mean   :0.7187                                \n 3rd Qu.:0.7900                                \n Max.   :1.0900                                \n NA's   :263                                   \n Weekly Average Percent ICU Bed Occupancy\n Min.   :0.0000                          \n 1st Qu.:0.6300                          \n Median :0.7000                          \n Mean   :0.6823                          \n 3rd Qu.:0.7600                          \n Max.   :1.0000                          \n NA's   :309                             \n Weekly Average Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.00000                                        \n 1st Qu.:0.01000                                        \n Median :0.03000                                        \n Mean   :0.04221                                        \n 3rd Qu.:0.05000                                        \n Max.   :0.35000                                        \n NA's   :263                                            \n Weekly Average Percent Influenza Inpatient Bed Occupancy\n Min.   :0.0000                                          \n 1st Qu.:0.0000                                          \n Median :0.0000                                          \n Mean   :0.0023                                          \n 3rd Qu.:0.0000                                          \n Max.   :0.0700                                          \n NA's   :781                                             \n Weekly Average Percent COVID-19 ICU Bed Occupancy\n Min.   :0.00000                                  \n 1st Qu.:0.01000                                  \n Median :0.03000                                  \n Mean   :0.05781                                  \n 3rd Qu.:0.07000                                  \n Max.   :0.68000                                  \n NA's   :305                                      \n Weekly Average Percent Influenza ICU Bed Occupancy\n Min.   :0.0000                                    \n 1st Qu.:0.0000                                    \n Median :0.0000                                    \n Mean   :0.0026                                    \n 3rd Qu.:0.0000                                    \n Max.   :0.0900                                    \n NA's   :839                                       \n Percent Adult COVID-19 Admissions Percent Pediatric COVID-19 Admissions\n Min.   :0.0000                    Min.   :0.0000                       \n 1st Qu.:0.9500                    1st Qu.:0.0200                       \n Median :0.9700                    Median :0.0300                       \n Mean   :0.9537                    Mean   :0.0454                       \n 3rd Qu.:0.9800                    3rd Qu.:0.0500                       \n Max.   :1.0000                    Max.   :1.0000                       \n NA's   :574                       NA's   :574                          \n Percent Hospitals Reporting Adult COVID-19 Admissions\n Min.   :0.0000                                       \n 1st Qu.:0.9600                                       \n Median :0.9800                                       \n Mean   :0.8984                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Pediatric COVID-19 Admissions\n Min.   :0.0000                                           \n 1st Qu.:0.9600                                           \n Median :0.9800                                           \n Mean   :0.8927                                           \n 3rd Qu.:1.0000                                           \n Max.   :1.0000                                           \n                                                          \n Percent Hospitals Reporting Influenza Admissions\n Min.   :0.0000                                  \n 1st Qu.:0.9200                                  \n Median :0.9700                                  \n Mean   :0.8312                                  \n 3rd Qu.:1.0000                                  \n Max.   :1.0000                                  \n                                                 \n Percent Hospitals Reporting Prevalent Influenza Hospitalizations\n Min.   :0.0000                                                  \n 1st Qu.:0.9200                                                  \n Median :0.9700                                                  \n Mean   :0.8313                                                  \n 3rd Qu.:1.0000                                                  \n Max.   :1.0000                                                  \n                                                                 \n Percent Hospitals Reporting Hospitalized Influenza ICU Patients\n Min.   :0.0000                                                 \n 1st Qu.:0.9200                                                 \n Median :0.9700                                                 \n Mean   :0.8312                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Inpatient Beds\n Min.   :0.0000                            \n 1st Qu.:0.9500                            \n Median :0.9800                            \n Mean   :0.8945                            \n 3rd Qu.:1.0000                            \n Max.   :1.0000                            \n                                           \n Percent Hospitals Reporting ICU Beds\n Min.   :0.0000                      \n 1st Qu.:0.9600                      \n Median :0.9800                      \n Mean   :0.8969                      \n 3rd Qu.:1.0000                      \n Max.   :1.0000                      \n                                     \n Percent Hospitals Reporting Inpatient Beds Occupied\n Min.   :0.0000                                     \n 1st Qu.:0.9500                                     \n Median :0.9800                                     \n Mean   :0.8945                                     \n 3rd Qu.:1.0000                                     \n Max.   :1.0000                                     \n                                                    \n Percent Hospitals Reporting ICU Beds Occupied\n Min.   :0.0000                               \n 1st Qu.:0.9500                               \n Median :0.9800                               \n Mean   :0.8881                               \n 3rd Qu.:1.0000                               \n Max.   :1.0000                               \n                                              \n Percent Hospitals Reporting Percent Inpatient Bed Occupancy\n Min.   :0.0000                                             \n 1st Qu.:0.9500                                             \n Median :0.9800                                             \n Mean   :0.8945                                             \n 3rd Qu.:1.0000                                             \n Max.   :1.0000                                             \n                                                            \n Percent Hospitals Reporting Percent ICU Bed Occupancy\n Min.   :0.0000                                       \n 1st Qu.:0.9500                                       \n Median :0.9800                                       \n Mean   :0.8881                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.0000                                                      \n 1st Qu.:0.9500                                                      \n Median :0.9800                                                      \n Mean   :0.8926                                                      \n 3rd Qu.:1.0000                                                      \n Max.   :1.0000                                                      \n                                                                     \n Percent Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\n Min.   :0.0000                                                       \n 1st Qu.:0.9200                                                       \n Median :0.9700                                                       \n Mean   :0.8293                                                       \n 3rd Qu.:1.0000                                                       \n Max.   :1.0000                                                       \n                                                                      \n Percent Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\n Min.   :0.0000                                                \n 1st Qu.:0.9600                                                \n Median :0.9800                                                \n Mean   :0.8916                                                \n 3rd Qu.:1.0000                                                \n Max.   :1.0000                                                \n                                                               \n Percent Hospitals Reporting Percent Influenza ICU Bed Occupancy\n Min.   :0.0000                                                 \n 1st Qu.:0.9200                                                 \n Median :0.9700                                                 \n Mean   :0.8305                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Total COVID-19 Admissions\n Min.   :0.0000                                       \n 1st Qu.:0.9600                                       \n Median :0.9800                                       \n Mean   :0.8985                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Prevalent COVID-19 Hospitalizations\n Min.   :0.0000                                                 \n 1st Qu.:0.9600                                                 \n Median :0.9800                                                 \n Mean   :0.8963                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Hospitalized COVID-19 ICU Patients\n Min.   :0.0000                                                \n 1st Qu.:0.9600                                                \n Median :0.9800                                                \n Mean   :0.8925                                                \n 3rd Qu.:1.0000                                                \n Max.   :1.0000                                                \n                                                               \n Absolute Change in the Percent Hospitals Reporting Adult COVID-19 Admissions from Prior Week\n Min.   :-100.0000                                                                           \n 1st Qu.:   0.0000                                                                           \n Median :   0.0000                                                                           \n Mean   :  -0.2037                                                                           \n 3rd Qu.:   0.0000                                                                           \n Max.   : 100.0000                                                                           \n NA's   :57                                                                                  \n Absolute Change in the Percent Hospitals Reporting Pediatric COVID-19 Admissions from Prior Week\n Min.   :-100.0000                                                                               \n 1st Qu.:   0.0000                                                                               \n Median :   0.0000                                                                               \n Mean   :  -0.1778                                                                               \n 3rd Qu.:   0.0000                                                                               \n Max.   : 100.0000                                                                               \n NA's   :57                                                                                      \n Absolute Change in the Percent Hospitals Reporting Influenza Admissions from Prior Week\n Min.   :-100.0000                                                                      \n 1st Qu.:   0.0000                                                                      \n Median :   0.0000                                                                      \n Mean   :   0.1491                                                                      \n 3rd Qu.:   0.0000                                                                      \n Max.   : 100.0000                                                                      \n NA's   :57                                                                             \n Absolute Change in the Percent Hospitals Reporting Prevalent Influenza Hospitalizations from Prior Week\n Min.   :-100.0000                                                                                      \n 1st Qu.:   0.0000                                                                                      \n Median :   0.0000                                                                                      \n Mean   :   0.1487                                                                                      \n 3rd Qu.:   0.0000                                                                                      \n Max.   : 100.0000                                                                                      \n NA's   :57                                                                                             \n Absolute Change in the Percent Hospitals Reporting Hospitalized Influenza ICU Patients from Prior Week\n Min.   :-100.0000                                                                                     \n 1st Qu.:   0.0000                                                                                     \n Median :   0.0000                                                                                     \n Mean   :   0.1485                                                                                     \n 3rd Qu.:   0.0000                                                                                     \n Max.   : 100.0000                                                                                     \n NA's   :57                                                                                            \n Absolute Change in the Percent Hospitals Reporting Inpatient Beds from Prior Week\n Min.   :-100.0000                                                                \n 1st Qu.:   0.0000                                                                \n Median :   0.0000                                                                \n Mean   :  -0.2326                                                                \n 3rd Qu.:   0.0000                                                                \n Max.   : 100.0000                                                                \n NA's   :57                                                                       \n Absolute Change in the Percent Hospitals Reporting ICU Beds from Prior Week\n Min.   :-100.0000                                                          \n 1st Qu.:   0.0000                                                          \n Median :   0.0000                                                          \n Mean   :  -0.2561                                                          \n 3rd Qu.:   0.0000                                                          \n Max.   : 100.0000                                                          \n NA's   :57                                                                 \n Absolute Change in the Percent Hospitals Reporting Inpatient Beds Occupied from Prior Week\n Min.   :-100.0000                                                                         \n 1st Qu.:   0.0000                                                                         \n Median :   0.0000                                                                         \n Mean   :  -0.2326                                                                         \n 3rd Qu.:   0.0000                                                                         \n Max.   : 100.0000                                                                         \n NA's   :57                                                                                \n Absolute Change in the Percent Hospitals Reporting ICU Beds Occupied from Prior Week\n Min.   :-100.0000                                                                   \n 1st Qu.:   0.0000                                                                   \n Median :   0.0000                                                                   \n Mean   :  -0.1627                                                                   \n 3rd Qu.:   0.0000                                                                   \n Max.   : 100.0000                                                                   \n NA's   :57                                                                          \n Absolute Change in the Percent Hospitals Reporting Percent Inpatient Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                                 \n 1st Qu.:   0.0000                                                                                 \n Median :   0.0000                                                                                 \n Mean   :  -0.2326                                                                                 \n 3rd Qu.:   0.0000                                                                                 \n Max.   : 100.0000                                                                                 \n NA's   :57                                                                                        \n Absolute Change in the Percent Hospitals Reporting Percent ICU Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                           \n 1st Qu.:   0.0000                                                                           \n Median :   0.0000                                                                           \n Mean   :  -0.1627                                                                           \n 3rd Qu.:   0.0000                                                                           \n Max.   : 100.0000                                                                           \n NA's   :57                                                                                  \n Absolute Change in the Percent Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                                          \n 1st Qu.:   0.0000                                                                                          \n Median :   0.0000                                                                                          \n Mean   :  -0.1973                                                                                          \n 3rd Qu.:   0.0000                                                                                          \n Max.   : 100.0000                                                                                          \n NA's   :57                                                                                                 \n Absolute Change in the Percent Hospitals Reporting Percent Influenza Inpatient Bed Occupancy from Prior Week\n Min.   :-100.0000                                                                                           \n 1st Qu.:   0.0000                                                                                           \n Median :   0.0000                                                                                           \n Mean   :   0.1487                                                                                           \n 3rd Qu.:   0.0000                                                                                           \n Max.   : 100.0000                                                                                           \n NA's   :57                                                                                                  \n Absolute Change in the Percent Hospitals Reporting Percent COVID-19 ICU Bed Occupancy from Prior Week\n Min.   :-100.000                                                                                     \n 1st Qu.:   0.000                                                                                     \n Median :   0.000                                                                                     \n Mean   :  -0.195                                                                                     \n 3rd Qu.:   0.000                                                                                     \n Max.   : 100.000                                                                                     \n NA's   :57                                                                                           \n Absolute Change in the Percent Hospitals Reporting Percent Influenza ICU Bed Occupancy from Prior Week\n Min.   :-100.000                                                                                      \n 1st Qu.:   0.000                                                                                      \n Median :   0.000                                                                                      \n Mean   :   0.142                                                                                      \n 3rd Qu.:   0.000                                                                                      \n Max.   : 100.000                                                                                      \n NA's   :57                                                                                            \n Absolute Change in the Percent Hospitals Reporting Total COVID-19 Admissions from Prior Week\n Min.   :-100.0000                                                                           \n 1st Qu.:   0.0000                                                                           \n Median :   0.0000                                                                           \n Mean   :  -0.2091                                                                           \n 3rd Qu.:   0.0000                                                                           \n Max.   : 100.0000                                                                           \n NA's   :57                                                                                  \n Absolute Change in the Percent Hospitals Reporting Prevalent COVID-19 Hospitalizations from Prior Week\n Min.   :-100.0000                                                                                     \n 1st Qu.:   0.0000                                                                                     \n Median :   0.0000                                                                                     \n Mean   :  -0.2261                                                                                     \n 3rd Qu.:   0.0000                                                                                     \n Max.   : 100.0000                                                                                     \n NA's   :57                                                                                            \n Absolute Change in the Percent Hospitals Reporting Hospitalized COVID-19 ICU Patients from Prior Week\n Min.   :-100.000                                                                                     \n 1st Qu.:   0.000                                                                                     \n Median :   0.000                                                                                     \n Mean   :  -0.201                                                                                     \n 3rd Qu.:   0.000                                                                                     \n Max.   : 100.000                                                                                     \n NA's   :57                                                                                           \n\nview(cdc_dataset)\nprint(cdc_dataset)\n\n# A tibble: 12,597 × 82\n   `Week Ending Date` `Geographic aggregation` `Hospital Reporting Days`\n   &lt;date&gt;             &lt;chr&gt;                                        &lt;dbl&gt;\n 1 2020-08-08         AK                                              98\n 2 2020-08-15         AK                                              98\n 3 2020-08-22         AK                                              98\n 4 2020-08-29         AK                                              97\n 5 2020-09-05         AK                                              97\n 6 2020-09-12         AK                                              98\n 7 2020-09-19         AK                                              98\n 8 2020-09-26         AK                                              96\n 9 2020-10-03         AK                                              97\n10 2020-10-10         AK                                              98\n# ℹ 12,587 more rows\n# ℹ 79 more variables: `Percent Hospital Reporting Days` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Adult COVID-19 Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Pediatric COVID-19 Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Influenza Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Prevalent Influenza Hospitalizations` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Hospitalized Influenza ICU Patients` &lt;dbl&gt;, …\n\n\nThe summary of the dataset is valuable but it helps to actually view and scroll through the data for a better overview and feel of the data.\nWe can see that this is a very large dataset. There are 12597 observations in this dataset and 82 columns. We have to do a lot of scrolling to see much, including our summary, which is not very helpful. This is very big for us to evaluate and so we might want to cut it down a bit before we spend time filtering through and cleaning all of the data that is not of interest to us.\nFor this evaluation we want to have an idea of how COVID-19 and infuenza affected the population by evaluating the hospitilization rates. We are interested in knowing how this affected the healthcare system so we will include the information related to how many hospitals reported data and the occupancy of these hospitals. From viewing the data there seems to be overlap and repetition of some of the data, as well as categories and definitions that are not necessary for us, so we will reduce our columns to those of greatest interest to us. We will focus on adult data as the reporting of pediatric data seems to be limited to COVID-19 and thus will limit any comparisons to influenza.\nWe are also not going to split up ICU vs general admissions for our evaluations, but this is something that could be broken down.\nWe will also cut down our observations by focusing on one State and by defining our time frame.\nThis part could be done in either order and the State and time frame could be adapted.\nCutting down our dataset: Since we know that the mandatory reporting period will have the best information reported, we will focus on this time frame August 1, 2020 - April 30, 2024. We will also start by focusing on the state of Georgia (region=GA). We will use the filter function to get the rows with these variables.\n\n#Using the filter function to select for our rows of interest. We will assign these to two new variables.\n#Filter by State = selecting GA\ncdc_data_georgia &lt;- cdc_dataset |&gt; filter(`Geographic aggregation` == \"GA\")\n\n#Filter further by date = August 1, 2020 - April 30, 2024\ncdc_georgia_mandatory_data &lt;- cdc_data_georgia |&gt; filter(`Week Ending Date`&gt;=\"2020-08-01\" & `Week Ending Date` &lt;= \"2024-04-30\")\n\nWe can see that we have filtered our observations down to 195 observations. We could do the same thing for multiple states (or have included multiple states) and compared the data between states.\nNow we will focus on selecting the columns that we are most interested in: We will use the select function. Columns of interest to be kept: - Week Ending Date - Geographic aggregation - Number Hospitals Reporting Adult COVID-19 Admissions - Number Hospitals Reporting Influenza Admissions - Weekly Total Adult COVID-19 Admissions - Weekly Total Influenza Admissions - Weekly Average Hospitalized COVID-19 ICU Patients - Weekly Average Hospitalized Influenza ICU Patients - Weekly Average Inpatient Beds - Weekly Average ICU Beds - Weekly Average Inpatient Beds Occupied - Weekly Average ICU Beds Occupied\nDefining our new dataset with columns of interest:\n\n#Using the select function to define our columns of interest for the GA dataset that we refined\ncdc_ga_mandatory_refined &lt;- cdc_georgia_mandatory_data |&gt; dplyr::select(`Week Ending Date`,\n`Geographic aggregation`, `Number Hospitals Reporting Adult COVID-19 Admissions`,\n`Number Hospitals Reporting Influenza Admissions`,\n`Weekly Total Adult COVID-19 Admissions`,\n`Weekly Total Influenza Admissions`,\n`Weekly Average Hospitalized COVID-19 ICU Patients`,\n`Weekly Average Hospitalized Influenza ICU Patients`,\n`Weekly Average Inpatient Beds`,\n`Weekly Average ICU Beds`,\n`Weekly Average Inpatient Beds Occupied`,\n`Weekly Average ICU Beds Occupied`)\n\n#View the new datset and a summary\nsummary(cdc_ga_mandatory_refined)\n\n Week Ending Date     Geographic aggregation\n Min.   :2020-08-08   Length:195            \n 1st Qu.:2021-07-13   Class :character      \n Median :2022-06-18   Mode  :character      \n Mean   :2022-06-18                         \n 3rd Qu.:2023-05-23                         \n Max.   :2024-04-27                         \n                                            \n Number Hospitals Reporting Adult COVID-19 Admissions\n Min.   : 87.0                                       \n 1st Qu.:137.0                                       \n Median :138.0                                       \n Mean   :137.5                                       \n 3rd Qu.:139.0                                       \n Max.   :140.0                                       \n                                                     \n Number Hospitals Reporting Influenza Admissions\n Min.   :  0.0                                  \n 1st Qu.:134.0                                  \n Median :137.0                                  \n Mean   :127.2                                  \n 3rd Qu.:139.0                                  \n Max.   :139.0                                  \n                                                \n Weekly Total Adult COVID-19 Admissions Weekly Total Influenza Admissions\n Min.   : 122.0                         Min.   :   0.00                  \n 1st Qu.: 410.5                         1st Qu.:  17.00                  \n Median : 816.0                         Median :  30.00                  \n Mean   :1259.5                         Mean   :  73.04                  \n 3rd Qu.:1446.5                         3rd Qu.:  54.75                  \n Max.   :5459.0                         Max.   :1123.00                  \n                                        NA's   :5                        \n Weekly Average Hospitalized COVID-19 ICU Patients\n Min.   :  28.57                                  \n 1st Qu.:  69.64                                  \n Median : 137.29                                  \n Mean   : 283.58                                  \n 3rd Qu.: 376.29                                  \n Max.   :1319.86                                  \n                                                  \n Weekly Average Hospitalized Influenza ICU Patients\n Min.   :  0.000                                   \n 1st Qu.:  2.895                                   \n Median : 31.570                                   \n Mean   : 45.580                                   \n 3rd Qu.: 71.395                                   \n Max.   :255.000                                   \n NA's   :5                                         \n Weekly Average Inpatient Beds Weekly Average ICU Beds\n Min.   :12754                 Min.   :2403           \n 1st Qu.:17263                 1st Qu.:3007           \n Median :17994                 Median :3043           \n Mean   :18361                 Mean   :3069           \n 3rd Qu.:20123                 3rd Qu.:3153           \n Max.   :20852                 Max.   :3422           \n                                                      \n Weekly Average Inpatient Beds Occupied Weekly Average ICU Beds Occupied\n Min.   :10065                          Min.   :1210                    \n 1st Qu.:13769                          1st Qu.:2315                    \n Median :14365                          Median :2395                    \n Mean   :14415                          Mean   :2377                    \n 3rd Qu.:15170                          3rd Qu.:2484                    \n Max.   :16660                          Max.   :2828                    \n                                                                        \n\nview(cdc_ga_mandatory_refined)\nprint(cdc_ga_mandatory_refined)\n\n# A tibble: 195 × 12\n   `Week Ending Date` `Geographic aggregation` Number Hospitals Reporting Adul…¹\n   &lt;date&gt;             &lt;chr&gt;                                                &lt;dbl&gt;\n 1 2020-08-08         GA                                                      87\n 2 2020-08-15         GA                                                     130\n 3 2020-08-22         GA                                                     130\n 4 2020-08-29         GA                                                     131\n 5 2020-09-05         GA                                                     133\n 6 2020-09-12         GA                                                     134\n 7 2020-09-19         GA                                                     129\n 8 2020-09-26         GA                                                     131\n 9 2020-10-03         GA                                                     139\n10 2020-10-10         GA                                                     139\n# ℹ 185 more rows\n# ℹ abbreviated name: ¹​`Number Hospitals Reporting Adult COVID-19 Admissions`\n# ℹ 9 more variables: `Number Hospitals Reporting Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Total Adult COVID-19 Admissions` &lt;dbl&gt;,\n#   `Weekly Total Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized COVID-19 ICU Patients` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized Influenza ICU Patients` &lt;dbl&gt;, …\n\n\nWe have reduced our datset to a much more manageable set of information that we can refine and evalaute. If we feel that we should incude an additional column (left something valuable out) we can go back and add it now. Our sumamry shows us that we have a few NA values in our datset.\nWe will look for these using the is.na function. We will create a specific variable from these NA values to make it easier to visualise without searching. In reality, with the summary and reduced size of the dataset we can scroll through and find these if we want. However, printing these specifically allows us to isolate them quickly.\n\ncdc_refined_ga_NA &lt;- cdc_ga_mandatory_refined |&gt; dplyr::filter(if_any(everything(), is.na))\n\nprint(cdc_refined_ga_NA)\n\n# A tibble: 5 × 12\n  `Week Ending Date` `Geographic aggregation` Number Hospitals Reporting Adult…¹\n  &lt;date&gt;             &lt;chr&gt;                                                 &lt;dbl&gt;\n1 2020-08-08         GA                                                       87\n2 2020-08-15         GA                                                      130\n3 2020-08-22         GA                                                      130\n4 2020-08-29         GA                                                      131\n5 2020-10-10         GA                                                      139\n# ℹ abbreviated name: ¹​`Number Hospitals Reporting Adult COVID-19 Admissions`\n# ℹ 9 more variables: `Number Hospitals Reporting Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Total Adult COVID-19 Admissions` &lt;dbl&gt;,\n#   `Weekly Total Influenza Admissions` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized COVID-19 ICU Patients` &lt;dbl&gt;,\n#   `Weekly Average Hospitalized Influenza ICU Patients` &lt;dbl&gt;,\n#   `Weekly Average Inpatient Beds` &lt;dbl&gt;, `Weekly Average ICU Beds` &lt;dbl&gt;, …\n\n\nWe can see that these NA values are in the first four weeks of observations, and one week in October 2020, and that they are all in the influenza observations. We might be led to believe that perhaps the recording/reporting (or testing) of influenza tests was halted in the peak of the COVID-19 pandemic when resources were limited, or that the order to report influenza cases for this use only cae through later. This is something that we might be able to find on the CDC website if desired. We will not delete these observations because we would lose valuable information related to the COVID-19 pandemic from these weeks. We could potentially use synthetic data for these missing variables or we could work on date ranges that do not include these weeks (2020-08-08 - 2020-08-29 and 2020-10-10) if we want to compare COVID-19 to Influenza.\nWe will make a few plots of different variables. We will start by analyzing the COVID-19 data.\n\n#We are looking at the distribution of the total adult COVID-19 cases  \ncovid_adult_admissions_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Total Adult COVID-19 Admissions`)) + geom_histogram(bins=10,fill= \"green\", color = \"black\") + labs(title = \"Distribution of the weekly total adult COVID-19 Hospital Admissions\", x = \"Weekly Total Adult COVID-19 Admissions\", y = \"Week counts\")\n\nprint(covid_adult_admissions_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of this data (Distribution of the weekly total adult COVID-19 Hospital Admissions) and calculate the standard deviation\n\nsummary(cdc_ga_mandatory_refined$`Weekly Total Adult COVID-19 Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  122.0   410.5   816.0  1259.5  1446.5  5459.0 \n\n#We will calculate the standard deviation\nsd(cdc_ga_mandatory_refined$`Weekly Total Adult COVID-19 Admissions`)\n\n[1] 1267.792\n\n\n\n#We are looking at the distribution of the average hospitalized COVID-19 ICU patients  \ncovid_icu_patients_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average Hospitalized COVID-19 ICU Patients`)) + geom_histogram(bins=10,fill= \"red\", color = \"black\") + labs(title = \"Distribution of the average COVID-19 Hospital ICU Patients\", x = \"Weekly Average COVID-19 ICU Patients\", y = \"Week counts\")\n\nprint(covid_icu_patients_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of this data (Distribution of the average COVID-19 Hospital ICU Patients) and calculate the standard deviation\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average Hospitalized COVID-19 ICU Patients`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.57   69.64  137.29  283.58  376.29 1319.86 \n\n#We will calculate the standard deviation\nsd(cdc_ga_mandatory_refined$`Weekly Average Hospitalized COVID-19 ICU Patients`)\n\n[1] 315.2279\n\n\nWe see how many more patients were hospitalized than were in ICU in Georgia, as the max number for the ICU patients is quite close to mean value of the hospital admissions.\nLooking at the same distributions for the Influenza data:\n\n#We are looking at the distribution of the total Influenza cases  \ninfluenza_adult_admissions_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Total Influenza Admissions`)) + geom_histogram(bins=10,fill= \"blue\", color = \"black\") + labs(title = \"Distribution of the weekly Influenza Hospital Admissions\", x = \"Weekly Total Influenza Admissions\", y = \"Week counts\")\n\nprint(influenza_adult_admissions_plot)\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe receive a warning that some values were excluded (likely our NA values).\nWe will get a summary of this data (Weekly Total Influenza Admissions) and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Total Influenza Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   17.00   30.00   73.04   54.75 1123.00       5 \n\n#We will calculate the standard deviation and exclude the NA values in the influenza data\nsd(cdc_ga_mandatory_refined$`Weekly Total Influenza Admissions`, na.rm = TRUE)  \n\n[1] 137.6914\n\n\nThe mean value for admissions for the influenza cases is a great deal lower than that from the COVID-19 cases. We see that the mean of the COVID-19 hospital admissions is 17.2 times greater than the mean from the influenza admissions.\n\n#We are looking at the distribution of the average influenza ICU patients  \ninfluenza_icu_patients_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average Hospitalized Influenza ICU Patients`)) + geom_histogram(bins=10,fill= \"red\", color = \"black\") + labs(title = \"Distribution of the average Influenza Hospital ICU Patients\", x = \"Weekly Average Influenza ICU Patients\", y = \"Week counts\")\n\nprint(influenza_icu_patients_plot)\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe get the same warning as our previous plot for the influenza cases (regarding the missing NA values).\nWe will get a summary of this data (Weekly Average Influenza ICU Patients) and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average Hospitalized Influenza ICU Patients`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   2.895  31.570  45.580  71.395 255.000       5 \n\n#We will calculate the standard deviation and exclude the NA values in the influenza data\nsd(cdc_ga_mandatory_refined$`Weekly Average Hospitalized Influenza ICU Patients`, na.rm = TRUE)\n\n[1] 51.3058\n\n\nThe mean value for ICU patients for the influenza cases is also much lower than that from the COVID-19 cases (not quite as much compared to admissions). We see that the mean of the COVID-19 hospital admissions is 6.2 times greater than the mean from the influenza ICU patients.\nBy comparing the plots and summary data we generated for COVID-19 and Influenza cases we can already see the much greater number of hospital admissions and ICU cases from COVID-19 compared to influenza. We also see the wider range for the COVID-19 cases compared to Influenza. The pronounced impact of COVID-19 as a respiratory disease is obvious before we do further analysis.\nWe can evaluate the strain on the healthcare system by looking at the hospital beds that were occupied (we will just look at total numbers between inpatient and ICU and not as a percentage). This does not distinguish what caused the beds to be occupied.\nLooking at the distributions of occupied inptient beds:\n\n#We are looking at the distribution of the occupied inptient beds  \ncdc_inpatient_occupied_beds_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average Inpatient Beds Occupied`)) + geom_histogram(bins=10,fill= \"blue\", color = \"black\") + labs(title = \"Distribution of the weekly Average Occupied Inpatient Beds\", x = \"Weekly Inpatient Beds\", y = \"Week counts\")\n\nprint(cdc_inpatient_occupied_beds_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of the inpatient occupied beds data and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average Inpatient Beds Occupied`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10065   13769   14365   14415   15170   16660 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Weekly Average Inpatient Beds Occupied`)\n\n[1] 1097.536\n\n\n\n#We are looking at the distribution of the occupied ICU beds  \ncdc_icu_occupied_beds_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Weekly Average ICU Beds Occupied`)) + geom_histogram(bins=10,fill= \"red\", color = \"black\") + labs(title = \"Distribution of the weekly Average Occupied ICU Beds\", x = \"Weekly Occupied ICU Beds\", y = \"Week counts\")\n\nprint(cdc_icu_occupied_beds_plot)\n\n\n\n\n\n\n\n\nWe will get a summary of the occupied ICU beds data and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Weekly Average ICU Beds Occupied`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1210    2315    2395    2377    2484    2828 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Weekly Average ICU Beds Occupied`)\n\n[1] 237.6443\n\n\nWe can look at the occupancy of the inpatient hosptial beds by week for the period evaluated, we will generate a line plot:\n\n#We are looking at the distribution of the occupied inptient beds over time \ncdc_inpatient_beds_time_plot&lt;- ggplot(cdc_ga_mandatory_refined, aes(x=`Week Ending Date`, y=`Weekly Average Inpatient Beds Occupied`)) + geom_line(color = \"black\") + labs(title = \"Average Occupancy of Inpatient Beds over time\", x = \"Week Evaluated\", y = \"Weekly Occupied Inpatient Beds\")\n\nprint(cdc_inpatient_beds_time_plot)\n\n\n\n\n\n\n\n\nWe will provide a summary of the number of hospitals reporting COVID-19 and influenza respectively without plotting a graph.\nWe will get a summary of the number of hospital reporting Adult COVID-19 Admissions and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Number Hospitals Reporting Adult COVID-19 Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   87.0   137.0   138.0   137.5   139.0   140.0 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Number Hospitals Reporting Adult COVID-19 Admissions`)\n\n[1] 4.080754\n\n\nWe will get a summary of the number of hospital reporting Influenza Admissions and calculate the standard deviation.\n\nsummary(cdc_ga_mandatory_refined$`Number Hospitals Reporting Influenza Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   134.0   137.0   127.2   139.0   139.0 \n\n#We will calculate the standard deviation \nsd(cdc_ga_mandatory_refined$`Number Hospitals Reporting Influenza Admissions`)\n\n[1] 31.62036\n\n\nWe see very similar values for influenza and COVID-19, although we realize that the 0 values in the influenza cases might be an artifact carried by the original creators from the NA values. We will create a new variable looking at the number of hospitals reporting influenza cases without these entries.\n\n#We are using a loop function to first select for rows where there are no NA values, this is based on using one of the columns that we previously saw had an NA value. We then select the rows to only keep the influenza data\n\nfiltered_infuenza_cases&lt;- cdc_ga_mandatory_refined |&gt; dplyr::filter(!is.na(`Weekly Total Influenza Admissions`)) |&gt; \n  dplyr::select(`Week Ending Date`, `Number Hospitals Reporting Influenza Admissions`, `Weekly Total Influenza Admissions`,`Weekly Average Hospitalized Influenza ICU Patients`)\n\nWe can now run the previous summary of the influenza reported cases by hospitals again and compare results\n\nsummary(filtered_infuenza_cases$`Number Hospitals Reporting Influenza Admissions`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0   135.0   137.0   130.6   139.0   139.0 \n\n#We will calculate the standard deviation \nsd(filtered_infuenza_cases$`Number Hospitals Reporting Influenza Admissions`)\n\n[1] 24.22654\n\n\nWe see that the values did change, with the standard deviation decreasing by around 7, but it perhaps did not decrease by as much as expected because the minimum value is now 1 instead of 0."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#some-further-eda",
    "href": "cdcdata-exercise/cdcdata-exercise.html#some-further-eda",
    "title": "CDC Data Exercise",
    "section": "Some further EDA",
    "text": "Some further EDA\nVariables:\n\nWeek Ending Date: from 2020-08-08 to 2024-04-27\nGeographic Aggregation: GA\nNumber Hospitals Reporting Adult COVID-19 Admissions: Min: 87, Mean: 137.5, Max: 140,\nNumber Hospitals Reporting Influenza Admissions: Min: 0, Mean: 127.2, Max: 139\nWeekly Total Adult COVID-19 Admissions: Min: 122, Mean: 1259.5, Max: 5459\nWeekly Total Influenza Admissions: Min: 0, Mean: 73.04, Max: 1123\nWeekly Average Hospitalized COVID-19 ICU Patients: Min: 28.57, Mean: 283.58, Max: 1319.86\nWeekly Average Hospitalized Influenza ICU Patients: Min: 0, Mean: 45.58, Max: 255\nWeekly Average Inpatient Beds: Min: 12754, Mean: 18361, Max: 20852\nWeekly Average ICU Beds: Min: 2403, Mean: 3043, Max: 3422\nWeekly Average Inpatient Beds Occupied: Min: 10065, Mean: 14415, Max: 16660\nWeekly Average ICU Beds Occupied: Min: 1210, Mean: 2377, Max: 2828\n\nIt seems plausible that many of these variables will be associated with time due to the COVID-19 pandemic and general fluctuations of the influenza season. Explore the changes over time for all variables.\n\n# rename variables for working convenience\ndat &lt;- cdc_ga_mandatory_refined %&gt;%\n  rename(\n    date = `Week Ending Date`,\n    state = `Geographic aggregation`,\n    hosp_report_covid = `Number Hospitals Reporting Adult COVID-19 Admissions`,\n    hosp_report_flu = `Number Hospitals Reporting Influenza Admissions`,\n    num_covid_admin = `Weekly Total Adult COVID-19 Admissions`,\n    num_flu_admin = `Weekly Total Influenza Admissions`,\n    avg_hosp_covid = `Weekly Average Hospitalized COVID-19 ICU Patients`,\n    avg_hosp_flu = `Weekly Average Hospitalized Influenza ICU Patients`,\n    avg_inpat_bed = `Weekly Average Inpatient Beds`,\n    avg_icu_bed = `Weekly Average ICU Beds`,\n    avg_inpat_bed_ocp = `Weekly Average Inpatient Beds Occupied`,\n    avg_icu_bed_ocp = `Weekly Average ICU Beds Occupied`\n  )\n\n\n# Plot date by reporting COVID hospitals\nplot(dat$date, dat$hosp_report_covid)  \n# One outlier of 87 in the first week (2020-08-08)  \nplot(dat$date, dat$hosp_report_covid, ylim = c(130, 140))  \n# Constant at 139, drops off at the end to about 138, 137, 136, 134  \n\n# Plot date by reporting flu hospitals\nplot(dat$date, dat$hosp_report_flu)  \n# First 11 observations are between 0-2  \nplot(dat$date, dat$hosp_report_flu, ylim = c(100, 140))  \n# Cluster around 120-125 from 08-2021 to 01-2022  \nplot(dat$date, dat$hosp_report_flu, ylim = c(130, 140))  \n# Constant at 139 in 2021-2022, then drops to 138, 137, and 134  \n\n# Plot date by number of COVID admissions\nplot(dat$date, dat$num_covid_admin)  \n# Clear spikes: Jan-Feb 2021, Aug-Sep 2021, Jan 2022  \n# Smaller spikes: Jul-Aug 2022, Jan 2023  \n\n# Plot date by number of flu admissions\nplot(dat$date, dat$num_flu_admin)  \n# Peaks in January: 25 (2021), 50 (2022), 80 (2023), 250 (2023), 800 (2024)  \n\n# Plot date by average hospitalized COVID patients\nplot(dat$date, dat$avg_hosp_covid)  \n# Lower bounds: about 400 (2020-2021), about 25 (2022-2024)  \n# Peaks: Jan 2021 (1200), Sep 2021 (1300), Late Jan 2022 (900),  \n# Sep 2022 (200), Jan 2023 (200), Sep 2023 (100), Jan 2024 (150)  \n\n# Plot date by average hospitalized flu patients\nplot(dat$date, dat$avg_hosp_flu)  \n# Slight exponential increase from 0 to 60 (start to Nov 2022)  \n# Noticeable peak in Jan 2023 (100), constant at 90 (2023)  \n# Another peak in Jan 2024, then drops back to 100  \n\n# Plot date by average inpatient beds\nplot(dat$date, dat$avg_inpat_bed)  \n# Slight increase followed by a tapered decrease (start to end of 2022)  \n# Starts at 16,000, peaks at 19,000, then drops back to 16,000  \n# Big jump at the start of 2023, lower at 19,500,  \n# Peaks in March (20,500), drops to 20,000 (Aug)  \n# Peaks again in Jan 2024 (20,600), then declines until April  \n\n# Plot date by average ICU beds\nplot(dat$date, dat$avg_icu_bed)  \n# Starts at 2,400, peaks at 3,400 (Feb 2021)  \n# Gradual decline to 3000 (Nov 2022), trough in Dec 2022 (2800)  \n# Rises back to 3000 (Feb 2023), remains constant thereafter  \n\n# Plot date by average inpatient beds occupied\nplot(dat$date, dat$avg_inpat_bed_ocp, ylim = c(14000, 17000))  \n# Starts at 10000, rises to 13000 (Oct 2020)  \n# Stays variable between 13000-15000 until Dec 2022  \n# Jumps to 15000 (Jan 2023), peaks at 16000 (end of Jan)  \n# Trough in July (15000), peaks again in Jan 2024 (16,500)  \n# Declines to 15500 at the end of data collection  \n\n# Plot date by average ICU beds occupied\nplot(dat$date, dat$avg_icu_bed_ocp, ylim = c(1000, 3000))  \n# Starts at about 1500, peaks at 2,600 (Jan 2021)  \n# Remains fairly constant with seasonal variation (peaks in winter, troughs in summer) \n\nNow that we have general idea of how the data looks with time, lets look at the correlations in the data\n\ncor &lt;- cor(dat %&gt;% select(-date, -state) %&gt;% na.omit())\n# correlations of &gt;= 0.7 include: \n# num_covid_admin and avg_hosp_covid have cor = 0.93\n# num_flu_admin and avg_hosp_flu have cor = 0.77,\n# avg_hosp_flu and avg_inpat_bed have cor = 0.73,\n# avg_inpat_bed and avg_inpat_bed_ocp have cor = 0.89,\n# avg_icu_bed and avg_icu_bed_ocp have cor 0.70"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#generate-synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#generate-synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Generate synthetic data",
    "text": "Generate synthetic data\nPer the encouragement in the instructions for this exercise, I asked ChatGPT to create my code. I first presented the code and text above and asked it to interpret it and provide an “ideal input” for actually simulating the data. It provided the following prompt:\n“Generate a synthetic dataset that mirrors the statistical properties and temporal trends of real hospitalization data in Georgia from August 2020 to April 2024. The dataset should include the following variables:\n\nWeek Ending Date: Weekly timestamps from 2020-08-08 to 2024-04-27.\nGeographic Aggregation: Constant value “GA”.\nNumber of Hospitals Reporting Adult COVID-19 Admissions: Typically stable around 137-140, except an initial outlier at 87 in August 2020 and slight declines in later years.\nNumber of Hospitals Reporting Influenza Admissions: Initially low (0-2 for first 11 weeks), stabilizing around 120-139, with some seasonal variations.\nWeekly Total Adult COVID-19 Admissions: Clear spikes in Jan-Feb 2021, Aug-Sep 2021, Jan 2022, with smaller spikes in Jul-Aug 2022, Jan 2023, and Jan 2024.\nWeekly Total Influenza Admissions: Peaks in January, increasing year over year (25 in 2021, 50 in 2022, 80 in 2023, 800 in 2024).\nWeekly Average Hospitalized COVID-19 ICU Patients: High peaks in Jan 2021 (), Sep 2021 (), and Jan 2022 (), then decreasing post-2022.\nWeekly Average Hospitalized Influenza ICU Patients: Gradual increase until Nov 2022 (), peak in Jan 2023 (), stable at before another peak in Jan 2024.\nWeekly Average Inpatient Beds: General increasing trend with fluctuations, peaking in early 2023 (,500), dropping in late 2023 before another peak in early 2024.\nWeekly Average ICU Beds: Peaked at 3400 in Feb 2021, then a gradual decline stabilizing around 3000.\nWeekly Average Inpatient Beds Occupied: Increased steadily until 2022, peaked in Jan 2023 (,000), then fluctuated seasonally.\nWeekly Average ICU Beds Occupied: Seasonal peaks in winter, troughs in summer, high correlation with ICU bed availability.”\n\nI then asked ChatGPT to proceed with providing the code to produce the synthetic data. It gave me the following:\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(Matrix)\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Define date range\ndates &lt;- seq(ymd(\"2020-08-08\"), ymd(\"2024-04-27\"), by = \"week\")\nn &lt;- length(dates)\n\n# Generate synthetic reporting hospitals (with slight variations)\nhosp_report_covid &lt;- c(87, rep(139, n - 1)) + rnorm(n, 0, 1)\nhosp_report_flu &lt;- c(rep(0, 11), rep(125, n - 11)) + rnorm(n, 0, 5)\n\n# Create seasonal trends\nseasonal_covid &lt;- 500 * sin(2 * pi * (month(dates) / 12))\nseasonal_flu &lt;- 200 * sin(2 * pi * ((month(dates) - 1) / 12))\n\n# Define mean values\nmeans &lt;- c(1259.5, 73.04, 283.58, 45.58, 18361, 3043, 14415, 2377)\n\n# Define standard deviations\nsds &lt;- c(1000, 200, 500, 50, 2000, 300, 1500, 500)\n\n# Define correlation matrix\ncor_matrix &lt;- matrix(\n  c(1, 0.5, 0.93, 0.3, 0.2, 0.1, 0.4, 0.3,\n    0.5, 1, 0.3, 0.77, 0.3, 0.2, 0.4, 0.3,\n    0.93, 0.3, 1, 0.4, 0.3, 0.2, 0.5, 0.4,\n    0.3, 0.77, 0.4, 1, 0.73, 0.5, 0.4, 0.3,\n    0.2, 0.3, 0.3, 0.73, 1, 0.7, 0.89, 0.6,\n    0.1, 0.2, 0.2, 0.5, 0.7, 1, 0.5, 0.7,\n    0.4, 0.4, 0.5, 0.4, 0.89, 0.5, 1, 0.7,\n    0.3, 0.3, 0.4, 0.3, 0.6, 0.7, 0.7, 1),\n  nrow = 8\n)\n\n# Make correlation matrix positive definite\ncor_matrix_pd &lt;- nearPD(cor_matrix, corr = TRUE)$mat\n\n# Generate correlated synthetic data using multivariate normal distribution\ncov_matrix &lt;- diag(sds) %*% as.matrix(cor_matrix_pd) %*% diag(sds)\nsynthetic_data &lt;- mvrnorm(n, mu = means, Sigma = cov_matrix)\n\n# Convert to data frame\ndat_synth &lt;- data.frame(\n  date = dates,\n  state = \"GA\",\n  hosp_report_covid = round(hosp_report_covid),\n  hosp_report_flu = round(hosp_report_flu),\n  num_covid_admin = round(synthetic_data[, 1] + seasonal_covid),\n  num_flu_admin = round(synthetic_data[, 2] + seasonal_flu),\n  avg_hosp_covid = round(synthetic_data[, 3]),\n  avg_hosp_flu = round(synthetic_data[, 4]),\n  avg_inpat_bed = round(synthetic_data[, 5]),\n  avg_icu_bed = round(synthetic_data[, 6]),\n  avg_inpat_bed_ocp = round(synthetic_data[, 7]),\n  avg_icu_bed_ocp = round(synthetic_data[, 8])\n)\n\n# Ensure values stay within realistic bounds\ndat_synth &lt;- dat_synth %&gt;%\n  mutate(\n    num_covid_admin = pmax(num_covid_admin, 122),\n    num_flu_admin = pmax(num_flu_admin, 0),\n    avg_hosp_covid = pmax(avg_hosp_covid, 28),\n    avg_hosp_flu = pmax(avg_hosp_flu, 0),\n    avg_inpat_bed = pmax(avg_inpat_bed, 12754),\n    avg_icu_bed = pmax(avg_icu_bed, 2403),\n    avg_inpat_bed_ocp = pmax(avg_inpat_bed_ocp, 10065),\n    avg_icu_bed_ocp = pmax(avg_icu_bed_ocp, 1210)\n  )\n\n# View the first few rows\nhead(dat_synth)\n\n        date state hosp_report_covid hosp_report_flu num_covid_admin\n1 2020-08-08    GA                86              10            1143\n2 2020-08-15    GA               139               3            1058\n3 2020-08-22    GA               141              -6            1176\n4 2020-08-29    GA               139              -3             754\n5 2020-09-05    GA               139              -6            2391\n6 2020-09-12    GA               141              11            3042\n  num_flu_admin avg_hosp_covid avg_hosp_flu avg_inpat_bed avg_icu_bed\n1            43            367           42         19299        3293\n2           417            458          143         22447        3496\n3             0            561           14         16353        2695\n4           103            418          121         20512        3355\n5            64           1117          102         19348        3084\n6            31            900           10         14572        2403\n  avg_inpat_bed_ocp avg_icu_bed_ocp\n1             15625            3090\n2             17691            3509\n3             13328            2144\n4             14948            2578\n5             14974            2468\n6             12839            1214\n\n\nIn the following sections, I’ll compare the synthetic data with the real data and evaluate."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#variables-by-time",
    "href": "cdcdata-exercise/cdcdata-exercise.html#variables-by-time",
    "title": "CDC Data Exercise",
    "section": "Variables by time",
    "text": "Variables by time\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = hosp_report_covid), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = hosp_report_covid), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = hosp_report_flu), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = hosp_report_flu), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = num_covid_admin), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = num_covid_admin), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = num_flu_admin), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = num_flu_admin), color = \"red\", size = 2) +\n  theme_minimal()\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = avg_hosp_covid), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = avg_hosp_covid), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = avg_hosp_flu), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = avg_hosp_flu), color = \"red\", size = 2) +\n  theme_minimal()\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = avg_inpat_bed), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = avg_inpat_bed), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = avg_icu_bed), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = avg_icu_bed), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = avg_inpat_bed_ocp), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = avg_inpat_bed_ocp), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n# plot real data and synthetic data to compare\nggplot() +\n  geom_point(data = dat, aes(x = date, y = avg_icu_bed_ocp), color = \"blue\", size = 2) +\n  geom_point(data = dat_synth, aes(x = date, y = avg_icu_bed_ocp), color = \"red\", size = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSome of the variables are decent but most lack the more granular trends seen in the real data. Unfortunately, I was expecting this. The data we are working with are very unique in nature because they are from such a unique time. Surely generating data with more precise patterns is possible but it would require more equations."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#variable-distributions",
    "href": "cdcdata-exercise/cdcdata-exercise.html#variable-distributions",
    "title": "CDC Data Exercise",
    "section": "Variable distributions",
    "text": "Variable distributions\nThese plots correspond to the distribution plots of the real data in part 1.\n\nggplot(dat_synth, aes(x=num_covid_admin)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(dat_synth, aes(x=avg_hosp_covid)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(dat_synth, aes(x=num_flu_admin)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(dat_synth, aes(x=avg_hosp_flu)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(dat_synth, aes(x=avg_icu_bed)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThese plots are decent. None are extremely bad in comparison to the real data but none are great."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#summary-statistics",
    "href": "cdcdata-exercise/cdcdata-exercise.html#summary-statistics",
    "title": "CDC Data Exercise",
    "section": "Summary statistics",
    "text": "Summary statistics\nThese correspond to the summary statistics produced in part 1.\n\nsummary(dat_synth$hosp_report_covid)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   86.0   138.0   139.0   138.7   140.0   142.0 \n\nsummary(dat_synth$hosp_report_flu)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   -6.0   121.0   125.0   118.2   128.0   138.0 \n\n\nThe summary statistics for these two variables in the synthetic data are actually pretty good! This is likely due to the fact that the synthetic data was largely based on such measurements. Though I find this interesting because we know from the plots above that the synthetic data isn’t a great representation of the real data."
  },
  {
    "objectID": "presentation-exercise/presentation_exercise.html",
    "href": "presentation-exercise/presentation_exercise.html",
    "title": "presentation-exercise",
    "section": "",
    "text": "I will be attempting to recreate graphs from an article published by fivethirtyeight.com about contestants in the television shows, The Bachelor and Bachelorette. The article and data demonstrates a pattern between roses that are received in the early stages of the show and the performance of a candidate overall. The data evaluates all 33 seasons of the show.\nThe article may be found here: https://fivethirtyeight.com/features/the-bachelorette/\nThe raw data may be found here: https://github.com/fivethirtyeight/data/tree/master/bachelorette\nDisclaimer: I do not know anything about this show and have not watched any episodes, so forgive me if my explanations are not entirely on point. I chose this article because the graphs and topic seemed fun and light-hearted amongst all the political content.\n\n\nFirst I will load all of the required packages and well as read the raw data CSV file:\n\n#install.packages(\"ggforce\")\n#install.packages(\"packcircles\")\n#install.packages(\"ggrepel\")\n#install.packages(\"viridis\")\n#install.packages(\"kableExtra\")\n\n\nlibrary(\"ggplot2\")\nlibrary(\"tidyverse\")\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"knitr\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggforce\")\n\nWarning: package 'ggforce' was built under R version 4.4.2\n\nlibrary(\"packcircles\")\n\nWarning: package 'packcircles' was built under R version 4.4.2\n\nlibrary(\"ggrepel\")\n\nWarning: package 'ggrepel' was built under R version 4.4.2\n\nlibrary(\"viridis\")\n\nWarning: package 'viridis' was built under R version 4.4.2\n\n\nLoading required package: viridisLite\n\nlibrary(\"kableExtra\")\n\nWarning: package 'kableExtra' was built under R version 4.4.2\n\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nbachelorette_data &lt;- read_csv(here::here(\n  \"presentation-exercise/bachelorette.csv\"))\n\nRows: 920 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): SHOW, SEASON, CONTESTANT, ELIMINATION-1, ELIMINATION-2, ELIMINATIO...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(bachelorette_data)\n\n     SHOW              SEASON           CONTESTANT        ELIMINATION-1     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-2      ELIMINATION-3      ELIMINATION-4      ELIMINATION-5     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-6      ELIMINATION-7      ELIMINATION-8      ELIMINATION-9     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-10       DATES-1            DATES-2            DATES-3         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-4            DATES-5            DATES-6            DATES-7         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-8            DATES-9            DATES-10        \n Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n\n\nPerform some formating of the data to remove the first row which is a repetition of headings and to change the seasons to numeric values. We are also removing rows where headings are repeated within the data:\n\n# Remove the incorrect header row if necessary\nbachelorette_data &lt;- bachelorette_data[-1, ]  # Remove first row if it's incorrect\n\n# Convert SEASON to numeric\nbachelorette_data$SEASON &lt;- as.numeric(bachelorette_data$SEASON)\n\nWarning: NAs introduced by coercion\n\n# Remove rows where the CONTESTANT column contains \"ID\"\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  filter(CONTESTANT != \"ID\")\n\n# Reset row indices (optional)\nbachelorette_data &lt;- bachelorette_data %&gt;% mutate(row_id = row_number())\n\n# Display the cleaned data\nprint(head(bachelorette_data))\n\n# A tibble: 6 × 24\n  SHOW         SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n1 Bachelorette     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n2 Bachelorette     13 13_PETER_K &lt;NA&gt;            R               &lt;NA&gt;           \n3 Bachelorette     13 13_ERIC_B  &lt;NA&gt;            &lt;NA&gt;            R              \n4 Bachelorette     13 13_DEAN_U  &lt;NA&gt;            R               &lt;NA&gt;           \n5 Bachelorette     13 13_ADAM_G  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n6 Bachelorette     13 13_MATT_M  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 18 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;, `DATES-1` &lt;chr&gt;,\n#   `DATES-2` &lt;chr&gt;, `DATES-3` &lt;chr&gt;, `DATES-4` &lt;chr&gt;, `DATES-5` &lt;chr&gt;,\n#   `DATES-6` &lt;chr&gt;, `DATES-7` &lt;chr&gt;, `DATES-8` &lt;chr&gt;, `DATES-9` &lt;chr&gt;,\n#   `DATES-10` &lt;chr&gt;, row_id &lt;int&gt;\n\n\nSome information was provided on the website with the raw data which may help with interpretation of this data:\n\n#Adding additional data into a table\ncolumn_names &lt;- c(\"Header\", \"Description\")\ntable_data &lt;- data.frame(\n  Header = c(\n    \"SHOW\", \"SEASON\", \"CONTESTANT\", \"ELIMINATION-1\", \"ELIMINATION-2\",\n    \"ELIMINATION-3\", \"ELIMINATION-4\", \"ELIMINATION-5\", \"ELIMINATION-6\",\n    \"ELIMINATION-7\", \"ELIMINATION-8\", \"ELIMINATION-9\", \"ELIMINATION-10\",\n    \"DATES-1\", \"DATES-2\", \"DATES-3\", \"DATES-4\", \"DATES-5\", \"DATES-6\",\n    \"DATES-7\", \"DATES-8\", \"DATES-9\", \"DATES-10\"\n  ),\n  Description = c(\n    \"Bachelor or Bachelorette\", \"Which season\", \"An identifier for the contestant in a given season\",\n    \"Who was eliminated in week 1\", \"Who was eliminated in week 2\", \"Who was eliminated in week 3\",\n    \"Who was eliminated in week 4\", \"Who was eliminated in week 5\", \"Who was eliminated in week 6\",\n    \"Who was eliminated in week 7\", \"Who was eliminated in week 8\", \"Who was eliminated in week 9\",\n    \"Who was eliminated in week 10\", \"Who was on which date in week 1\", \"Who was on which date in week 2\",\n    \"Who was on which date in week 3\", \"Who was on which date in week 4\", \"Who was on which date in week 5\",\n    \"Who was on which date in week 6\", \"Who was on which date in week 7\", \"Who was on which date in week 8\",\n    \"Who was on which date in week 9\", \"Who was on which date in week 10\"\n  )\n)\n\n# Print table using kable for better formatting\nknitr::kable(table_data, col.names = column_names, caption = \"Table of Data Headers and Descriptions\")\n\n\nTable of Data Headers and Descriptions\n\n\nHeader\nDescription\n\n\n\n\nSHOW\nBachelor or Bachelorette\n\n\nSEASON\nWhich season\n\n\nCONTESTANT\nAn identifier for the contestant in a given season\n\n\nELIMINATION-1\nWho was eliminated in week 1\n\n\nELIMINATION-2\nWho was eliminated in week 2\n\n\nELIMINATION-3\nWho was eliminated in week 3\n\n\nELIMINATION-4\nWho was eliminated in week 4\n\n\nELIMINATION-5\nWho was eliminated in week 5\n\n\nELIMINATION-6\nWho was eliminated in week 6\n\n\nELIMINATION-7\nWho was eliminated in week 7\n\n\nELIMINATION-8\nWho was eliminated in week 8\n\n\nELIMINATION-9\nWho was eliminated in week 9\n\n\nELIMINATION-10\nWho was eliminated in week 10\n\n\nDATES-1\nWho was on which date in week 1\n\n\nDATES-2\nWho was on which date in week 2\n\n\nDATES-3\nWho was on which date in week 3\n\n\nDATES-4\nWho was on which date in week 4\n\n\nDATES-5\nWho was on which date in week 5\n\n\nDATES-6\nWho was on which date in week 6\n\n\nDATES-7\nWho was on which date in week 7\n\n\nDATES-8\nWho was on which date in week 8\n\n\nDATES-9\nWho was on which date in week 9\n\n\nDATES-10\nWho was on which date in week 10\n\n\n\n\n\nFurther information required for interpretation and processing:\nEliminates connote either an elimination (starts with “E”) or a rose (starts with “R”). Eliminations supercede roses. “E” connotes a standard elimination, typically at a rose ceremony. “EQ” means the contestant quits. “EF” means the contestant was fired by production. “ED” connotes a date elimination. “EU” connotes an unscheduled elimination, one that takes place at a time outside of a date or rose ceremony. “R” means the contestant received a rose. “R1” means the contestant got a first impression rose. “D1” means a one-on-one date, “D2” means a 2-on-1, “D3” means a 3-on-1 group date, and so on. “W” in E10 indicates that the contestant won. Weeks of the show are deliminated by rose ceremonies, and may not line up exactly with episodes.\n………………….\nI will be using ChatGPT to help me to generate the code that will help to obtain the graph that I want. I will post my prompts and the outcome as well as the different iterations below.\nWe are not getting the correct number of winners\n\n# Get all ELIMINATION columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Count occurrences of \"W\" in each ELIMINATION column\nw_counts &lt;- sapply(elim_columns, function(col) sum(bachelorette_data[[col]] == \"W\", na.rm = TRUE))\n\n# Print counts for each column\nprint(\"Number of 'W' occurrences per ELIMINATION column:\")\n\n[1] \"Number of 'W' occurrences per ELIMINATION column:\"\n\nprint(w_counts)\n\n ELIMINATION-1  ELIMINATION-2  ELIMINATION-3  ELIMINATION-4  ELIMINATION-5 \n             0              0              0              0              0 \n ELIMINATION-6  ELIMINATION-7  ELIMINATION-8  ELIMINATION-9 ELIMINATION-10 \n             2              8              7              1             15 \n\n# Total winners found in dataset\ntotal_winners &lt;- sum(w_counts)\nprint(paste(\"Total expected winners:\", total_winners))\n\n[1] \"Total expected winners: 33\"\n\n\nNow we found that it seems that winners were marked in different levels of the competition (Elimination 6 - 10 and not just the last two). We will run this again:\n\n# Define function to classify contestants, now checking ELIMINATION-6 to ELIMINATION-10\nclassify_contestant &lt;- function(row) {\n  elim_columns &lt;- grep(\"ELIMINATION-\", names(row), value = TRUE)\n\n  # **Ensure all ELIMINATION columns are checked for \"W\"**\n  w_found &lt;- any(sapply(elim_columns, function(col) toupper(trimws(row[[col]])) == \"W\"), na.rm = TRUE)\n\n  # **First, Check for Winners** - If \"W\" appears in ELIMINATION-6 through ELIMINATION-10, classify as \"Won\"\n  if (w_found) {\n    return(\"Won\")\n  }\n\n  # **Next, Track First Elimination Event**\n  for (col in elim_columns) {\n    value &lt;- ifelse(is.na(row[[col]]), \"\", toupper(trimws(as.character(row[[col]]))))  # Handle NA values\n\n    if (grepl(\"^E\", value)) {  # Any elimination starting with \"E\"\n      if (value == \"E\" && col == \"ELIMINATION-1\") {\n        return(\"Eliminated in week 1 rose ceremony\")\n      } else if (value == \"E\") {\n        return(\"Eliminated in weeks 2+ rose ceremony\")\n      } else if (value == \"EQ\") {\n        return(\"Quit\")\n      } else if (value == \"EF\") {\n        return(\"Fired by production\")\n      } else if (value == \"ED\") {\n        return(\"Eliminated on a date\")\n      } else if (value == \"EU\") {\n        return(\"Eliminated at an unscheduled time\")\n      }\n    }\n  }\n\n  return(NA)  # Avoid misclassification\n}\n\n# Apply classification to dataset using `pick()`\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  rowwise() %&gt;%\n  mutate(Category = classify_contestant(pick(everything()))) %&gt;%\n  ungroup()\n\n# Remove NA rows (contestants who do not fit a category)\nbachelorette_data &lt;- bachelorette_data %&gt;% filter(!is.na(Category))\n\n# Count occurrences of each category\ncategory_counts &lt;- bachelorette_data %&gt;%\n  group_by(Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  arrange(desc(Count))\n\n# Print corrected category counts\nprint(category_counts)\n\n# A tibble: 7 × 2\n  Category                             Count\n  &lt;chr&gt;                                &lt;int&gt;\n1 Eliminated in weeks 2+ rose ceremony   425\n2 Eliminated in week 1 rose ceremony     298\n3 Eliminated on a date                    75\n4 Won                                     33\n5 Quit                                    28\n6 Eliminated at an unscheduled time       21\n7 Fired by production                      6\n\n# Display the updated category breakdown table using kable\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\"), caption = \"Final Category Breakdown\")\n\n\nFinal Category Breakdown\n\n\nCategory\nCount\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n\n\nEliminated in week 1 rose ceremony\n298\n\n\nEliminated on a date\n75\n\n\nWon\n33\n\n\nQuit\n28\n\n\nEliminated at an unscheduled time\n21\n\n\nFired by production\n6\n\n\n\n\n\nWe have picked up an additonal winner than was in the original dataset (33 instead of 32), we will use this code to verify who they were so we can be confident with our analysis:\n\n# Create a separate copy of the dataset (optional safety step)\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns: SHOW, SEASON, CONTESTANT, and elimination columns where \"W\" appeared\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Print the identified winners\nprint(winners)\n\n# A tibble: 33 × 13\n   SHOW        SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n 1 Bacheloret…     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n 2 Bacheloret…     12 12_JORDAN… R1              &lt;NA&gt;            &lt;NA&gt;           \n 3 Bacheloret…     11 11_SHAWN_B R1              &lt;NA&gt;            R              \n 4 Bacheloret…     10 10_JOSH_M  &lt;NA&gt;            &lt;NA&gt;            R              \n 5 Bacheloret…      9 09_CHRIS_S &lt;NA&gt;            &lt;NA&gt;            R              \n 6 Bacheloret…      8 08_JEF_H   &lt;NA&gt;            R               &lt;NA&gt;           \n 7 Bacheloret…      7 07_JP_R    &lt;NA&gt;            &lt;NA&gt;            R              \n 8 Bacheloret…      6 06_ROBERT… R1              &lt;NA&gt;            R              \n 9 Bacheloret…      5 05_ED_S    &lt;NA&gt;            &lt;NA&gt;            R              \n10 Bacheloret…      4 04_JESSE_C R1              &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 23 more rows\n# ℹ 7 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;\n\n\nWe have higher numbers in the other categories too, but we will assume that this is because the raw data has an additional season which was not included in the published graphs.\n\n# Count the total number of unique seasons for each show (Bachelor/Bachelorette)\nseason_counts &lt;- bachelorette_data %&gt;%\n  group_by(SHOW) %&gt;%\n  summarise(Total_Seasons = n_distinct(SEASON)) %&gt;%\n  arrange(desc(Total_Seasons))  # Sort by number of seasons\n\n# Print the season counts\nprint(season_counts)\n\n# A tibble: 2 × 2\n  SHOW         Total_Seasons\n  &lt;chr&gt;                &lt;int&gt;\n1 Bachelor                21\n2 Bachelorette            13\n\n# Display the table using kable\nknitr::kable(season_counts, col.names = c(\"Show\", \"Total Seasons\"),\n             caption = \"Total Number of Seasons by Show\")\n\n\nTotal Number of Seasons by Show\n\n\nShow\nTotal Seasons\n\n\n\n\nBachelor\n21\n\n\nBachelorette\n13\n\n\n\n\n\nTheir data indicated: Eliminated in weeks 2+ rose ceremony 411 Eliminated in week 1 rose ceremony 291 Won 32 Eliminated on a date 68 Eliminated at an unscheduled time 20 Quit 28 Fired by production 6\nOur results are: Eliminated in weeks 2+ rose ceremony 425\nEliminated in week 1 rose ceremony 298\nWon 33\nEliminated on a date 75\nEliminated at an unscheduled time 21\nQuit 28\nFired by production 6\nWe want to get the percentage for each of the categories so that we can include these in the graphic.\n\n# Calculate total number of contestants classified\ntotal_contestants &lt;- sum(category_counts$Count)\n\n# Add percentage column, rounded to the nearest whole number\n# Ensure category_counts has correctly formatted columns\ncategory_counts &lt;- category_counts %&gt;%\n  rename(Count = Count) %&gt;%  # Ensure correct naming\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Add percentage column\n\n\n#category_counts &lt;- category_counts %&gt;%\n # mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Calculate and format as \"X%\"\n\n\n# Print updated table\nprint(category_counts)\n\n# A tibble: 7 × 3\n  Category                             Count Percentage\n  &lt;chr&gt;                                &lt;int&gt; &lt;chr&gt;     \n1 Eliminated in weeks 2+ rose ceremony   425 48%       \n2 Eliminated in week 1 rose ceremony     298 34%       \n3 Eliminated on a date                    75 8%        \n4 Won                                     33 4%        \n5 Quit                                    28 3%        \n6 Eliminated at an unscheduled time       21 2%        \n7 Fired by production                      6 1%        \n\n# Display table using kable with percentages\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\", \"Percentage (%)\"),\n             caption = \"Final Category Breakdown with Rounded Percentages\")\n\n\nFinal Category Breakdown with Rounded Percentages\n\n\nCategory\nCount\nPercentage (%)\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n48%\n\n\nEliminated in week 1 rose ceremony\n298\n34%\n\n\nEliminated on a date\n75\n8%\n\n\nWon\n33\n4%\n\n\nQuit\n28\n3%\n\n\nEliminated at an unscheduled time\n21\n2%\n\n\nFired by production\n6\n1%\n\n\n\n\n\n\n# Extract rose-related events (e.g., R1, R, D1, etc.) from elimination columns\nrose_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check if a contestant received a rose in each episode\nget_rose_status &lt;- function(row) {\n  rose_events &lt;- row[rose_columns]\n  return(ifelse(any(grepl(\"R\", rose_events)), which(grepl(\"R\", rose_events))[1], NA))\n}\n\n# Apply the function to get the first episode where each contestant received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_status)\n\n\n# Identify eliminations\neliminate_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check elimination status (either eliminated or not)\nget_elimination_status &lt;- function(row) {\n  elimination_status &lt;- row[eliminate_columns]\n  return(sum(grepl(\"E\", elimination_status), na.rm = TRUE))\n}\n\n# Calculate the cumulative number of eliminations by episode\nbachelorette_data$Eliminations &lt;- apply(bachelorette_data, 1, get_elimination_status)\n\n# Calculate total contestants\ntotal_contestants &lt;- nrow(bachelorette_data)\n\n# Calculate cumulative eliminations\ncumulative_elim &lt;- cumsum(bachelorette_data$Eliminations)\n\n# Calculate cumulative percent chance of elimination\nbachelorette_data$Cumulative_Elim_Percent &lt;- cumulative_elim / total_contestants * 100\n\n\n\nHere are the two original images that i tried to recreate, with the original graph looking at the effect of receiving a rose early on being eliminated:\n\n\n\nGraph evaluating the effect of wiining roses early on and elimination.\n\n\nThe second graph (image) - see at the bottom of the document:\n\n\n\nThe breakdown of different contestants on the bachelor and bachelorette\n\n\nThis is how we (ChatGPT and me) started with the recreation of this graph:\nFor my prompt I uploaded the image of the graph and asked it to analyse it. I then uploaded the raw dataset and asked if it could recreate the graph. It seemed to trace a similar graph but I think it might have just copied the image a bit. So I asked it to provide the R code and to provide steps to recreate it. I then received the first graph provided below which was not similar at all (as you can see).\nI then explained what the variables on each axis were and I explained that there are two curves and what they are based upon. I then explained what we might be calculating and that it should include any data analysis steps that may be required. The results thereafter were much better and closer to the original and i weaked some of the smaller labels and things like that.\nIt was quite good at rendering a similar graph however our shape never quite reenacted the original data. Unfortunately, I am not sure how they performed this for clear comparison but the end result was somewhat similar. Given the issues that I picked uo with the data processing part I am not sure where the issue might be coming from.\n\nlibrary(ggplot2)\n\n# Filter the data for contestants who received a rose and those who did not\nrose_data &lt;- bachelorette_data[!is.na(bachelorette_data$Rose_Episode), ]\nno_rose_data &lt;- bachelorette_data[is.na(bachelorette_data$Rose_Episode), ]\n\n# Create the plot\nggplot() +\n  geom_line(data = rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Gets a date rose\"), size = 1) +\n  geom_line(data = no_rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Doesn't get a date rose\"), size = 1) +\n  labs(title = \"The Date Rose Advantage\\nCumulative Chance of Being Eliminated\",\n       x = \"Episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 674 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWhere we ended up after many back and forths and starting again to focus on the basic principles of the curve:\n\n# Identify the columns for eliminations\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to identify when a contestant received a rose\nget_rose_episode &lt;- function(row) {\n  for (i in seq_along(row[elim_columns])) {\n    if (grepl(\"R\", row[elim_columns[i]])) {\n      return(i)  # Episode number starts from 1\n    }\n  }\n  return(NA)  # If no rose received, return NA\n}\n\n# Apply the function to the dataset to create a new column with the episode they received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_episode)\n\n# Initialize empty lists to track eliminations for both groups (rose vs no-rose)\nelim_counts_rose &lt;- numeric(0)\nelim_counts_no_rose &lt;- numeric(0)\nweeks_after_rose_rose &lt;- numeric(0)\nweeks_after_rose_no_rose &lt;- numeric(0)\n\n# Variables to store total eliminations for both groups\nelim_rose &lt;- 0\nelim_no_rose &lt;- 0\n\n# Track eliminations and weeks after receiving the rose\nfor (i in 1:length(elim_columns)) {\n  \n  # For eliminations in the current episode\n  eliminated_in_episode &lt;- grepl(\"E\", bachelorette_data[[elim_columns[i]]])\n  \n  # For contestants who received a rose\n  eliminated_rose_group &lt;- eliminated_in_episode & !is.na(bachelorette_data$Rose_Episode)\n  elim_rose &lt;- elim_rose + sum(eliminated_rose_group)\n  elim_counts_rose &lt;- c(elim_counts_rose, elim_rose / sum(!is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from rose group\n  weeks_after_rose_rose &lt;- c(weeks_after_rose_rose, i)  # Track weeks after the rose\n  \n  # For contestants who did not receive a rose\n  eliminated_no_rose_group &lt;- eliminated_in_episode & is.na(bachelorette_data$Rose_Episode)\n  elim_no_rose &lt;- elim_no_rose + sum(eliminated_no_rose_group)\n  elim_counts_no_rose &lt;- c(elim_counts_no_rose, elim_no_rose / sum(is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from no-rose group\n  weeks_after_rose_no_rose &lt;- c(weeks_after_rose_no_rose, i)  # Track weeks after the rose\n\n}\n\n# Create a combined dataset for plotting\nelim_data &lt;- data.frame(\n  Weeks_After_Rose = c(weeks_after_rose_rose, weeks_after_rose_no_rose),\n  Cumulative_Elim_Percent = c(elim_counts_rose, elim_counts_no_rose),\n  Group = rep(c(\"Gets a date rose\", \"Doesn't get a date rose\"), each = length(elim_columns))\n)\n\n# Plot the graph\nplot &lt;- ggplot(elim_data, aes(x = Weeks_After_Rose, y = Cumulative_Elim_Percent, color = Group)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  labs(title = \"The Date Rose Advantage\",\n       x = \"Weeks after episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_x_continuous(breaks = seq(0, length(elim_columns), by = 1), labels = paste0(\"+\", seq(0, length(elim_columns), by = 1))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", \n        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n        axis.text.y = element_text(size = 12),\n        axis.title.x = element_text(size = 14),\n        axis.title.y = element_text(size = 14)) +\n  ylim(0, 100)  # Ensure y-axis is from 0 to 100%\n\n# Add the shorter measurement line between the two curves (shifted to the left)\nplot + \n  geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", linetype = \"dashed\", size = 1) +  # Line from x = 2 to x = 6\n  \n  # Add label for \"Rose advantage\"\n  annotate(\"text\", x = 4, y = 55, label = \"Rose advantage\", size = 6, angle = 0, hjust = 0.5)\n\nWarning in geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", : All aesthetics have length 1, but the data has 20 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nI do not know if the differences come from the original data or the approach in data anaylsis or the graph rendering.\n\n\n\n\nThere was no table to recreate so I decided to create one that looked at and ranked the number of roses that each winner received in their respective season.\nI tried to fix my table to have the first two rows with bigger font and to have different colours but I could not get it right. ChatGPT could not help me and kept on giving me the same responses and I gave up with my editing. I will pick it up again sometime.\n\n# Create a separate copy of the dataset\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column (winners)\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Identify the date columns (for checking the dates roses were given)\ndate_columns &lt;- grep(\"DATES-\", names(bachelorette_winner_check), value = TRUE)\n\n# Function to count the number of roses each winner received\ncount_roses &lt;- function(winner_row, date_columns, elim_columns) {\n  contestant &lt;- as.character(winner_row$CONTESTANT)\n  \n  date_roses &lt;- sum(sapply(date_columns, function(col) {\n    sum(grepl(contestant, as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  elim_roses &lt;- sum(sapply(elim_columns, function(col) {\n    sum(grepl(\"R\", as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  return(date_roses + elim_roses)  \n}\n\n# Create a data frame to store the winner and their respective rose counts\nrose_counts &lt;- data.frame(Winner = character(), Season = numeric(), Show_Type = character(), \n                          Roses_Received = numeric(), Roses = character(), stringsAsFactors = FALSE)\n\n# Loop over each winner and count the number of roses they received in their season\nfor (i in 1:nrow(winners)) {\n  winner_row &lt;- winners[i, ]\n  rose_count &lt;- count_roses(winner_row, date_columns, elim_columns)\n  winner_season &lt;- as.character(winner_row$SEASON)\n  winner_show &lt;- as.character(winner_row$SHOW)\n  \n  cleaned_winner_name &lt;- gsub(\"^\\\\d+_\", \"\", winner_row$CONTESTANT)\n  rose_symbol &lt;- paste(rep(\"🌹\", rose_count), collapse = \"\")  # Restored rose emoji 🌹\n  \n  rose_counts &lt;- rbind(rose_counts, data.frame(Winner = cleaned_winner_name, \n                                               Season = winner_season, \n                                               Show_Type = winner_show, \n                                               Roses_Received = rose_count, \n                                               Roses = rose_symbol, \n                                               stringsAsFactors = FALSE))\n}\n\n# Sort the table from most roses received to least\nrose_counts &lt;- rose_counts %&gt;% arrange(desc(Roses_Received))\n\n# Explanation text row\nexplanation_text &lt;- \"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette.\"\n\n# Display the table with improved formatting\nrose_counts %&gt;%\n  kable(col.names = c(\"Winner\", \"Season\", \"Show Type\", \"Roses Received\", \"Roses\"),\n        align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), escape = FALSE) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", \n                bootstrap_options = c(\"striped\", \"bordered\", \"hover\")) %&gt;%\n  \n  \n  \n  # Explanation row (light red background, no gridlines)\n  add_header_above(c(\"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\" = 5)) %&gt;%\n  row_spec(1, background = \"#FFCCCC\", italic = TRUE, hline_after = FALSE, extra_css = \"border-bottom: none;\") %&gt;%\n  \n  # Main title row: \"Roses to the Winners\" (large font)\n  add_header_above(c(\"Roses to the Winners\" = 5)) %&gt;%\n  row_spec(0, bold = TRUE, font_size = 28) %&gt;%  # Increased font size for title\n  \n  \n  # Subheading row (Winner, Season, etc.) - Pink background\n  row_spec(2, background = \"#FFD9D9\", bold = TRUE) %&gt;%\n  \n  # Adjust column spacing for better readability\n  column_spec(2, width = \"8em\") %&gt;%\n  column_spec(3, width = \"12em\") %&gt;%\n  column_spec(4, width = \"10em\") %&gt;%\n  column_spec(5, width = \"15em\") %&gt;%\n  \n  # Footer with source information\n  footnote(general = \"Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\")\n\n\n\n\n\n\n\n\n\n\n\n\nRoses to the Winners\n\n\n\n\nA breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\n\n\n\nWinner\nSeason\nShow Type\nRoses Received\nRoses\n\n\n\n\nBRYAN_A\n13\nBachelorette\n4\n🌹🌹🌹🌹 |\n\n\nJORDAN_R\n12\nBachelorette\n3\n🌹🌹🌹 |\n\n\nSHAWN_B\n11\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJEF_H\n8\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJP_R\n7\nBachelorette\n3\n🌹🌹🌹 |\n\n\nNIKKI_F\n18\nBachelor\n3\n🌹🌹🌹 |\n\n\nEMILY_M\n15\nBachelor\n3\n🌹🌹🌹 |\n\n\nJOSH_M\n10\nBachelorette\n2\n🌹🌹 |\n\n\nCHRIS_S\n9\nBachelorette\n2\n🌹🌹 |\n\n\nROBERT_M\n6\nBachelorette\n2\n🌹🌹 |\n\n\nJESSE_C\n4\nBachelorette\n2\n🌹🌹 |\n\n\nVANESSA_G\n21\nBachelor\n2\n🌹🌹 |\n\n\nLAUREN_B\n20\nBachelor\n2\n🌹🌹 |\n\n\nWHITNEY_B\n19\nBachelor\n2\n🌹🌹 |\n\n\nCATHERINE_G\n17\nBachelor\n2\n🌹🌹 |\n\n\nCOURTNEY_R\n16\nBachelor\n2\n🌹🌹 |\n\n\nJENNIFER_W\n9\nBachelor\n2\n🌹🌹 |\n\n\nED_S\n5\nBachelorette\n1\n🌹 |\n\n\nVIENNA_G\n14\nBachelor\n1\n🌹 |\n\n\nMELISSA_R\n13\nBachelor\n1\n🌹 |\n\n\nSHAYNE_L\n12\nBachelor\n1\n🌹 |\n\n\nTESSA_H\n10\nBachelor\n1\n🌹 |\n\n\nSarah_S\n8\nBachelor\n1\n🌹 |\n\n\nSarah B.\n7\nBachelor\n1\n🌹 |\n\n\nJERRY_F\n3\nBachelorette\n0\n\n\n\nIAN_M\n2\nBachelorette\n0\n\n\n\nRYAN_S\n1\nBachelorette\n0\n\n\n\nMary\n6\nBachelor\n0\n\n\n\nJESSICA_B\n5\nBachelor\n0\n\n\n\nEstella\n4\nBachelor\n0\n\n\n\nJEN_X\n3\nBachelor\n0\n\n\n\nHELENE_E\n2\nBachelor\n0\n\n\n\nAMANDA_M\n1\nBachelor\n0\n\n\n\n\nNote: \n\n\n\n\n\n\n Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI spent an embarrasingly long time trying to recreate this image that was in the article. I spent two days going back and forth with ChatGPT and trying to get it to format the graphic to recreate the original. I even tried with DeepSeek.\nI think I used over 3000 lines of R script with my back and forths and so I am not sharing my directions to R. But I even uploaded the image multiple times and asked it to expalin the image and how everything fitted together and I corrected each element. However, no matter how much promise it showed, even with starting multiple new chats, I was never able to fully correct the image to the correct order (challenge for someone else?). Every time I tried to fix one thing, it would break everything else.\nAfter spending all of the time (I am not exagerrating when I say two days) on this I eventually realised that the authors probably used an imaging software for this since it is more of an image than a graph……..🤕😭\nNonetheless, here are two different attempts. Please feel free to try your hand.\nThis was my best attempt:\n\n# Define dataset dynamically\ncategory_counts &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Eliminated on a date\",\n    \"Won\",\n    \"Quit\",\n    \"Eliminated at an unscheduled time\",\n    \"Fired by production\"\n  ),\n  Count = c(425, 298, 75, 33, 28, 21, 6)  # Updated counts from the provided data\n)\n\n# Calculate percentages dynamically\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Compute the circle packing layout\npacking &lt;- circleProgressiveLayout(category_counts$Count, sizetype = \"area\")\ncategory_counts$x &lt;- packing$x\ncategory_counts$y &lt;- packing$y\ncategory_counts$r &lt;- packing$radius\n\n# Generate circle layout vertices for plotting\ncircle_data &lt;- circleLayoutVertices(packing, npoints = 100)\n\n# Define colors: Pink for all except \"Won\", which is red\ncolors &lt;- c(\"#FADBD8\", \"#FADBD8\", \"#FADBD8\", \"#E74C3C\", \"#FADBD8\", \"#FADBD8\", \"#FADBD8\")\n\n# Create the bubble chart\nggplot() +\n  # Draw proportional circles with specified colors\n  geom_polygon(data = circle_data, aes(x, y, group = id, fill = as.factor(id)), \n               color = \"black\", size = 0.4, alpha = 0.8) +\n  \n  # Add contestant count inside bubbles\n  geom_text(data = category_counts, aes(x = x, y = y, label = Count), \n            size = 5, fontface = \"bold\", color = \"black\") +\n  \n  # Add category names with percentages using text repulsion\n  geom_text_repel(data = category_counts, \n                  aes(x = x, y = y, label = paste0(Category, \"\\n\", Percentage)), \n                  size = 4, color = \"black\", \n                  nudge_y = 0.5, nudge_x = 0.5, \n                  box.padding = 0.4, segment.color = \"black\") +\n  \n  # Clean up the theme\n  theme_void() +\n  theme(legend.position = \"none\") +\n  \n  # Apply the pink color for all except \"Won\" (which is red)\n  scale_fill_manual(values = colors)\n\n\n\n\n\n\n\n\nFor comedy relief I will share other attempts:\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Scale bubble size\nbubble_data$BubbleSize &lt;- sqrt(bubble_data$Count) * 12  \n\n# Generate circle layout\nset.seed(42)  \npacking &lt;- circleProgressiveLayout(bubble_data$BubbleSize, sizetype = \"radius\")\nbubble_data$x &lt;- packing$x\nbubble_data$y &lt;- packing$y\nbubble_data$radius &lt;- packing$radius\n\n# Generate circle layout vertices\ncircle_vertices &lt;- circleLayoutVertices(packing, npoints = 50)\n\n# Define colors for the bubbles\ncategory_colors &lt;- c(\n  \"Eliminated in weeks 2+ rose ceremony\" = \"#fbb4ae\",\n  \"Eliminated in week 1 rose ceremony\" = \"#fbb4ae\",\n  \"Fired by production\" = \"#fbb4ae\",\n  \"Quit\" = \"#fbb4ae\",\n  \"Eliminated on a date\" = \"#fbb4ae\",\n  \"Eliminated at an unscheduled time\" = \"#fbb4ae\",\n  \"Won\" = \"#e31a1c\"\n)\n\n# ===== MANUALLY ADJUST BUBBLE POSITIONS =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    x = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -1.6,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 1.2,  \n      Category == \"Eliminated on a date\" ~ 0.2,  \n      Category == \"Quit\" ~ 1.8,  \n      Category == \"Fired by production\" ~ 2.0,  \n      Category == \"Won\" ~ -0.8,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.5,  \n      TRUE ~ 0\n    ),\n    y = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.8,  \n      Category == \"Quit\" ~ -0.2,  \n      Category == \"Fired by production\" ~ 1.3,  \n      Category == \"Won\" ~ -0.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -0.9,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== ENSURING TEXT MOVES WITH BUBBLES =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    # Count label inside bubbles\n    Nudge_X = x,\n    Nudge_Y = y,\n\n    # Category text label positioning outside bubbles\n    Nudge_X_Category = case_when(\n      Category == \"Won\" ~ -1.2,  \n      Category == \"Quit\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.5,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -2,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.6,  \n      Category == \"Fired by production\" ~ 2.1,  \n      TRUE ~ 0\n    ),\n    Nudge_Y_Category = case_when(\n      Category == \"Won\" ~ -0.8,  \n      Category == \"Quit\" ~ -0.5,  \n      Category == \"Eliminated on a date\" ~ 0.6,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -1.2,  \n      Category == \"Fired by production\" ~ 1.5,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== CREATE THE BUBBLE PLOT =====\nggplot() +\n  # Draw bubbles\n  geom_polygon(data = circle_vertices, aes(x + bubble_data$x[id], y + bubble_data$y[id], \n                                           group = id, fill = bubble_data$Category[id]), \n               color = \"black\", alpha = 0.8) +  \n\n  # **Move COUNT labels inside bubbles**\n  geom_label(data = bubble_data %&gt;% filter(radius &gt; 30), \n             aes(x = x, y = y, label = Count, size = radius / 10), \n             fontface = \"bold\", fill = \"white\", alpha = 0.8,  \n             label.size = 0, hjust = 0.5, vjust = 0.5) +\n\n  # **Move COUNT labels outside small bubbles**\n  geom_label_repel(data = bubble_data %&gt;% filter(radius &lt;= 30), \n                   aes(x = x, y = y + radius * 3, label = Count, size = 5), \n                   fontface = \"bold\", fill = \"white\", color = \"black\",\n                   box.padding = 2, segment.color = \"black\", \n                   nudge_y = 20, direction = \"both\", force = 150) +\n\n  # **Move CATEGORY labels outside bubbles & allow multiple line text**\n  geom_label_repel(data = bubble_data, \n                   aes(x = x + Nudge_X_Category, y = y + Nudge_Y_Category, label = Category),  \n                   size = 4, color = \"black\", fontface = \"italic\",\n                   box.padding = 2, segment.color = \"black\", segment.size = 0.8,\n                   direction = \"both\", force = 400, label.size = 0.3, \n                   fill = \"white\", alpha = 0.9) +  \n\n  # **Move percentage labels inside bubbles**\n  geom_text(data = bubble_data, \n            aes(x = x, y = y + radius * 0.3, label = Percentage), \n            size = 3, color = \"black\", hjust = 0.5, vjust = 0.5) +\n\n  # Assign colors to the bubbles\n  scale_fill_manual(values = category_colors) +\n\n  # Remove default ggplot background\n  theme_void() +\n\n  # Adjust text styling\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n\n  # Set the title\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\nWarning: ggrepel: 3 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Create bubble chart without manual position adjustments\nggplot(bubble_data, aes(x = Count, y = Count, size = Count, fill = Category)) +\n  geom_point(alpha = 0.8, shape = 21, color = \"black\") +  # Use points instead of polygons\n  scale_size(range = c(5, 25)) +  # Adjust bubble scaling dynamically\n  scale_fill_viridis_d() +  # Dynamic color scaling for categories\n  geom_text_repel(aes(label = Count), size = 5, fontface = \"bold\", color = \"black\") +  # Automatic text positioning\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\n\n\n\n\n\n\n\n~ Fin"
  },
  {
    "objectID": "presentation-exercise/presentation_exercise.html#processing-of-the-data",
    "href": "presentation-exercise/presentation_exercise.html#processing-of-the-data",
    "title": "presentation-exercise",
    "section": "",
    "text": "First I will load all of the required packages and well as read the raw data CSV file:\n\n#install.packages(\"ggforce\")\n#install.packages(\"packcircles\")\n#install.packages(\"ggrepel\")\n#install.packages(\"viridis\")\n#install.packages(\"kableExtra\")\n\n\nlibrary(\"ggplot2\")\nlibrary(\"tidyverse\")\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"knitr\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggforce\")\n\nWarning: package 'ggforce' was built under R version 4.4.2\n\nlibrary(\"packcircles\")\n\nWarning: package 'packcircles' was built under R version 4.4.2\n\nlibrary(\"ggrepel\")\n\nWarning: package 'ggrepel' was built under R version 4.4.2\n\nlibrary(\"viridis\")\n\nWarning: package 'viridis' was built under R version 4.4.2\n\n\nLoading required package: viridisLite\n\nlibrary(\"kableExtra\")\n\nWarning: package 'kableExtra' was built under R version 4.4.2\n\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nbachelorette_data &lt;- read_csv(here::here(\n  \"presentation-exercise/bachelorette.csv\"))\n\nRows: 920 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): SHOW, SEASON, CONTESTANT, ELIMINATION-1, ELIMINATION-2, ELIMINATIO...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(bachelorette_data)\n\n     SHOW              SEASON           CONTESTANT        ELIMINATION-1     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-2      ELIMINATION-3      ELIMINATION-4      ELIMINATION-5     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-6      ELIMINATION-7      ELIMINATION-8      ELIMINATION-9     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-10       DATES-1            DATES-2            DATES-3         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-4            DATES-5            DATES-6            DATES-7         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-8            DATES-9            DATES-10        \n Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n\n\nPerform some formating of the data to remove the first row which is a repetition of headings and to change the seasons to numeric values. We are also removing rows where headings are repeated within the data:\n\n# Remove the incorrect header row if necessary\nbachelorette_data &lt;- bachelorette_data[-1, ]  # Remove first row if it's incorrect\n\n# Convert SEASON to numeric\nbachelorette_data$SEASON &lt;- as.numeric(bachelorette_data$SEASON)\n\nWarning: NAs introduced by coercion\n\n# Remove rows where the CONTESTANT column contains \"ID\"\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  filter(CONTESTANT != \"ID\")\n\n# Reset row indices (optional)\nbachelorette_data &lt;- bachelorette_data %&gt;% mutate(row_id = row_number())\n\n# Display the cleaned data\nprint(head(bachelorette_data))\n\n# A tibble: 6 × 24\n  SHOW         SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n1 Bachelorette     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n2 Bachelorette     13 13_PETER_K &lt;NA&gt;            R               &lt;NA&gt;           \n3 Bachelorette     13 13_ERIC_B  &lt;NA&gt;            &lt;NA&gt;            R              \n4 Bachelorette     13 13_DEAN_U  &lt;NA&gt;            R               &lt;NA&gt;           \n5 Bachelorette     13 13_ADAM_G  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n6 Bachelorette     13 13_MATT_M  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 18 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;, `DATES-1` &lt;chr&gt;,\n#   `DATES-2` &lt;chr&gt;, `DATES-3` &lt;chr&gt;, `DATES-4` &lt;chr&gt;, `DATES-5` &lt;chr&gt;,\n#   `DATES-6` &lt;chr&gt;, `DATES-7` &lt;chr&gt;, `DATES-8` &lt;chr&gt;, `DATES-9` &lt;chr&gt;,\n#   `DATES-10` &lt;chr&gt;, row_id &lt;int&gt;\n\n\nSome information was provided on the website with the raw data which may help with interpretation of this data:\n\n#Adding additional data into a table\ncolumn_names &lt;- c(\"Header\", \"Description\")\ntable_data &lt;- data.frame(\n  Header = c(\n    \"SHOW\", \"SEASON\", \"CONTESTANT\", \"ELIMINATION-1\", \"ELIMINATION-2\",\n    \"ELIMINATION-3\", \"ELIMINATION-4\", \"ELIMINATION-5\", \"ELIMINATION-6\",\n    \"ELIMINATION-7\", \"ELIMINATION-8\", \"ELIMINATION-9\", \"ELIMINATION-10\",\n    \"DATES-1\", \"DATES-2\", \"DATES-3\", \"DATES-4\", \"DATES-5\", \"DATES-6\",\n    \"DATES-7\", \"DATES-8\", \"DATES-9\", \"DATES-10\"\n  ),\n  Description = c(\n    \"Bachelor or Bachelorette\", \"Which season\", \"An identifier for the contestant in a given season\",\n    \"Who was eliminated in week 1\", \"Who was eliminated in week 2\", \"Who was eliminated in week 3\",\n    \"Who was eliminated in week 4\", \"Who was eliminated in week 5\", \"Who was eliminated in week 6\",\n    \"Who was eliminated in week 7\", \"Who was eliminated in week 8\", \"Who was eliminated in week 9\",\n    \"Who was eliminated in week 10\", \"Who was on which date in week 1\", \"Who was on which date in week 2\",\n    \"Who was on which date in week 3\", \"Who was on which date in week 4\", \"Who was on which date in week 5\",\n    \"Who was on which date in week 6\", \"Who was on which date in week 7\", \"Who was on which date in week 8\",\n    \"Who was on which date in week 9\", \"Who was on which date in week 10\"\n  )\n)\n\n# Print table using kable for better formatting\nknitr::kable(table_data, col.names = column_names, caption = \"Table of Data Headers and Descriptions\")\n\n\nTable of Data Headers and Descriptions\n\n\nHeader\nDescription\n\n\n\n\nSHOW\nBachelor or Bachelorette\n\n\nSEASON\nWhich season\n\n\nCONTESTANT\nAn identifier for the contestant in a given season\n\n\nELIMINATION-1\nWho was eliminated in week 1\n\n\nELIMINATION-2\nWho was eliminated in week 2\n\n\nELIMINATION-3\nWho was eliminated in week 3\n\n\nELIMINATION-4\nWho was eliminated in week 4\n\n\nELIMINATION-5\nWho was eliminated in week 5\n\n\nELIMINATION-6\nWho was eliminated in week 6\n\n\nELIMINATION-7\nWho was eliminated in week 7\n\n\nELIMINATION-8\nWho was eliminated in week 8\n\n\nELIMINATION-9\nWho was eliminated in week 9\n\n\nELIMINATION-10\nWho was eliminated in week 10\n\n\nDATES-1\nWho was on which date in week 1\n\n\nDATES-2\nWho was on which date in week 2\n\n\nDATES-3\nWho was on which date in week 3\n\n\nDATES-4\nWho was on which date in week 4\n\n\nDATES-5\nWho was on which date in week 5\n\n\nDATES-6\nWho was on which date in week 6\n\n\nDATES-7\nWho was on which date in week 7\n\n\nDATES-8\nWho was on which date in week 8\n\n\nDATES-9\nWho was on which date in week 9\n\n\nDATES-10\nWho was on which date in week 10\n\n\n\n\n\nFurther information required for interpretation and processing:\nEliminates connote either an elimination (starts with “E”) or a rose (starts with “R”). Eliminations supercede roses. “E” connotes a standard elimination, typically at a rose ceremony. “EQ” means the contestant quits. “EF” means the contestant was fired by production. “ED” connotes a date elimination. “EU” connotes an unscheduled elimination, one that takes place at a time outside of a date or rose ceremony. “R” means the contestant received a rose. “R1” means the contestant got a first impression rose. “D1” means a one-on-one date, “D2” means a 2-on-1, “D3” means a 3-on-1 group date, and so on. “W” in E10 indicates that the contestant won. Weeks of the show are deliminated by rose ceremonies, and may not line up exactly with episodes.\n………………….\nI will be using ChatGPT to help me to generate the code that will help to obtain the graph that I want. I will post my prompts and the outcome as well as the different iterations below.\nWe are not getting the correct number of winners\n\n# Get all ELIMINATION columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Count occurrences of \"W\" in each ELIMINATION column\nw_counts &lt;- sapply(elim_columns, function(col) sum(bachelorette_data[[col]] == \"W\", na.rm = TRUE))\n\n# Print counts for each column\nprint(\"Number of 'W' occurrences per ELIMINATION column:\")\n\n[1] \"Number of 'W' occurrences per ELIMINATION column:\"\n\nprint(w_counts)\n\n ELIMINATION-1  ELIMINATION-2  ELIMINATION-3  ELIMINATION-4  ELIMINATION-5 \n             0              0              0              0              0 \n ELIMINATION-6  ELIMINATION-7  ELIMINATION-8  ELIMINATION-9 ELIMINATION-10 \n             2              8              7              1             15 \n\n# Total winners found in dataset\ntotal_winners &lt;- sum(w_counts)\nprint(paste(\"Total expected winners:\", total_winners))\n\n[1] \"Total expected winners: 33\"\n\n\nNow we found that it seems that winners were marked in different levels of the competition (Elimination 6 - 10 and not just the last two). We will run this again:\n\n# Define function to classify contestants, now checking ELIMINATION-6 to ELIMINATION-10\nclassify_contestant &lt;- function(row) {\n  elim_columns &lt;- grep(\"ELIMINATION-\", names(row), value = TRUE)\n\n  # **Ensure all ELIMINATION columns are checked for \"W\"**\n  w_found &lt;- any(sapply(elim_columns, function(col) toupper(trimws(row[[col]])) == \"W\"), na.rm = TRUE)\n\n  # **First, Check for Winners** - If \"W\" appears in ELIMINATION-6 through ELIMINATION-10, classify as \"Won\"\n  if (w_found) {\n    return(\"Won\")\n  }\n\n  # **Next, Track First Elimination Event**\n  for (col in elim_columns) {\n    value &lt;- ifelse(is.na(row[[col]]), \"\", toupper(trimws(as.character(row[[col]]))))  # Handle NA values\n\n    if (grepl(\"^E\", value)) {  # Any elimination starting with \"E\"\n      if (value == \"E\" && col == \"ELIMINATION-1\") {\n        return(\"Eliminated in week 1 rose ceremony\")\n      } else if (value == \"E\") {\n        return(\"Eliminated in weeks 2+ rose ceremony\")\n      } else if (value == \"EQ\") {\n        return(\"Quit\")\n      } else if (value == \"EF\") {\n        return(\"Fired by production\")\n      } else if (value == \"ED\") {\n        return(\"Eliminated on a date\")\n      } else if (value == \"EU\") {\n        return(\"Eliminated at an unscheduled time\")\n      }\n    }\n  }\n\n  return(NA)  # Avoid misclassification\n}\n\n# Apply classification to dataset using `pick()`\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  rowwise() %&gt;%\n  mutate(Category = classify_contestant(pick(everything()))) %&gt;%\n  ungroup()\n\n# Remove NA rows (contestants who do not fit a category)\nbachelorette_data &lt;- bachelorette_data %&gt;% filter(!is.na(Category))\n\n# Count occurrences of each category\ncategory_counts &lt;- bachelorette_data %&gt;%\n  group_by(Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  arrange(desc(Count))\n\n# Print corrected category counts\nprint(category_counts)\n\n# A tibble: 7 × 2\n  Category                             Count\n  &lt;chr&gt;                                &lt;int&gt;\n1 Eliminated in weeks 2+ rose ceremony   425\n2 Eliminated in week 1 rose ceremony     298\n3 Eliminated on a date                    75\n4 Won                                     33\n5 Quit                                    28\n6 Eliminated at an unscheduled time       21\n7 Fired by production                      6\n\n# Display the updated category breakdown table using kable\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\"), caption = \"Final Category Breakdown\")\n\n\nFinal Category Breakdown\n\n\nCategory\nCount\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n\n\nEliminated in week 1 rose ceremony\n298\n\n\nEliminated on a date\n75\n\n\nWon\n33\n\n\nQuit\n28\n\n\nEliminated at an unscheduled time\n21\n\n\nFired by production\n6\n\n\n\n\n\nWe have picked up an additonal winner than was in the original dataset (33 instead of 32), we will use this code to verify who they were so we can be confident with our analysis:\n\n# Create a separate copy of the dataset (optional safety step)\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns: SHOW, SEASON, CONTESTANT, and elimination columns where \"W\" appeared\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Print the identified winners\nprint(winners)\n\n# A tibble: 33 × 13\n   SHOW        SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n 1 Bacheloret…     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n 2 Bacheloret…     12 12_JORDAN… R1              &lt;NA&gt;            &lt;NA&gt;           \n 3 Bacheloret…     11 11_SHAWN_B R1              &lt;NA&gt;            R              \n 4 Bacheloret…     10 10_JOSH_M  &lt;NA&gt;            &lt;NA&gt;            R              \n 5 Bacheloret…      9 09_CHRIS_S &lt;NA&gt;            &lt;NA&gt;            R              \n 6 Bacheloret…      8 08_JEF_H   &lt;NA&gt;            R               &lt;NA&gt;           \n 7 Bacheloret…      7 07_JP_R    &lt;NA&gt;            &lt;NA&gt;            R              \n 8 Bacheloret…      6 06_ROBERT… R1              &lt;NA&gt;            R              \n 9 Bacheloret…      5 05_ED_S    &lt;NA&gt;            &lt;NA&gt;            R              \n10 Bacheloret…      4 04_JESSE_C R1              &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 23 more rows\n# ℹ 7 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;\n\n\nWe have higher numbers in the other categories too, but we will assume that this is because the raw data has an additional season which was not included in the published graphs.\n\n# Count the total number of unique seasons for each show (Bachelor/Bachelorette)\nseason_counts &lt;- bachelorette_data %&gt;%\n  group_by(SHOW) %&gt;%\n  summarise(Total_Seasons = n_distinct(SEASON)) %&gt;%\n  arrange(desc(Total_Seasons))  # Sort by number of seasons\n\n# Print the season counts\nprint(season_counts)\n\n# A tibble: 2 × 2\n  SHOW         Total_Seasons\n  &lt;chr&gt;                &lt;int&gt;\n1 Bachelor                21\n2 Bachelorette            13\n\n# Display the table using kable\nknitr::kable(season_counts, col.names = c(\"Show\", \"Total Seasons\"),\n             caption = \"Total Number of Seasons by Show\")\n\n\nTotal Number of Seasons by Show\n\n\nShow\nTotal Seasons\n\n\n\n\nBachelor\n21\n\n\nBachelorette\n13\n\n\n\n\n\nTheir data indicated: Eliminated in weeks 2+ rose ceremony 411 Eliminated in week 1 rose ceremony 291 Won 32 Eliminated on a date 68 Eliminated at an unscheduled time 20 Quit 28 Fired by production 6\nOur results are: Eliminated in weeks 2+ rose ceremony 425\nEliminated in week 1 rose ceremony 298\nWon 33\nEliminated on a date 75\nEliminated at an unscheduled time 21\nQuit 28\nFired by production 6\nWe want to get the percentage for each of the categories so that we can include these in the graphic.\n\n# Calculate total number of contestants classified\ntotal_contestants &lt;- sum(category_counts$Count)\n\n# Add percentage column, rounded to the nearest whole number\n# Ensure category_counts has correctly formatted columns\ncategory_counts &lt;- category_counts %&gt;%\n  rename(Count = Count) %&gt;%  # Ensure correct naming\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Add percentage column\n\n\n#category_counts &lt;- category_counts %&gt;%\n # mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Calculate and format as \"X%\"\n\n\n# Print updated table\nprint(category_counts)\n\n# A tibble: 7 × 3\n  Category                             Count Percentage\n  &lt;chr&gt;                                &lt;int&gt; &lt;chr&gt;     \n1 Eliminated in weeks 2+ rose ceremony   425 48%       \n2 Eliminated in week 1 rose ceremony     298 34%       \n3 Eliminated on a date                    75 8%        \n4 Won                                     33 4%        \n5 Quit                                    28 3%        \n6 Eliminated at an unscheduled time       21 2%        \n7 Fired by production                      6 1%        \n\n# Display table using kable with percentages\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\", \"Percentage (%)\"),\n             caption = \"Final Category Breakdown with Rounded Percentages\")\n\n\nFinal Category Breakdown with Rounded Percentages\n\n\nCategory\nCount\nPercentage (%)\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n48%\n\n\nEliminated in week 1 rose ceremony\n298\n34%\n\n\nEliminated on a date\n75\n8%\n\n\nWon\n33\n4%\n\n\nQuit\n28\n3%\n\n\nEliminated at an unscheduled time\n21\n2%\n\n\nFired by production\n6\n1%\n\n\n\n\n\n\n# Extract rose-related events (e.g., R1, R, D1, etc.) from elimination columns\nrose_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check if a contestant received a rose in each episode\nget_rose_status &lt;- function(row) {\n  rose_events &lt;- row[rose_columns]\n  return(ifelse(any(grepl(\"R\", rose_events)), which(grepl(\"R\", rose_events))[1], NA))\n}\n\n# Apply the function to get the first episode where each contestant received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_status)\n\n\n# Identify eliminations\neliminate_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check elimination status (either eliminated or not)\nget_elimination_status &lt;- function(row) {\n  elimination_status &lt;- row[eliminate_columns]\n  return(sum(grepl(\"E\", elimination_status), na.rm = TRUE))\n}\n\n# Calculate the cumulative number of eliminations by episode\nbachelorette_data$Eliminations &lt;- apply(bachelorette_data, 1, get_elimination_status)\n\n# Calculate total contestants\ntotal_contestants &lt;- nrow(bachelorette_data)\n\n# Calculate cumulative eliminations\ncumulative_elim &lt;- cumsum(bachelorette_data$Eliminations)\n\n# Calculate cumulative percent chance of elimination\nbachelorette_data$Cumulative_Elim_Percent &lt;- cumulative_elim / total_contestants * 100\n\n\n\nHere are the two original images that i tried to recreate, with the original graph looking at the effect of receiving a rose early on being eliminated:\n\n\n\nGraph evaluating the effect of wiining roses early on and elimination.\n\n\nThe second graph (image) - see at the bottom of the document:\n\n\n\nThe breakdown of different contestants on the bachelor and bachelorette\n\n\nThis is how we (ChatGPT and me) started with the recreation of this graph:\nFor my prompt I uploaded the image of the graph and asked it to analyse it. I then uploaded the raw dataset and asked if it could recreate the graph. It seemed to trace a similar graph but I think it might have just copied the image a bit. So I asked it to provide the R code and to provide steps to recreate it. I then received the first graph provided below which was not similar at all (as you can see).\nI then explained what the variables on each axis were and I explained that there are two curves and what they are based upon. I then explained what we might be calculating and that it should include any data analysis steps that may be required. The results thereafter were much better and closer to the original and i weaked some of the smaller labels and things like that.\nIt was quite good at rendering a similar graph however our shape never quite reenacted the original data. Unfortunately, I am not sure how they performed this for clear comparison but the end result was somewhat similar. Given the issues that I picked uo with the data processing part I am not sure where the issue might be coming from.\n\nlibrary(ggplot2)\n\n# Filter the data for contestants who received a rose and those who did not\nrose_data &lt;- bachelorette_data[!is.na(bachelorette_data$Rose_Episode), ]\nno_rose_data &lt;- bachelorette_data[is.na(bachelorette_data$Rose_Episode), ]\n\n# Create the plot\nggplot() +\n  geom_line(data = rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Gets a date rose\"), size = 1) +\n  geom_line(data = no_rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Doesn't get a date rose\"), size = 1) +\n  labs(title = \"The Date Rose Advantage\\nCumulative Chance of Being Eliminated\",\n       x = \"Episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 674 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWhere we ended up after many back and forths and starting again to focus on the basic principles of the curve:\n\n# Identify the columns for eliminations\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to identify when a contestant received a rose\nget_rose_episode &lt;- function(row) {\n  for (i in seq_along(row[elim_columns])) {\n    if (grepl(\"R\", row[elim_columns[i]])) {\n      return(i)  # Episode number starts from 1\n    }\n  }\n  return(NA)  # If no rose received, return NA\n}\n\n# Apply the function to the dataset to create a new column with the episode they received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_episode)\n\n# Initialize empty lists to track eliminations for both groups (rose vs no-rose)\nelim_counts_rose &lt;- numeric(0)\nelim_counts_no_rose &lt;- numeric(0)\nweeks_after_rose_rose &lt;- numeric(0)\nweeks_after_rose_no_rose &lt;- numeric(0)\n\n# Variables to store total eliminations for both groups\nelim_rose &lt;- 0\nelim_no_rose &lt;- 0\n\n# Track eliminations and weeks after receiving the rose\nfor (i in 1:length(elim_columns)) {\n  \n  # For eliminations in the current episode\n  eliminated_in_episode &lt;- grepl(\"E\", bachelorette_data[[elim_columns[i]]])\n  \n  # For contestants who received a rose\n  eliminated_rose_group &lt;- eliminated_in_episode & !is.na(bachelorette_data$Rose_Episode)\n  elim_rose &lt;- elim_rose + sum(eliminated_rose_group)\n  elim_counts_rose &lt;- c(elim_counts_rose, elim_rose / sum(!is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from rose group\n  weeks_after_rose_rose &lt;- c(weeks_after_rose_rose, i)  # Track weeks after the rose\n  \n  # For contestants who did not receive a rose\n  eliminated_no_rose_group &lt;- eliminated_in_episode & is.na(bachelorette_data$Rose_Episode)\n  elim_no_rose &lt;- elim_no_rose + sum(eliminated_no_rose_group)\n  elim_counts_no_rose &lt;- c(elim_counts_no_rose, elim_no_rose / sum(is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from no-rose group\n  weeks_after_rose_no_rose &lt;- c(weeks_after_rose_no_rose, i)  # Track weeks after the rose\n\n}\n\n# Create a combined dataset for plotting\nelim_data &lt;- data.frame(\n  Weeks_After_Rose = c(weeks_after_rose_rose, weeks_after_rose_no_rose),\n  Cumulative_Elim_Percent = c(elim_counts_rose, elim_counts_no_rose),\n  Group = rep(c(\"Gets a date rose\", \"Doesn't get a date rose\"), each = length(elim_columns))\n)\n\n# Plot the graph\nplot &lt;- ggplot(elim_data, aes(x = Weeks_After_Rose, y = Cumulative_Elim_Percent, color = Group)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  labs(title = \"The Date Rose Advantage\",\n       x = \"Weeks after episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_x_continuous(breaks = seq(0, length(elim_columns), by = 1), labels = paste0(\"+\", seq(0, length(elim_columns), by = 1))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", \n        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n        axis.text.y = element_text(size = 12),\n        axis.title.x = element_text(size = 14),\n        axis.title.y = element_text(size = 14)) +\n  ylim(0, 100)  # Ensure y-axis is from 0 to 100%\n\n# Add the shorter measurement line between the two curves (shifted to the left)\nplot + \n  geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", linetype = \"dashed\", size = 1) +  # Line from x = 2 to x = 6\n  \n  # Add label for \"Rose advantage\"\n  annotate(\"text\", x = 4, y = 55, label = \"Rose advantage\", size = 6, angle = 0, hjust = 0.5)\n\nWarning in geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", : All aesthetics have length 1, but the data has 20 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nI do not know if the differences come from the original data or the approach in data anaylsis or the graph rendering."
  },
  {
    "objectID": "presentation-exercise/presentation_exercise.html#part-2-creating-a-table",
    "href": "presentation-exercise/presentation_exercise.html#part-2-creating-a-table",
    "title": "presentation-exercise",
    "section": "",
    "text": "There was no table to recreate so I decided to create one that looked at and ranked the number of roses that each winner received in their respective season.\nI tried to fix my table to have the first two rows with bigger font and to have different colours but I could not get it right. ChatGPT could not help me and kept on giving me the same responses and I gave up with my editing. I will pick it up again sometime.\n\n# Create a separate copy of the dataset\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column (winners)\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Identify the date columns (for checking the dates roses were given)\ndate_columns &lt;- grep(\"DATES-\", names(bachelorette_winner_check), value = TRUE)\n\n# Function to count the number of roses each winner received\ncount_roses &lt;- function(winner_row, date_columns, elim_columns) {\n  contestant &lt;- as.character(winner_row$CONTESTANT)\n  \n  date_roses &lt;- sum(sapply(date_columns, function(col) {\n    sum(grepl(contestant, as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  elim_roses &lt;- sum(sapply(elim_columns, function(col) {\n    sum(grepl(\"R\", as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  return(date_roses + elim_roses)  \n}\n\n# Create a data frame to store the winner and their respective rose counts\nrose_counts &lt;- data.frame(Winner = character(), Season = numeric(), Show_Type = character(), \n                          Roses_Received = numeric(), Roses = character(), stringsAsFactors = FALSE)\n\n# Loop over each winner and count the number of roses they received in their season\nfor (i in 1:nrow(winners)) {\n  winner_row &lt;- winners[i, ]\n  rose_count &lt;- count_roses(winner_row, date_columns, elim_columns)\n  winner_season &lt;- as.character(winner_row$SEASON)\n  winner_show &lt;- as.character(winner_row$SHOW)\n  \n  cleaned_winner_name &lt;- gsub(\"^\\\\d+_\", \"\", winner_row$CONTESTANT)\n  rose_symbol &lt;- paste(rep(\"🌹\", rose_count), collapse = \"\")  # Restored rose emoji 🌹\n  \n  rose_counts &lt;- rbind(rose_counts, data.frame(Winner = cleaned_winner_name, \n                                               Season = winner_season, \n                                               Show_Type = winner_show, \n                                               Roses_Received = rose_count, \n                                               Roses = rose_symbol, \n                                               stringsAsFactors = FALSE))\n}\n\n# Sort the table from most roses received to least\nrose_counts &lt;- rose_counts %&gt;% arrange(desc(Roses_Received))\n\n# Explanation text row\nexplanation_text &lt;- \"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette.\"\n\n# Display the table with improved formatting\nrose_counts %&gt;%\n  kable(col.names = c(\"Winner\", \"Season\", \"Show Type\", \"Roses Received\", \"Roses\"),\n        align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), escape = FALSE) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", \n                bootstrap_options = c(\"striped\", \"bordered\", \"hover\")) %&gt;%\n  \n  \n  \n  # Explanation row (light red background, no gridlines)\n  add_header_above(c(\"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\" = 5)) %&gt;%\n  row_spec(1, background = \"#FFCCCC\", italic = TRUE, hline_after = FALSE, extra_css = \"border-bottom: none;\") %&gt;%\n  \n  # Main title row: \"Roses to the Winners\" (large font)\n  add_header_above(c(\"Roses to the Winners\" = 5)) %&gt;%\n  row_spec(0, bold = TRUE, font_size = 28) %&gt;%  # Increased font size for title\n  \n  \n  # Subheading row (Winner, Season, etc.) - Pink background\n  row_spec(2, background = \"#FFD9D9\", bold = TRUE) %&gt;%\n  \n  # Adjust column spacing for better readability\n  column_spec(2, width = \"8em\") %&gt;%\n  column_spec(3, width = \"12em\") %&gt;%\n  column_spec(4, width = \"10em\") %&gt;%\n  column_spec(5, width = \"15em\") %&gt;%\n  \n  # Footer with source information\n  footnote(general = \"Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\")\n\n\n\n\n\n\n\n\n\n\n\n\nRoses to the Winners\n\n\n\n\nA breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\n\n\n\nWinner\nSeason\nShow Type\nRoses Received\nRoses\n\n\n\n\nBRYAN_A\n13\nBachelorette\n4\n🌹🌹🌹🌹 |\n\n\nJORDAN_R\n12\nBachelorette\n3\n🌹🌹🌹 |\n\n\nSHAWN_B\n11\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJEF_H\n8\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJP_R\n7\nBachelorette\n3\n🌹🌹🌹 |\n\n\nNIKKI_F\n18\nBachelor\n3\n🌹🌹🌹 |\n\n\nEMILY_M\n15\nBachelor\n3\n🌹🌹🌹 |\n\n\nJOSH_M\n10\nBachelorette\n2\n🌹🌹 |\n\n\nCHRIS_S\n9\nBachelorette\n2\n🌹🌹 |\n\n\nROBERT_M\n6\nBachelorette\n2\n🌹🌹 |\n\n\nJESSE_C\n4\nBachelorette\n2\n🌹🌹 |\n\n\nVANESSA_G\n21\nBachelor\n2\n🌹🌹 |\n\n\nLAUREN_B\n20\nBachelor\n2\n🌹🌹 |\n\n\nWHITNEY_B\n19\nBachelor\n2\n🌹🌹 |\n\n\nCATHERINE_G\n17\nBachelor\n2\n🌹🌹 |\n\n\nCOURTNEY_R\n16\nBachelor\n2\n🌹🌹 |\n\n\nJENNIFER_W\n9\nBachelor\n2\n🌹🌹 |\n\n\nED_S\n5\nBachelorette\n1\n🌹 |\n\n\nVIENNA_G\n14\nBachelor\n1\n🌹 |\n\n\nMELISSA_R\n13\nBachelor\n1\n🌹 |\n\n\nSHAYNE_L\n12\nBachelor\n1\n🌹 |\n\n\nTESSA_H\n10\nBachelor\n1\n🌹 |\n\n\nSarah_S\n8\nBachelor\n1\n🌹 |\n\n\nSarah B.\n7\nBachelor\n1\n🌹 |\n\n\nJERRY_F\n3\nBachelorette\n0\n\n\n\nIAN_M\n2\nBachelorette\n0\n\n\n\nRYAN_S\n1\nBachelorette\n0\n\n\n\nMary\n6\nBachelor\n0\n\n\n\nJESSICA_B\n5\nBachelor\n0\n\n\n\nEstella\n4\nBachelor\n0\n\n\n\nJEN_X\n3\nBachelor\n0\n\n\n\nHELENE_E\n2\nBachelor\n0\n\n\n\nAMANDA_M\n1\nBachelor\n0\n\n\n\n\nNote: \n\n\n\n\n\n\n Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI spent an embarrasingly long time trying to recreate this image that was in the article. I spent two days going back and forth with ChatGPT and trying to get it to format the graphic to recreate the original. I even tried with DeepSeek.\nI think I used over 3000 lines of R script with my back and forths and so I am not sharing my directions to R. But I even uploaded the image multiple times and asked it to expalin the image and how everything fitted together and I corrected each element. However, no matter how much promise it showed, even with starting multiple new chats, I was never able to fully correct the image to the correct order (challenge for someone else?). Every time I tried to fix one thing, it would break everything else.\nAfter spending all of the time (I am not exagerrating when I say two days) on this I eventually realised that the authors probably used an imaging software for this since it is more of an image than a graph……..🤕😭\nNonetheless, here are two different attempts. Please feel free to try your hand.\nThis was my best attempt:\n\n# Define dataset dynamically\ncategory_counts &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Eliminated on a date\",\n    \"Won\",\n    \"Quit\",\n    \"Eliminated at an unscheduled time\",\n    \"Fired by production\"\n  ),\n  Count = c(425, 298, 75, 33, 28, 21, 6)  # Updated counts from the provided data\n)\n\n# Calculate percentages dynamically\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Compute the circle packing layout\npacking &lt;- circleProgressiveLayout(category_counts$Count, sizetype = \"area\")\ncategory_counts$x &lt;- packing$x\ncategory_counts$y &lt;- packing$y\ncategory_counts$r &lt;- packing$radius\n\n# Generate circle layout vertices for plotting\ncircle_data &lt;- circleLayoutVertices(packing, npoints = 100)\n\n# Define colors: Pink for all except \"Won\", which is red\ncolors &lt;- c(\"#FADBD8\", \"#FADBD8\", \"#FADBD8\", \"#E74C3C\", \"#FADBD8\", \"#FADBD8\", \"#FADBD8\")\n\n# Create the bubble chart\nggplot() +\n  # Draw proportional circles with specified colors\n  geom_polygon(data = circle_data, aes(x, y, group = id, fill = as.factor(id)), \n               color = \"black\", size = 0.4, alpha = 0.8) +\n  \n  # Add contestant count inside bubbles\n  geom_text(data = category_counts, aes(x = x, y = y, label = Count), \n            size = 5, fontface = \"bold\", color = \"black\") +\n  \n  # Add category names with percentages using text repulsion\n  geom_text_repel(data = category_counts, \n                  aes(x = x, y = y, label = paste0(Category, \"\\n\", Percentage)), \n                  size = 4, color = \"black\", \n                  nudge_y = 0.5, nudge_x = 0.5, \n                  box.padding = 0.4, segment.color = \"black\") +\n  \n  # Clean up the theme\n  theme_void() +\n  theme(legend.position = \"none\") +\n  \n  # Apply the pink color for all except \"Won\" (which is red)\n  scale_fill_manual(values = colors)\n\n\n\n\n\n\n\n\nFor comedy relief I will share other attempts:\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Scale bubble size\nbubble_data$BubbleSize &lt;- sqrt(bubble_data$Count) * 12  \n\n# Generate circle layout\nset.seed(42)  \npacking &lt;- circleProgressiveLayout(bubble_data$BubbleSize, sizetype = \"radius\")\nbubble_data$x &lt;- packing$x\nbubble_data$y &lt;- packing$y\nbubble_data$radius &lt;- packing$radius\n\n# Generate circle layout vertices\ncircle_vertices &lt;- circleLayoutVertices(packing, npoints = 50)\n\n# Define colors for the bubbles\ncategory_colors &lt;- c(\n  \"Eliminated in weeks 2+ rose ceremony\" = \"#fbb4ae\",\n  \"Eliminated in week 1 rose ceremony\" = \"#fbb4ae\",\n  \"Fired by production\" = \"#fbb4ae\",\n  \"Quit\" = \"#fbb4ae\",\n  \"Eliminated on a date\" = \"#fbb4ae\",\n  \"Eliminated at an unscheduled time\" = \"#fbb4ae\",\n  \"Won\" = \"#e31a1c\"\n)\n\n# ===== MANUALLY ADJUST BUBBLE POSITIONS =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    x = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -1.6,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 1.2,  \n      Category == \"Eliminated on a date\" ~ 0.2,  \n      Category == \"Quit\" ~ 1.8,  \n      Category == \"Fired by production\" ~ 2.0,  \n      Category == \"Won\" ~ -0.8,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.5,  \n      TRUE ~ 0\n    ),\n    y = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.8,  \n      Category == \"Quit\" ~ -0.2,  \n      Category == \"Fired by production\" ~ 1.3,  \n      Category == \"Won\" ~ -0.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -0.9,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== ENSURING TEXT MOVES WITH BUBBLES =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    # Count label inside bubbles\n    Nudge_X = x,\n    Nudge_Y = y,\n\n    # Category text label positioning outside bubbles\n    Nudge_X_Category = case_when(\n      Category == \"Won\" ~ -1.2,  \n      Category == \"Quit\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.5,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -2,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.6,  \n      Category == \"Fired by production\" ~ 2.1,  \n      TRUE ~ 0\n    ),\n    Nudge_Y_Category = case_when(\n      Category == \"Won\" ~ -0.8,  \n      Category == \"Quit\" ~ -0.5,  \n      Category == \"Eliminated on a date\" ~ 0.6,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -1.2,  \n      Category == \"Fired by production\" ~ 1.5,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== CREATE THE BUBBLE PLOT =====\nggplot() +\n  # Draw bubbles\n  geom_polygon(data = circle_vertices, aes(x + bubble_data$x[id], y + bubble_data$y[id], \n                                           group = id, fill = bubble_data$Category[id]), \n               color = \"black\", alpha = 0.8) +  \n\n  # **Move COUNT labels inside bubbles**\n  geom_label(data = bubble_data %&gt;% filter(radius &gt; 30), \n             aes(x = x, y = y, label = Count, size = radius / 10), \n             fontface = \"bold\", fill = \"white\", alpha = 0.8,  \n             label.size = 0, hjust = 0.5, vjust = 0.5) +\n\n  # **Move COUNT labels outside small bubbles**\n  geom_label_repel(data = bubble_data %&gt;% filter(radius &lt;= 30), \n                   aes(x = x, y = y + radius * 3, label = Count, size = 5), \n                   fontface = \"bold\", fill = \"white\", color = \"black\",\n                   box.padding = 2, segment.color = \"black\", \n                   nudge_y = 20, direction = \"both\", force = 150) +\n\n  # **Move CATEGORY labels outside bubbles & allow multiple line text**\n  geom_label_repel(data = bubble_data, \n                   aes(x = x + Nudge_X_Category, y = y + Nudge_Y_Category, label = Category),  \n                   size = 4, color = \"black\", fontface = \"italic\",\n                   box.padding = 2, segment.color = \"black\", segment.size = 0.8,\n                   direction = \"both\", force = 400, label.size = 0.3, \n                   fill = \"white\", alpha = 0.9) +  \n\n  # **Move percentage labels inside bubbles**\n  geom_text(data = bubble_data, \n            aes(x = x, y = y + radius * 0.3, label = Percentage), \n            size = 3, color = \"black\", hjust = 0.5, vjust = 0.5) +\n\n  # Assign colors to the bubbles\n  scale_fill_manual(values = category_colors) +\n\n  # Remove default ggplot background\n  theme_void() +\n\n  # Adjust text styling\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n\n  # Set the title\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\nWarning: ggrepel: 3 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Create bubble chart without manual position adjustments\nggplot(bubble_data, aes(x = Count, y = Count, size = Count, fill = Category)) +\n  geom_point(alpha = 0.8, shape = 21, color = \"black\") +  # Use points instead of polygons\n  scale_size(range = c(5, 25)) +  # Adjust bubble scaling dynamically\n  scale_fill_viridis_d() +  # Dynamic color scaling for categories\n  geom_text_repel(aes(label = Count), size = 5, fontface = \"bold\", color = \"black\") +  # Automatic text positioning\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\n\n\n\n\n\n\n\n~ Fin"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation-exercise",
    "section": "",
    "text": "I will be attempting to recreate graphs from an article published by fivethirtyeight.com about contestants in the television shows, The Bachelor and Bachelorette. The article and data demonstrates a pattern between roses that are received in the early stages of the show and the performance of a candidate overall. The data evaluates all 33 seasons of the show.\nThe article may be found here: https://fivethirtyeight.com/features/the-bachelorette/\nThe raw data may be found here: https://github.com/fivethirtyeight/data/tree/master/bachelorette\nDisclaimer: I do not know anything about this show and have not watched any episodes, so forgive me if my explanations are not entirely on point. I chose this article because the graphs and topic seemed fun and light-hearted amongst all the political content.\n\n\nFirst I will load all of the required packages and well as read the raw data CSV file:\n\n#install.packages(\"ggforce\")\n#install.packages(\"packcircles\")\n#install.packages(\"ggrepel\")\n#install.packages(\"viridis\")\n#install.packages(\"kableExtra\")\n\n\nlibrary(\"ggplot2\")\nlibrary(\"tidyverse\")\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"knitr\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggforce\")\n\nWarning: package 'ggforce' was built under R version 4.4.2\n\nlibrary(\"packcircles\")\n\nWarning: package 'packcircles' was built under R version 4.4.2\n\nlibrary(\"ggrepel\")\n\nWarning: package 'ggrepel' was built under R version 4.4.2\n\nlibrary(\"viridis\")\n\nWarning: package 'viridis' was built under R version 4.4.2\n\n\nLoading required package: viridisLite\n\nlibrary(\"kableExtra\")\n\nWarning: package 'kableExtra' was built under R version 4.4.2\n\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nbachelorette_data &lt;- read_csv(here::here(\n  \"presentation-exercise/bachelorette.csv\"))\n\nRows: 920 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): SHOW, SEASON, CONTESTANT, ELIMINATION-1, ELIMINATION-2, ELIMINATIO...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(bachelorette_data)\n\n     SHOW              SEASON           CONTESTANT        ELIMINATION-1     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-2      ELIMINATION-3      ELIMINATION-4      ELIMINATION-5     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-6      ELIMINATION-7      ELIMINATION-8      ELIMINATION-9     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-10       DATES-1            DATES-2            DATES-3         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-4            DATES-5            DATES-6            DATES-7         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-8            DATES-9            DATES-10        \n Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n\n\nPerform some formating of the data to remove the first row which is a repetition of headings and to change the seasons to numeric values. We are also removing rows where headings are repeated within the data:\n\n# Remove the incorrect header row if necessary\nbachelorette_data &lt;- bachelorette_data[-1, ]  # Remove first row if it's incorrect\n\n# Convert SEASON to numeric\nbachelorette_data$SEASON &lt;- as.numeric(bachelorette_data$SEASON)\n\nWarning: NAs introduced by coercion\n\n# Remove rows where the CONTESTANT column contains \"ID\"\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  filter(CONTESTANT != \"ID\")\n\n# Reset row indices (optional)\nbachelorette_data &lt;- bachelorette_data %&gt;% mutate(row_id = row_number())\n\n# Display the cleaned data\nprint(head(bachelorette_data))\n\n# A tibble: 6 × 24\n  SHOW         SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n1 Bachelorette     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n2 Bachelorette     13 13_PETER_K &lt;NA&gt;            R               &lt;NA&gt;           \n3 Bachelorette     13 13_ERIC_B  &lt;NA&gt;            &lt;NA&gt;            R              \n4 Bachelorette     13 13_DEAN_U  &lt;NA&gt;            R               &lt;NA&gt;           \n5 Bachelorette     13 13_ADAM_G  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n6 Bachelorette     13 13_MATT_M  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 18 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;, `DATES-1` &lt;chr&gt;,\n#   `DATES-2` &lt;chr&gt;, `DATES-3` &lt;chr&gt;, `DATES-4` &lt;chr&gt;, `DATES-5` &lt;chr&gt;,\n#   `DATES-6` &lt;chr&gt;, `DATES-7` &lt;chr&gt;, `DATES-8` &lt;chr&gt;, `DATES-9` &lt;chr&gt;,\n#   `DATES-10` &lt;chr&gt;, row_id &lt;int&gt;\n\n\nSome information was provided on the website with the raw data which may help with interpretation of this data:\n\n#Adding additional data into a table\ncolumn_names &lt;- c(\"Header\", \"Description\")\ntable_data &lt;- data.frame(\n  Header = c(\n    \"SHOW\", \"SEASON\", \"CONTESTANT\", \"ELIMINATION-1\", \"ELIMINATION-2\",\n    \"ELIMINATION-3\", \"ELIMINATION-4\", \"ELIMINATION-5\", \"ELIMINATION-6\",\n    \"ELIMINATION-7\", \"ELIMINATION-8\", \"ELIMINATION-9\", \"ELIMINATION-10\",\n    \"DATES-1\", \"DATES-2\", \"DATES-3\", \"DATES-4\", \"DATES-5\", \"DATES-6\",\n    \"DATES-7\", \"DATES-8\", \"DATES-9\", \"DATES-10\"\n  ),\n  Description = c(\n    \"Bachelor or Bachelorette\", \"Which season\", \"An identifier for the contestant in a given season\",\n    \"Who was eliminated in week 1\", \"Who was eliminated in week 2\", \"Who was eliminated in week 3\",\n    \"Who was eliminated in week 4\", \"Who was eliminated in week 5\", \"Who was eliminated in week 6\",\n    \"Who was eliminated in week 7\", \"Who was eliminated in week 8\", \"Who was eliminated in week 9\",\n    \"Who was eliminated in week 10\", \"Who was on which date in week 1\", \"Who was on which date in week 2\",\n    \"Who was on which date in week 3\", \"Who was on which date in week 4\", \"Who was on which date in week 5\",\n    \"Who was on which date in week 6\", \"Who was on which date in week 7\", \"Who was on which date in week 8\",\n    \"Who was on which date in week 9\", \"Who was on which date in week 10\"\n  )\n)\n\n# Print table using kable for better formatting\nknitr::kable(table_data, col.names = column_names, caption = \"Table of Data Headers and Descriptions\")\n\n\nTable of Data Headers and Descriptions\n\n\nHeader\nDescription\n\n\n\n\nSHOW\nBachelor or Bachelorette\n\n\nSEASON\nWhich season\n\n\nCONTESTANT\nAn identifier for the contestant in a given season\n\n\nELIMINATION-1\nWho was eliminated in week 1\n\n\nELIMINATION-2\nWho was eliminated in week 2\n\n\nELIMINATION-3\nWho was eliminated in week 3\n\n\nELIMINATION-4\nWho was eliminated in week 4\n\n\nELIMINATION-5\nWho was eliminated in week 5\n\n\nELIMINATION-6\nWho was eliminated in week 6\n\n\nELIMINATION-7\nWho was eliminated in week 7\n\n\nELIMINATION-8\nWho was eliminated in week 8\n\n\nELIMINATION-9\nWho was eliminated in week 9\n\n\nELIMINATION-10\nWho was eliminated in week 10\n\n\nDATES-1\nWho was on which date in week 1\n\n\nDATES-2\nWho was on which date in week 2\n\n\nDATES-3\nWho was on which date in week 3\n\n\nDATES-4\nWho was on which date in week 4\n\n\nDATES-5\nWho was on which date in week 5\n\n\nDATES-6\nWho was on which date in week 6\n\n\nDATES-7\nWho was on which date in week 7\n\n\nDATES-8\nWho was on which date in week 8\n\n\nDATES-9\nWho was on which date in week 9\n\n\nDATES-10\nWho was on which date in week 10\n\n\n\n\n\nFurther information required for interpretation and processing:\nEliminates connote either an elimination (starts with “E”) or a rose (starts with “R”). Eliminations supercede roses. “E” connotes a standard elimination, typically at a rose ceremony. “EQ” means the contestant quits. “EF” means the contestant was fired by production. “ED” connotes a date elimination. “EU” connotes an unscheduled elimination, one that takes place at a time outside of a date or rose ceremony. “R” means the contestant received a rose. “R1” means the contestant got a first impression rose. “D1” means a one-on-one date, “D2” means a 2-on-1, “D3” means a 3-on-1 group date, and so on. “W” in E10 indicates that the contestant won. Weeks of the show are deliminated by rose ceremonies, and may not line up exactly with episodes.\n………………….\nI will be using ChatGPT to help me to generate the code that will help to obtain the graph that I want. I will post my prompts and the outcome as well as the different iterations below.\nWe are not getting the correct number of winners\n\n# Get all ELIMINATION columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Count occurrences of \"W\" in each ELIMINATION column\nw_counts &lt;- sapply(elim_columns, function(col) sum(bachelorette_data[[col]] == \"W\", na.rm = TRUE))\n\n# Print counts for each column\nprint(\"Number of 'W' occurrences per ELIMINATION column:\")\n\n[1] \"Number of 'W' occurrences per ELIMINATION column:\"\n\nprint(w_counts)\n\n ELIMINATION-1  ELIMINATION-2  ELIMINATION-3  ELIMINATION-4  ELIMINATION-5 \n             0              0              0              0              0 \n ELIMINATION-6  ELIMINATION-7  ELIMINATION-8  ELIMINATION-9 ELIMINATION-10 \n             2              8              7              1             15 \n\n# Total winners found in dataset\ntotal_winners &lt;- sum(w_counts)\nprint(paste(\"Total expected winners:\", total_winners))\n\n[1] \"Total expected winners: 33\"\n\n\nNow we found that it seems that winners were marked in different levels of the competition (Elimination 6 - 10 and not just the last two). We will run this again:\n\n# Define function to classify contestants, now checking ELIMINATION-6 to ELIMINATION-10\nclassify_contestant &lt;- function(row) {\n  elim_columns &lt;- grep(\"ELIMINATION-\", names(row), value = TRUE)\n\n  # **Ensure all ELIMINATION columns are checked for \"W\"**\n  w_found &lt;- any(sapply(elim_columns, function(col) toupper(trimws(row[[col]])) == \"W\"), na.rm = TRUE)\n\n  # **First, Check for Winners** - If \"W\" appears in ELIMINATION-6 through ELIMINATION-10, classify as \"Won\"\n  if (w_found) {\n    return(\"Won\")\n  }\n\n  # **Next, Track First Elimination Event**\n  for (col in elim_columns) {\n    value &lt;- ifelse(is.na(row[[col]]), \"\", toupper(trimws(as.character(row[[col]]))))  # Handle NA values\n\n    if (grepl(\"^E\", value)) {  # Any elimination starting with \"E\"\n      if (value == \"E\" && col == \"ELIMINATION-1\") {\n        return(\"Eliminated in week 1 rose ceremony\")\n      } else if (value == \"E\") {\n        return(\"Eliminated in weeks 2+ rose ceremony\")\n      } else if (value == \"EQ\") {\n        return(\"Quit\")\n      } else if (value == \"EF\") {\n        return(\"Fired by production\")\n      } else if (value == \"ED\") {\n        return(\"Eliminated on a date\")\n      } else if (value == \"EU\") {\n        return(\"Eliminated at an unscheduled time\")\n      }\n    }\n  }\n\n  return(NA)  # Avoid misclassification\n}\n\n# Apply classification to dataset using `pick()`\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  rowwise() %&gt;%\n  mutate(Category = classify_contestant(pick(everything()))) %&gt;%\n  ungroup()\n\n# Remove NA rows (contestants who do not fit a category)\nbachelorette_data &lt;- bachelorette_data %&gt;% filter(!is.na(Category))\n\n# Count occurrences of each category\ncategory_counts &lt;- bachelorette_data %&gt;%\n  group_by(Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  arrange(desc(Count))\n\n# Print corrected category counts\nprint(category_counts)\n\n# A tibble: 7 × 2\n  Category                             Count\n  &lt;chr&gt;                                &lt;int&gt;\n1 Eliminated in weeks 2+ rose ceremony   425\n2 Eliminated in week 1 rose ceremony     298\n3 Eliminated on a date                    75\n4 Won                                     33\n5 Quit                                    28\n6 Eliminated at an unscheduled time       21\n7 Fired by production                      6\n\n# Display the updated category breakdown table using kable\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\"), caption = \"Final Category Breakdown\")\n\n\nFinal Category Breakdown\n\n\nCategory\nCount\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n\n\nEliminated in week 1 rose ceremony\n298\n\n\nEliminated on a date\n75\n\n\nWon\n33\n\n\nQuit\n28\n\n\nEliminated at an unscheduled time\n21\n\n\nFired by production\n6\n\n\n\n\n\nWe have picked up an additonal winner than was in the original dataset (33 instead of 32), we will use this code to verify who they were so we can be confident with our analysis:\n\n# Create a separate copy of the dataset (optional safety step)\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns: SHOW, SEASON, CONTESTANT, and elimination columns where \"W\" appeared\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Print the identified winners\nprint(winners)\n\n# A tibble: 33 × 13\n   SHOW        SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n 1 Bacheloret…     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n 2 Bacheloret…     12 12_JORDAN… R1              &lt;NA&gt;            &lt;NA&gt;           \n 3 Bacheloret…     11 11_SHAWN_B R1              &lt;NA&gt;            R              \n 4 Bacheloret…     10 10_JOSH_M  &lt;NA&gt;            &lt;NA&gt;            R              \n 5 Bacheloret…      9 09_CHRIS_S &lt;NA&gt;            &lt;NA&gt;            R              \n 6 Bacheloret…      8 08_JEF_H   &lt;NA&gt;            R               &lt;NA&gt;           \n 7 Bacheloret…      7 07_JP_R    &lt;NA&gt;            &lt;NA&gt;            R              \n 8 Bacheloret…      6 06_ROBERT… R1              &lt;NA&gt;            R              \n 9 Bacheloret…      5 05_ED_S    &lt;NA&gt;            &lt;NA&gt;            R              \n10 Bacheloret…      4 04_JESSE_C R1              &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 23 more rows\n# ℹ 7 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;\n\n\nWe have higher numbers in the other categories too, but we will assume that this is because the raw data has an additional season which was not included in the published graphs.\n\n# Count the total number of unique seasons for each show (Bachelor/Bachelorette)\nseason_counts &lt;- bachelorette_data %&gt;%\n  group_by(SHOW) %&gt;%\n  summarise(Total_Seasons = n_distinct(SEASON)) %&gt;%\n  arrange(desc(Total_Seasons))  # Sort by number of seasons\n\n# Print the season counts\nprint(season_counts)\n\n# A tibble: 2 × 2\n  SHOW         Total_Seasons\n  &lt;chr&gt;                &lt;int&gt;\n1 Bachelor                21\n2 Bachelorette            13\n\n# Display the table using kable\nknitr::kable(season_counts, col.names = c(\"Show\", \"Total Seasons\"),\n             caption = \"Total Number of Seasons by Show\")\n\n\nTotal Number of Seasons by Show\n\n\nShow\nTotal Seasons\n\n\n\n\nBachelor\n21\n\n\nBachelorette\n13\n\n\n\n\n\nTheir data indicated: Eliminated in weeks 2+ rose ceremony 411 Eliminated in week 1 rose ceremony 291 Won 32 Eliminated on a date 68 Eliminated at an unscheduled time 20 Quit 28 Fired by production 6\nOur results are: Eliminated in weeks 2+ rose ceremony 425\nEliminated in week 1 rose ceremony 298\nWon 33\nEliminated on a date 75\nEliminated at an unscheduled time 21\nQuit 28\nFired by production 6\nWe want to get the percentage for each of the categories so that we can include these in the graphic.\n\n# Calculate total number of contestants classified\ntotal_contestants &lt;- sum(category_counts$Count)\n\n# Add percentage column, rounded to the nearest whole number\n# Ensure category_counts has correctly formatted columns\ncategory_counts &lt;- category_counts %&gt;%\n  rename(Count = Count) %&gt;%  # Ensure correct naming\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Add percentage column\n\n\n#category_counts &lt;- category_counts %&gt;%\n # mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Calculate and format as \"X%\"\n\n\n# Print updated table\nprint(category_counts)\n\n# A tibble: 7 × 3\n  Category                             Count Percentage\n  &lt;chr&gt;                                &lt;int&gt; &lt;chr&gt;     \n1 Eliminated in weeks 2+ rose ceremony   425 48%       \n2 Eliminated in week 1 rose ceremony     298 34%       \n3 Eliminated on a date                    75 8%        \n4 Won                                     33 4%        \n5 Quit                                    28 3%        \n6 Eliminated at an unscheduled time       21 2%        \n7 Fired by production                      6 1%        \n\n# Display table using kable with percentages\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\", \"Percentage (%)\"),\n             caption = \"Final Category Breakdown with Rounded Percentages\")\n\n\nFinal Category Breakdown with Rounded Percentages\n\n\nCategory\nCount\nPercentage (%)\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n48%\n\n\nEliminated in week 1 rose ceremony\n298\n34%\n\n\nEliminated on a date\n75\n8%\n\n\nWon\n33\n4%\n\n\nQuit\n28\n3%\n\n\nEliminated at an unscheduled time\n21\n2%\n\n\nFired by production\n6\n1%\n\n\n\n\n\n\n# Extract rose-related events (e.g., R1, R, D1, etc.) from elimination columns\nrose_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check if a contestant received a rose in each episode\nget_rose_status &lt;- function(row) {\n  rose_events &lt;- row[rose_columns]\n  return(ifelse(any(grepl(\"R\", rose_events)), which(grepl(\"R\", rose_events))[1], NA))\n}\n\n# Apply the function to get the first episode where each contestant received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_status)\n\n\n# Identify eliminations\neliminate_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check elimination status (either eliminated or not)\nget_elimination_status &lt;- function(row) {\n  elimination_status &lt;- row[eliminate_columns]\n  return(sum(grepl(\"E\", elimination_status), na.rm = TRUE))\n}\n\n# Calculate the cumulative number of eliminations by episode\nbachelorette_data$Eliminations &lt;- apply(bachelorette_data, 1, get_elimination_status)\n\n# Calculate total contestants\ntotal_contestants &lt;- nrow(bachelorette_data)\n\n# Calculate cumulative eliminations\ncumulative_elim &lt;- cumsum(bachelorette_data$Eliminations)\n\n# Calculate cumulative percent chance of elimination\nbachelorette_data$Cumulative_Elim_Percent &lt;- cumulative_elim / total_contestants * 100\n\n\n\nHere are the two original images that i tried to recreate, with the original graph looking at the effect of receiving a rose early on being eliminated:\n\n\n\nGraph evaluating the effect of wiining roses early on and elimination.\n\n\nThe second graph (image) - see at the bottom of the document:\n\n\n\nThe breakdown of different contestants on the bachelor and bachelorette\n\n\nThis is how we (ChatGPT and me) started with the recreation of this graph:\nFor my prompt I uploaded the image of the graph and asked it to analyse it. I then uploaded the raw dataset and asked if it could recreate the graph. It seemed to trace a similar graph but I think it might have just copied the image a bit. So I asked it to provide the R code and to provide steps to recreate it. I then received the first graph provided below which was not similar at all (as you can see).\nI then explained what the variables on each axis were and I explained that there are two curves and what they are based upon. I then explained what we might be calculating and that it should include any data analysis steps that may be required. The results thereafter were much better and closer to the original and i weaked some of the smaller labels and things like that.\nIt was quite good at rendering a similar graph however our shape never quite reenacted the original data. Unfortunately, I am not sure how they performed this for clear comparison but the end result was somewhat similar. Given the issues that I picked uo with the data processing part I am not sure where the issue might be coming from.\n\nlibrary(ggplot2)\n\n# Filter the data for contestants who received a rose and those who did not\nrose_data &lt;- bachelorette_data[!is.na(bachelorette_data$Rose_Episode), ]\nno_rose_data &lt;- bachelorette_data[is.na(bachelorette_data$Rose_Episode), ]\n\n# Create the plot\nggplot() +\n  geom_line(data = rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Gets a date rose\"), size = 1) +\n  geom_line(data = no_rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Doesn't get a date rose\"), size = 1) +\n  labs(title = \"The Date Rose Advantage\\nCumulative Chance of Being Eliminated\",\n       x = \"Episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 674 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWhere we ended up after many back and forths and starting again to focus on the basic principles of the curve:\n\n# Identify the columns for eliminations\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to identify when a contestant received a rose\nget_rose_episode &lt;- function(row) {\n  for (i in seq_along(row[elim_columns])) {\n    if (grepl(\"R\", row[elim_columns[i]])) {\n      return(i)  # Episode number starts from 1\n    }\n  }\n  return(NA)  # If no rose received, return NA\n}\n\n# Apply the function to the dataset to create a new column with the episode they received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_episode)\n\n# Initialize empty lists to track eliminations for both groups (rose vs no-rose)\nelim_counts_rose &lt;- numeric(0)\nelim_counts_no_rose &lt;- numeric(0)\nweeks_after_rose_rose &lt;- numeric(0)\nweeks_after_rose_no_rose &lt;- numeric(0)\n\n# Variables to store total eliminations for both groups\nelim_rose &lt;- 0\nelim_no_rose &lt;- 0\n\n# Track eliminations and weeks after receiving the rose\nfor (i in 1:length(elim_columns)) {\n  \n  # For eliminations in the current episode\n  eliminated_in_episode &lt;- grepl(\"E\", bachelorette_data[[elim_columns[i]]])\n  \n  # For contestants who received a rose\n  eliminated_rose_group &lt;- eliminated_in_episode & !is.na(bachelorette_data$Rose_Episode)\n  elim_rose &lt;- elim_rose + sum(eliminated_rose_group)\n  elim_counts_rose &lt;- c(elim_counts_rose, elim_rose / sum(!is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from rose group\n  weeks_after_rose_rose &lt;- c(weeks_after_rose_rose, i)  # Track weeks after the rose\n  \n  # For contestants who did not receive a rose\n  eliminated_no_rose_group &lt;- eliminated_in_episode & is.na(bachelorette_data$Rose_Episode)\n  elim_no_rose &lt;- elim_no_rose + sum(eliminated_no_rose_group)\n  elim_counts_no_rose &lt;- c(elim_counts_no_rose, elim_no_rose / sum(is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from no-rose group\n  weeks_after_rose_no_rose &lt;- c(weeks_after_rose_no_rose, i)  # Track weeks after the rose\n\n}\n\n# Create a combined dataset for plotting\nelim_data &lt;- data.frame(\n  Weeks_After_Rose = c(weeks_after_rose_rose, weeks_after_rose_no_rose),\n  Cumulative_Elim_Percent = c(elim_counts_rose, elim_counts_no_rose),\n  Group = rep(c(\"Gets a date rose\", \"Doesn't get a date rose\"), each = length(elim_columns))\n)\n\n# Plot the graph\nplot &lt;- ggplot(elim_data, aes(x = Weeks_After_Rose, y = Cumulative_Elim_Percent, color = Group)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  labs(title = \"The Date Rose Advantage\",\n       x = \"Weeks after episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_x_continuous(breaks = seq(0, length(elim_columns), by = 1), labels = paste0(\"+\", seq(0, length(elim_columns), by = 1))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", \n        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n        axis.text.y = element_text(size = 12),\n        axis.title.x = element_text(size = 14),\n        axis.title.y = element_text(size = 14)) +\n  ylim(0, 100)  # Ensure y-axis is from 0 to 100%\n\n# Add the shorter measurement line between the two curves (shifted to the left)\nplot + \n  geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", linetype = \"dashed\", size = 1) +  # Line from x = 2 to x = 6\n  \n  # Add label for \"Rose advantage\"\n  annotate(\"text\", x = 4, y = 55, label = \"Rose advantage\", size = 6, angle = 0, hjust = 0.5)\n\nWarning in geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", : All aesthetics have length 1, but the data has 20 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nI do not know if the differences come from the original data or the approach in data anaylsis or the graph rendering.\n\n\n\n\nThere was no table to recreate so I decided to create one that looked at and ranked the number of roses that each winner received in their respective season.\nI tried to fix my table to have the first two rows with bigger font and to have different colours but I could not get it right. ChatGPT could not help me and kept on giving me the same responses and I gave up with my editing. I will pick it up again sometime.\n\n# Create a separate copy of the dataset\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column (winners)\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Identify the date columns (for checking the dates roses were given)\ndate_columns &lt;- grep(\"DATES-\", names(bachelorette_winner_check), value = TRUE)\n\n# Function to count the number of roses each winner received\ncount_roses &lt;- function(winner_row, date_columns, elim_columns) {\n  contestant &lt;- as.character(winner_row$CONTESTANT)\n  \n  date_roses &lt;- sum(sapply(date_columns, function(col) {\n    sum(grepl(contestant, as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  elim_roses &lt;- sum(sapply(elim_columns, function(col) {\n    sum(grepl(\"R\", as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  return(date_roses + elim_roses)  \n}\n\n# Create a data frame to store the winner and their respective rose counts\nrose_counts &lt;- data.frame(Winner = character(), Season = numeric(), Show_Type = character(), \n                          Roses_Received = numeric(), Roses = character(), stringsAsFactors = FALSE)\n\n# Loop over each winner and count the number of roses they received in their season\nfor (i in 1:nrow(winners)) {\n  winner_row &lt;- winners[i, ]\n  rose_count &lt;- count_roses(winner_row, date_columns, elim_columns)\n  winner_season &lt;- as.character(winner_row$SEASON)\n  winner_show &lt;- as.character(winner_row$SHOW)\n  \n  cleaned_winner_name &lt;- gsub(\"^\\\\d+_\", \"\", winner_row$CONTESTANT)\n  rose_symbol &lt;- paste(rep(\"🌹\", rose_count), collapse = \"\")  # Restored rose emoji 🌹\n  \n  rose_counts &lt;- rbind(rose_counts, data.frame(Winner = cleaned_winner_name, \n                                               Season = winner_season, \n                                               Show_Type = winner_show, \n                                               Roses_Received = rose_count, \n                                               Roses = rose_symbol, \n                                               stringsAsFactors = FALSE))\n}\n\n# Sort the table from most roses received to least\nrose_counts &lt;- rose_counts %&gt;% arrange(desc(Roses_Received))\n\n# Explanation text row\nexplanation_text &lt;- \"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette.\"\n\n# Display the table with improved formatting\nrose_counts %&gt;%\n  kable(col.names = c(\"Winner\", \"Season\", \"Show Type\", \"Roses Received\", \"Roses\"),\n        align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), escape = FALSE) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", \n                bootstrap_options = c(\"striped\", \"bordered\", \"hover\")) %&gt;%\n  \n  \n  \n  # Explanation row (light red background, no gridlines)\n  add_header_above(c(\"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\" = 5)) %&gt;%\n  row_spec(1, background = \"#FFCCCC\", italic = TRUE, hline_after = FALSE, extra_css = \"border-bottom: none;\") %&gt;%\n  \n  # Main title row: \"Roses to the Winners\" (large font)\n  add_header_above(c(\"Roses to the Winners\" = 5)) %&gt;%\n  row_spec(0, bold = TRUE, font_size = 28) %&gt;%  # Increased font size for title\n  \n  \n  # Subheading row (Winner, Season, etc.) - Pink background\n  row_spec(2, background = \"#FFD9D9\", bold = TRUE) %&gt;%\n  \n  # Adjust column spacing for better readability\n  column_spec(2, width = \"8em\") %&gt;%\n  column_spec(3, width = \"12em\") %&gt;%\n  column_spec(4, width = \"10em\") %&gt;%\n  column_spec(5, width = \"15em\") %&gt;%\n  \n  # Footer with source information\n  footnote(general = \"Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\")\n\n\n\n\n\n\n\n\n\n\n\n\nRoses to the Winners\n\n\n\n\nA breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\n\n\n\nWinner\nSeason\nShow Type\nRoses Received\nRoses\n\n\n\n\nBRYAN_A\n13\nBachelorette\n4\n🌹🌹🌹🌹 |\n\n\nJORDAN_R\n12\nBachelorette\n3\n🌹🌹🌹 |\n\n\nSHAWN_B\n11\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJEF_H\n8\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJP_R\n7\nBachelorette\n3\n🌹🌹🌹 |\n\n\nNIKKI_F\n18\nBachelor\n3\n🌹🌹🌹 |\n\n\nEMILY_M\n15\nBachelor\n3\n🌹🌹🌹 |\n\n\nJOSH_M\n10\nBachelorette\n2\n🌹🌹 |\n\n\nCHRIS_S\n9\nBachelorette\n2\n🌹🌹 |\n\n\nROBERT_M\n6\nBachelorette\n2\n🌹🌹 |\n\n\nJESSE_C\n4\nBachelorette\n2\n🌹🌹 |\n\n\nVANESSA_G\n21\nBachelor\n2\n🌹🌹 |\n\n\nLAUREN_B\n20\nBachelor\n2\n🌹🌹 |\n\n\nWHITNEY_B\n19\nBachelor\n2\n🌹🌹 |\n\n\nCATHERINE_G\n17\nBachelor\n2\n🌹🌹 |\n\n\nCOURTNEY_R\n16\nBachelor\n2\n🌹🌹 |\n\n\nJENNIFER_W\n9\nBachelor\n2\n🌹🌹 |\n\n\nED_S\n5\nBachelorette\n1\n🌹 |\n\n\nVIENNA_G\n14\nBachelor\n1\n🌹 |\n\n\nMELISSA_R\n13\nBachelor\n1\n🌹 |\n\n\nSHAYNE_L\n12\nBachelor\n1\n🌹 |\n\n\nTESSA_H\n10\nBachelor\n1\n🌹 |\n\n\nSarah_S\n8\nBachelor\n1\n🌹 |\n\n\nSarah B.\n7\nBachelor\n1\n🌹 |\n\n\nJERRY_F\n3\nBachelorette\n0\n\n\n\nIAN_M\n2\nBachelorette\n0\n\n\n\nRYAN_S\n1\nBachelorette\n0\n\n\n\nMary\n6\nBachelor\n0\n\n\n\nJESSICA_B\n5\nBachelor\n0\n\n\n\nEstella\n4\nBachelor\n0\n\n\n\nJEN_X\n3\nBachelor\n0\n\n\n\nHELENE_E\n2\nBachelor\n0\n\n\n\nAMANDA_M\n1\nBachelor\n0\n\n\n\n\nNote: \n\n\n\n\n\n\n Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI spent an embarrasingly long time trying to recreate this image that was in the article. I spent two days going back and forth with ChatGPT and trying to get it to format the graphic to recreate the original. I even tried with DeepSeek.\nI think I used over 3000 lines of R script with my back and forths and so I am not sharing my directions to R. But I even uploaded the image multiple times and asked it to expalin the image and how everything fitted together and I corrected each element. However, no matter how much promise it showed, even with starting multiple new chats, I was never able to fully correct the image to the correct order (challenge for someone else?). Every time I tried to fix one thing, it would break everything else.\nAfter spending all of the time (I am not exagerrating when I say two days) on this I eventually realised that the authors probably used an imaging software for this since it is more of an image than a graph……..🤕😭\nNonetheless, here are two different attempts. Please feel free to try your hand.\nThis was my best attempt:\n\n# Define dataset dynamically\ncategory_counts &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Eliminated on a date\",\n    \"Won\",\n    \"Quit\",\n    \"Eliminated at an unscheduled time\",\n    \"Fired by production\"\n  ),\n  Count = c(425, 298, 75, 33, 28, 21, 6)  # Updated counts from the provided data\n)\n\n# Calculate percentages dynamically\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Compute the circle packing layout\npacking &lt;- circleProgressiveLayout(category_counts$Count, sizetype = \"area\")\ncategory_counts$x &lt;- packing$x\ncategory_counts$y &lt;- packing$y\ncategory_counts$r &lt;- packing$radius\n\n# Generate circle layout vertices for plotting\ncircle_data &lt;- circleLayoutVertices(packing, npoints = 100)\n\n# Define colors: Pink for all except \"Won\", which is red\ncolors &lt;- c(\"#FADBD8\", \"#FADBD8\", \"#FADBD8\", \"#E74C3C\", \"#FADBD8\", \"#FADBD8\", \"#FADBD8\")\n\n# Create the bubble chart\nggplot() +\n  # Draw proportional circles with specified colors\n  geom_polygon(data = circle_data, aes(x, y, group = id, fill = as.factor(id)), \n               color = \"black\", size = 0.4, alpha = 0.8) +\n  \n  # Add contestant count inside bubbles\n  geom_text(data = category_counts, aes(x = x, y = y, label = Count), \n            size = 5, fontface = \"bold\", color = \"black\") +\n  \n  # Add category names with percentages using text repulsion\n  geom_text_repel(data = category_counts, \n                  aes(x = x, y = y, label = paste0(Category, \"\\n\", Percentage)), \n                  size = 4, color = \"black\", \n                  nudge_y = 0.5, nudge_x = 0.5, \n                  box.padding = 0.4, segment.color = \"black\") +\n  \n  # Clean up the theme\n  theme_void() +\n  theme(legend.position = \"none\") +\n  \n  # Apply the pink color for all except \"Won\" (which is red)\n  scale_fill_manual(values = colors)\n\n\n\n\n\n\n\n\nFor comedy relief I will share other attempts:\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Scale bubble size\nbubble_data$BubbleSize &lt;- sqrt(bubble_data$Count) * 12  \n\n# Generate circle layout\nset.seed(42)  \npacking &lt;- circleProgressiveLayout(bubble_data$BubbleSize, sizetype = \"radius\")\nbubble_data$x &lt;- packing$x\nbubble_data$y &lt;- packing$y\nbubble_data$radius &lt;- packing$radius\n\n# Generate circle layout vertices\ncircle_vertices &lt;- circleLayoutVertices(packing, npoints = 50)\n\n# Define colors for the bubbles\ncategory_colors &lt;- c(\n  \"Eliminated in weeks 2+ rose ceremony\" = \"#fbb4ae\",\n  \"Eliminated in week 1 rose ceremony\" = \"#fbb4ae\",\n  \"Fired by production\" = \"#fbb4ae\",\n  \"Quit\" = \"#fbb4ae\",\n  \"Eliminated on a date\" = \"#fbb4ae\",\n  \"Eliminated at an unscheduled time\" = \"#fbb4ae\",\n  \"Won\" = \"#e31a1c\"\n)\n\n# ===== MANUALLY ADJUST BUBBLE POSITIONS =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    x = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -1.6,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 1.2,  \n      Category == \"Eliminated on a date\" ~ 0.2,  \n      Category == \"Quit\" ~ 1.8,  \n      Category == \"Fired by production\" ~ 2.0,  \n      Category == \"Won\" ~ -0.8,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.5,  \n      TRUE ~ 0\n    ),\n    y = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.8,  \n      Category == \"Quit\" ~ -0.2,  \n      Category == \"Fired by production\" ~ 1.3,  \n      Category == \"Won\" ~ -0.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -0.9,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== ENSURING TEXT MOVES WITH BUBBLES =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    # Count label inside bubbles\n    Nudge_X = x,\n    Nudge_Y = y,\n\n    # Category text label positioning outside bubbles\n    Nudge_X_Category = case_when(\n      Category == \"Won\" ~ -1.2,  \n      Category == \"Quit\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.5,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -2,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.6,  \n      Category == \"Fired by production\" ~ 2.1,  \n      TRUE ~ 0\n    ),\n    Nudge_Y_Category = case_when(\n      Category == \"Won\" ~ -0.8,  \n      Category == \"Quit\" ~ -0.5,  \n      Category == \"Eliminated on a date\" ~ 0.6,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -1.2,  \n      Category == \"Fired by production\" ~ 1.5,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== CREATE THE BUBBLE PLOT =====\nggplot() +\n  # Draw bubbles\n  geom_polygon(data = circle_vertices, aes(x + bubble_data$x[id], y + bubble_data$y[id], \n                                           group = id, fill = bubble_data$Category[id]), \n               color = \"black\", alpha = 0.8) +  \n\n  # **Move COUNT labels inside bubbles**\n  geom_label(data = bubble_data %&gt;% filter(radius &gt; 30), \n             aes(x = x, y = y, label = Count, size = radius / 10), \n             fontface = \"bold\", fill = \"white\", alpha = 0.8,  \n             label.size = 0, hjust = 0.5, vjust = 0.5) +\n\n  # **Move COUNT labels outside small bubbles**\n  geom_label_repel(data = bubble_data %&gt;% filter(radius &lt;= 30), \n                   aes(x = x, y = y + radius * 3, label = Count, size = 5), \n                   fontface = \"bold\", fill = \"white\", color = \"black\",\n                   box.padding = 2, segment.color = \"black\", \n                   nudge_y = 20, direction = \"both\", force = 150) +\n\n  # **Move CATEGORY labels outside bubbles & allow multiple line text**\n  geom_label_repel(data = bubble_data, \n                   aes(x = x + Nudge_X_Category, y = y + Nudge_Y_Category, label = Category),  \n                   size = 4, color = \"black\", fontface = \"italic\",\n                   box.padding = 2, segment.color = \"black\", segment.size = 0.8,\n                   direction = \"both\", force = 400, label.size = 0.3, \n                   fill = \"white\", alpha = 0.9) +  \n\n  # **Move percentage labels inside bubbles**\n  geom_text(data = bubble_data, \n            aes(x = x, y = y + radius * 0.3, label = Percentage), \n            size = 3, color = \"black\", hjust = 0.5, vjust = 0.5) +\n\n  # Assign colors to the bubbles\n  scale_fill_manual(values = category_colors) +\n\n  # Remove default ggplot background\n  theme_void() +\n\n  # Adjust text styling\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n\n  # Set the title\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\nWarning: ggrepel: 3 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Create bubble chart without manual position adjustments\nggplot(bubble_data, aes(x = Count, y = Count, size = Count, fill = Category)) +\n  geom_point(alpha = 0.8, shape = 21, color = \"black\") +  # Use points instead of polygons\n  scale_size(range = c(5, 25)) +  # Adjust bubble scaling dynamically\n  scale_fill_viridis_d() +  # Dynamic color scaling for categories\n  geom_text_repel(aes(label = Count), size = 5, fontface = \"bold\", color = \"black\") +  # Automatic text positioning\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\n\n\n\n\n\n\n\n~ Fin"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#processing-of-the-data",
    "href": "presentation-exercise/presentation-exercise.html#processing-of-the-data",
    "title": "Presentation-exercise",
    "section": "",
    "text": "First I will load all of the required packages and well as read the raw data CSV file:\n\n#install.packages(\"ggforce\")\n#install.packages(\"packcircles\")\n#install.packages(\"ggrepel\")\n#install.packages(\"viridis\")\n#install.packages(\"kableExtra\")\n\n\nlibrary(\"ggplot2\")\nlibrary(\"tidyverse\")\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"knitr\")\nlibrary(\"dplyr\")\nlibrary(\"readr\")\nlibrary(\"ggforce\")\n\nWarning: package 'ggforce' was built under R version 4.4.2\n\nlibrary(\"packcircles\")\n\nWarning: package 'packcircles' was built under R version 4.4.2\n\nlibrary(\"ggrepel\")\n\nWarning: package 'ggrepel' was built under R version 4.4.2\n\nlibrary(\"viridis\")\n\nWarning: package 'viridis' was built under R version 4.4.2\n\n\nLoading required package: viridisLite\n\nlibrary(\"kableExtra\")\n\nWarning: package 'kableExtra' was built under R version 4.4.2\n\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nbachelorette_data &lt;- read_csv(here::here(\n  \"presentation-exercise/bachelorette.csv\"))\n\nRows: 920 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): SHOW, SEASON, CONTESTANT, ELIMINATION-1, ELIMINATION-2, ELIMINATIO...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary(bachelorette_data)\n\n     SHOW              SEASON           CONTESTANT        ELIMINATION-1     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-2      ELIMINATION-3      ELIMINATION-4      ELIMINATION-5     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-6      ELIMINATION-7      ELIMINATION-8      ELIMINATION-9     \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n ELIMINATION-10       DATES-1            DATES-2            DATES-3         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-4            DATES-5            DATES-6            DATES-7         \n Length:920         Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   DATES-8            DATES-9            DATES-10        \n Length:920         Length:920         Length:920        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n\n\nPerform some formating of the data to remove the first row which is a repetition of headings and to change the seasons to numeric values. We are also removing rows where headings are repeated within the data:\n\n# Remove the incorrect header row if necessary\nbachelorette_data &lt;- bachelorette_data[-1, ]  # Remove first row if it's incorrect\n\n# Convert SEASON to numeric\nbachelorette_data$SEASON &lt;- as.numeric(bachelorette_data$SEASON)\n\nWarning: NAs introduced by coercion\n\n# Remove rows where the CONTESTANT column contains \"ID\"\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  filter(CONTESTANT != \"ID\")\n\n# Reset row indices (optional)\nbachelorette_data &lt;- bachelorette_data %&gt;% mutate(row_id = row_number())\n\n# Display the cleaned data\nprint(head(bachelorette_data))\n\n# A tibble: 6 × 24\n  SHOW         SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n1 Bachelorette     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n2 Bachelorette     13 13_PETER_K &lt;NA&gt;            R               &lt;NA&gt;           \n3 Bachelorette     13 13_ERIC_B  &lt;NA&gt;            &lt;NA&gt;            R              \n4 Bachelorette     13 13_DEAN_U  &lt;NA&gt;            R               &lt;NA&gt;           \n5 Bachelorette     13 13_ADAM_G  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n6 Bachelorette     13 13_MATT_M  &lt;NA&gt;            &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 18 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;, `DATES-1` &lt;chr&gt;,\n#   `DATES-2` &lt;chr&gt;, `DATES-3` &lt;chr&gt;, `DATES-4` &lt;chr&gt;, `DATES-5` &lt;chr&gt;,\n#   `DATES-6` &lt;chr&gt;, `DATES-7` &lt;chr&gt;, `DATES-8` &lt;chr&gt;, `DATES-9` &lt;chr&gt;,\n#   `DATES-10` &lt;chr&gt;, row_id &lt;int&gt;\n\n\nSome information was provided on the website with the raw data which may help with interpretation of this data:\n\n#Adding additional data into a table\ncolumn_names &lt;- c(\"Header\", \"Description\")\ntable_data &lt;- data.frame(\n  Header = c(\n    \"SHOW\", \"SEASON\", \"CONTESTANT\", \"ELIMINATION-1\", \"ELIMINATION-2\",\n    \"ELIMINATION-3\", \"ELIMINATION-4\", \"ELIMINATION-5\", \"ELIMINATION-6\",\n    \"ELIMINATION-7\", \"ELIMINATION-8\", \"ELIMINATION-9\", \"ELIMINATION-10\",\n    \"DATES-1\", \"DATES-2\", \"DATES-3\", \"DATES-4\", \"DATES-5\", \"DATES-6\",\n    \"DATES-7\", \"DATES-8\", \"DATES-9\", \"DATES-10\"\n  ),\n  Description = c(\n    \"Bachelor or Bachelorette\", \"Which season\", \"An identifier for the contestant in a given season\",\n    \"Who was eliminated in week 1\", \"Who was eliminated in week 2\", \"Who was eliminated in week 3\",\n    \"Who was eliminated in week 4\", \"Who was eliminated in week 5\", \"Who was eliminated in week 6\",\n    \"Who was eliminated in week 7\", \"Who was eliminated in week 8\", \"Who was eliminated in week 9\",\n    \"Who was eliminated in week 10\", \"Who was on which date in week 1\", \"Who was on which date in week 2\",\n    \"Who was on which date in week 3\", \"Who was on which date in week 4\", \"Who was on which date in week 5\",\n    \"Who was on which date in week 6\", \"Who was on which date in week 7\", \"Who was on which date in week 8\",\n    \"Who was on which date in week 9\", \"Who was on which date in week 10\"\n  )\n)\n\n# Print table using kable for better formatting\nknitr::kable(table_data, col.names = column_names, caption = \"Table of Data Headers and Descriptions\")\n\n\nTable of Data Headers and Descriptions\n\n\nHeader\nDescription\n\n\n\n\nSHOW\nBachelor or Bachelorette\n\n\nSEASON\nWhich season\n\n\nCONTESTANT\nAn identifier for the contestant in a given season\n\n\nELIMINATION-1\nWho was eliminated in week 1\n\n\nELIMINATION-2\nWho was eliminated in week 2\n\n\nELIMINATION-3\nWho was eliminated in week 3\n\n\nELIMINATION-4\nWho was eliminated in week 4\n\n\nELIMINATION-5\nWho was eliminated in week 5\n\n\nELIMINATION-6\nWho was eliminated in week 6\n\n\nELIMINATION-7\nWho was eliminated in week 7\n\n\nELIMINATION-8\nWho was eliminated in week 8\n\n\nELIMINATION-9\nWho was eliminated in week 9\n\n\nELIMINATION-10\nWho was eliminated in week 10\n\n\nDATES-1\nWho was on which date in week 1\n\n\nDATES-2\nWho was on which date in week 2\n\n\nDATES-3\nWho was on which date in week 3\n\n\nDATES-4\nWho was on which date in week 4\n\n\nDATES-5\nWho was on which date in week 5\n\n\nDATES-6\nWho was on which date in week 6\n\n\nDATES-7\nWho was on which date in week 7\n\n\nDATES-8\nWho was on which date in week 8\n\n\nDATES-9\nWho was on which date in week 9\n\n\nDATES-10\nWho was on which date in week 10\n\n\n\n\n\nFurther information required for interpretation and processing:\nEliminates connote either an elimination (starts with “E”) or a rose (starts with “R”). Eliminations supercede roses. “E” connotes a standard elimination, typically at a rose ceremony. “EQ” means the contestant quits. “EF” means the contestant was fired by production. “ED” connotes a date elimination. “EU” connotes an unscheduled elimination, one that takes place at a time outside of a date or rose ceremony. “R” means the contestant received a rose. “R1” means the contestant got a first impression rose. “D1” means a one-on-one date, “D2” means a 2-on-1, “D3” means a 3-on-1 group date, and so on. “W” in E10 indicates that the contestant won. Weeks of the show are deliminated by rose ceremonies, and may not line up exactly with episodes.\n………………….\nI will be using ChatGPT to help me to generate the code that will help to obtain the graph that I want. I will post my prompts and the outcome as well as the different iterations below.\nWe are not getting the correct number of winners\n\n# Get all ELIMINATION columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Count occurrences of \"W\" in each ELIMINATION column\nw_counts &lt;- sapply(elim_columns, function(col) sum(bachelorette_data[[col]] == \"W\", na.rm = TRUE))\n\n# Print counts for each column\nprint(\"Number of 'W' occurrences per ELIMINATION column:\")\n\n[1] \"Number of 'W' occurrences per ELIMINATION column:\"\n\nprint(w_counts)\n\n ELIMINATION-1  ELIMINATION-2  ELIMINATION-3  ELIMINATION-4  ELIMINATION-5 \n             0              0              0              0              0 \n ELIMINATION-6  ELIMINATION-7  ELIMINATION-8  ELIMINATION-9 ELIMINATION-10 \n             2              8              7              1             15 \n\n# Total winners found in dataset\ntotal_winners &lt;- sum(w_counts)\nprint(paste(\"Total expected winners:\", total_winners))\n\n[1] \"Total expected winners: 33\"\n\n\nNow we found that it seems that winners were marked in different levels of the competition (Elimination 6 - 10 and not just the last two). We will run this again:\n\n# Define function to classify contestants, now checking ELIMINATION-6 to ELIMINATION-10\nclassify_contestant &lt;- function(row) {\n  elim_columns &lt;- grep(\"ELIMINATION-\", names(row), value = TRUE)\n\n  # **Ensure all ELIMINATION columns are checked for \"W\"**\n  w_found &lt;- any(sapply(elim_columns, function(col) toupper(trimws(row[[col]])) == \"W\"), na.rm = TRUE)\n\n  # **First, Check for Winners** - If \"W\" appears in ELIMINATION-6 through ELIMINATION-10, classify as \"Won\"\n  if (w_found) {\n    return(\"Won\")\n  }\n\n  # **Next, Track First Elimination Event**\n  for (col in elim_columns) {\n    value &lt;- ifelse(is.na(row[[col]]), \"\", toupper(trimws(as.character(row[[col]]))))  # Handle NA values\n\n    if (grepl(\"^E\", value)) {  # Any elimination starting with \"E\"\n      if (value == \"E\" && col == \"ELIMINATION-1\") {\n        return(\"Eliminated in week 1 rose ceremony\")\n      } else if (value == \"E\") {\n        return(\"Eliminated in weeks 2+ rose ceremony\")\n      } else if (value == \"EQ\") {\n        return(\"Quit\")\n      } else if (value == \"EF\") {\n        return(\"Fired by production\")\n      } else if (value == \"ED\") {\n        return(\"Eliminated on a date\")\n      } else if (value == \"EU\") {\n        return(\"Eliminated at an unscheduled time\")\n      }\n    }\n  }\n\n  return(NA)  # Avoid misclassification\n}\n\n# Apply classification to dataset using `pick()`\nbachelorette_data &lt;- bachelorette_data %&gt;%\n  rowwise() %&gt;%\n  mutate(Category = classify_contestant(pick(everything()))) %&gt;%\n  ungroup()\n\n# Remove NA rows (contestants who do not fit a category)\nbachelorette_data &lt;- bachelorette_data %&gt;% filter(!is.na(Category))\n\n# Count occurrences of each category\ncategory_counts &lt;- bachelorette_data %&gt;%\n  group_by(Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  arrange(desc(Count))\n\n# Print corrected category counts\nprint(category_counts)\n\n# A tibble: 7 × 2\n  Category                             Count\n  &lt;chr&gt;                                &lt;int&gt;\n1 Eliminated in weeks 2+ rose ceremony   425\n2 Eliminated in week 1 rose ceremony     298\n3 Eliminated on a date                    75\n4 Won                                     33\n5 Quit                                    28\n6 Eliminated at an unscheduled time       21\n7 Fired by production                      6\n\n# Display the updated category breakdown table using kable\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\"), caption = \"Final Category Breakdown\")\n\n\nFinal Category Breakdown\n\n\nCategory\nCount\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n\n\nEliminated in week 1 rose ceremony\n298\n\n\nEliminated on a date\n75\n\n\nWon\n33\n\n\nQuit\n28\n\n\nEliminated at an unscheduled time\n21\n\n\nFired by production\n6\n\n\n\n\n\nWe have picked up an additonal winner than was in the original dataset (33 instead of 32), we will use this code to verify who they were so we can be confident with our analysis:\n\n# Create a separate copy of the dataset (optional safety step)\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns: SHOW, SEASON, CONTESTANT, and elimination columns where \"W\" appeared\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Print the identified winners\nprint(winners)\n\n# A tibble: 33 × 13\n   SHOW        SEASON CONTESTANT `ELIMINATION-1` `ELIMINATION-2` `ELIMINATION-3`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;          \n 1 Bacheloret…     13 13_BRYAN_A R1              &lt;NA&gt;            &lt;NA&gt;           \n 2 Bacheloret…     12 12_JORDAN… R1              &lt;NA&gt;            &lt;NA&gt;           \n 3 Bacheloret…     11 11_SHAWN_B R1              &lt;NA&gt;            R              \n 4 Bacheloret…     10 10_JOSH_M  &lt;NA&gt;            &lt;NA&gt;            R              \n 5 Bacheloret…      9 09_CHRIS_S &lt;NA&gt;            &lt;NA&gt;            R              \n 6 Bacheloret…      8 08_JEF_H   &lt;NA&gt;            R               &lt;NA&gt;           \n 7 Bacheloret…      7 07_JP_R    &lt;NA&gt;            &lt;NA&gt;            R              \n 8 Bacheloret…      6 06_ROBERT… R1              &lt;NA&gt;            R              \n 9 Bacheloret…      5 05_ED_S    &lt;NA&gt;            &lt;NA&gt;            R              \n10 Bacheloret…      4 04_JESSE_C R1              &lt;NA&gt;            &lt;NA&gt;           \n# ℹ 23 more rows\n# ℹ 7 more variables: `ELIMINATION-4` &lt;chr&gt;, `ELIMINATION-5` &lt;chr&gt;,\n#   `ELIMINATION-6` &lt;chr&gt;, `ELIMINATION-7` &lt;chr&gt;, `ELIMINATION-8` &lt;chr&gt;,\n#   `ELIMINATION-9` &lt;chr&gt;, `ELIMINATION-10` &lt;chr&gt;\n\n\nWe have higher numbers in the other categories too, but we will assume that this is because the raw data has an additional season which was not included in the published graphs.\n\n# Count the total number of unique seasons for each show (Bachelor/Bachelorette)\nseason_counts &lt;- bachelorette_data %&gt;%\n  group_by(SHOW) %&gt;%\n  summarise(Total_Seasons = n_distinct(SEASON)) %&gt;%\n  arrange(desc(Total_Seasons))  # Sort by number of seasons\n\n# Print the season counts\nprint(season_counts)\n\n# A tibble: 2 × 2\n  SHOW         Total_Seasons\n  &lt;chr&gt;                &lt;int&gt;\n1 Bachelor                21\n2 Bachelorette            13\n\n# Display the table using kable\nknitr::kable(season_counts, col.names = c(\"Show\", \"Total Seasons\"),\n             caption = \"Total Number of Seasons by Show\")\n\n\nTotal Number of Seasons by Show\n\n\nShow\nTotal Seasons\n\n\n\n\nBachelor\n21\n\n\nBachelorette\n13\n\n\n\n\n\nTheir data indicated: Eliminated in weeks 2+ rose ceremony 411 Eliminated in week 1 rose ceremony 291 Won 32 Eliminated on a date 68 Eliminated at an unscheduled time 20 Quit 28 Fired by production 6\nOur results are: Eliminated in weeks 2+ rose ceremony 425\nEliminated in week 1 rose ceremony 298\nWon 33\nEliminated on a date 75\nEliminated at an unscheduled time 21\nQuit 28\nFired by production 6\nWe want to get the percentage for each of the categories so that we can include these in the graphic.\n\n# Calculate total number of contestants classified\ntotal_contestants &lt;- sum(category_counts$Count)\n\n# Add percentage column, rounded to the nearest whole number\n# Ensure category_counts has correctly formatted columns\ncategory_counts &lt;- category_counts %&gt;%\n  rename(Count = Count) %&gt;%  # Ensure correct naming\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Add percentage column\n\n\n#category_counts &lt;- category_counts %&gt;%\n # mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))  # Calculate and format as \"X%\"\n\n\n# Print updated table\nprint(category_counts)\n\n# A tibble: 7 × 3\n  Category                             Count Percentage\n  &lt;chr&gt;                                &lt;int&gt; &lt;chr&gt;     \n1 Eliminated in weeks 2+ rose ceremony   425 48%       \n2 Eliminated in week 1 rose ceremony     298 34%       \n3 Eliminated on a date                    75 8%        \n4 Won                                     33 4%        \n5 Quit                                    28 3%        \n6 Eliminated at an unscheduled time       21 2%        \n7 Fired by production                      6 1%        \n\n# Display table using kable with percentages\nknitr::kable(category_counts, col.names = c(\"Category\", \"Count\", \"Percentage (%)\"),\n             caption = \"Final Category Breakdown with Rounded Percentages\")\n\n\nFinal Category Breakdown with Rounded Percentages\n\n\nCategory\nCount\nPercentage (%)\n\n\n\n\nEliminated in weeks 2+ rose ceremony\n425\n48%\n\n\nEliminated in week 1 rose ceremony\n298\n34%\n\n\nEliminated on a date\n75\n8%\n\n\nWon\n33\n4%\n\n\nQuit\n28\n3%\n\n\nEliminated at an unscheduled time\n21\n2%\n\n\nFired by production\n6\n1%\n\n\n\n\n\n\n# Extract rose-related events (e.g., R1, R, D1, etc.) from elimination columns\nrose_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check if a contestant received a rose in each episode\nget_rose_status &lt;- function(row) {\n  rose_events &lt;- row[rose_columns]\n  return(ifelse(any(grepl(\"R\", rose_events)), which(grepl(\"R\", rose_events))[1], NA))\n}\n\n# Apply the function to get the first episode where each contestant received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_status)\n\n\n# Identify eliminations\neliminate_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to check elimination status (either eliminated or not)\nget_elimination_status &lt;- function(row) {\n  elimination_status &lt;- row[eliminate_columns]\n  return(sum(grepl(\"E\", elimination_status), na.rm = TRUE))\n}\n\n# Calculate the cumulative number of eliminations by episode\nbachelorette_data$Eliminations &lt;- apply(bachelorette_data, 1, get_elimination_status)\n\n# Calculate total contestants\ntotal_contestants &lt;- nrow(bachelorette_data)\n\n# Calculate cumulative eliminations\ncumulative_elim &lt;- cumsum(bachelorette_data$Eliminations)\n\n# Calculate cumulative percent chance of elimination\nbachelorette_data$Cumulative_Elim_Percent &lt;- cumulative_elim / total_contestants * 100\n\n\n\nHere are the two original images that i tried to recreate, with the original graph looking at the effect of receiving a rose early on being eliminated:\n\n\n\nGraph evaluating the effect of wiining roses early on and elimination.\n\n\nThe second graph (image) - see at the bottom of the document:\n\n\n\nThe breakdown of different contestants on the bachelor and bachelorette\n\n\nThis is how we (ChatGPT and me) started with the recreation of this graph:\nFor my prompt I uploaded the image of the graph and asked it to analyse it. I then uploaded the raw dataset and asked if it could recreate the graph. It seemed to trace a similar graph but I think it might have just copied the image a bit. So I asked it to provide the R code and to provide steps to recreate it. I then received the first graph provided below which was not similar at all (as you can see).\nI then explained what the variables on each axis were and I explained that there are two curves and what they are based upon. I then explained what we might be calculating and that it should include any data analysis steps that may be required. The results thereafter were much better and closer to the original and i weaked some of the smaller labels and things like that.\nIt was quite good at rendering a similar graph however our shape never quite reenacted the original data. Unfortunately, I am not sure how they performed this for clear comparison but the end result was somewhat similar. Given the issues that I picked uo with the data processing part I am not sure where the issue might be coming from.\n\nlibrary(ggplot2)\n\n# Filter the data for contestants who received a rose and those who did not\nrose_data &lt;- bachelorette_data[!is.na(bachelorette_data$Rose_Episode), ]\nno_rose_data &lt;- bachelorette_data[is.na(bachelorette_data$Rose_Episode), ]\n\n# Create the plot\nggplot() +\n  geom_line(data = rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Gets a date rose\"), size = 1) +\n  geom_line(data = no_rose_data, aes(x = Rose_Episode, y = Cumulative_Elim_Percent, color = \"Doesn't get a date rose\"), size = 1) +\n  labs(title = \"The Date Rose Advantage\\nCumulative Chance of Being Eliminated\",\n       x = \"Episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 674 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWhere we ended up after many back and forths and starting again to focus on the basic principles of the curve:\n\n# Identify the columns for eliminations\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_data), value = TRUE)\n\n# Function to identify when a contestant received a rose\nget_rose_episode &lt;- function(row) {\n  for (i in seq_along(row[elim_columns])) {\n    if (grepl(\"R\", row[elim_columns[i]])) {\n      return(i)  # Episode number starts from 1\n    }\n  }\n  return(NA)  # If no rose received, return NA\n}\n\n# Apply the function to the dataset to create a new column with the episode they received a rose\nbachelorette_data$Rose_Episode &lt;- apply(bachelorette_data, 1, get_rose_episode)\n\n# Initialize empty lists to track eliminations for both groups (rose vs no-rose)\nelim_counts_rose &lt;- numeric(0)\nelim_counts_no_rose &lt;- numeric(0)\nweeks_after_rose_rose &lt;- numeric(0)\nweeks_after_rose_no_rose &lt;- numeric(0)\n\n# Variables to store total eliminations for both groups\nelim_rose &lt;- 0\nelim_no_rose &lt;- 0\n\n# Track eliminations and weeks after receiving the rose\nfor (i in 1:length(elim_columns)) {\n  \n  # For eliminations in the current episode\n  eliminated_in_episode &lt;- grepl(\"E\", bachelorette_data[[elim_columns[i]]])\n  \n  # For contestants who received a rose\n  eliminated_rose_group &lt;- eliminated_in_episode & !is.na(bachelorette_data$Rose_Episode)\n  elim_rose &lt;- elim_rose + sum(eliminated_rose_group)\n  elim_counts_rose &lt;- c(elim_counts_rose, elim_rose / sum(!is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from rose group\n  weeks_after_rose_rose &lt;- c(weeks_after_rose_rose, i)  # Track weeks after the rose\n  \n  # For contestants who did not receive a rose\n  eliminated_no_rose_group &lt;- eliminated_in_episode & is.na(bachelorette_data$Rose_Episode)\n  elim_no_rose &lt;- elim_no_rose + sum(eliminated_no_rose_group)\n  elim_counts_no_rose &lt;- c(elim_counts_no_rose, elim_no_rose / sum(is.na(bachelorette_data$Rose_Episode)) * 100)  # Percent eliminated from no-rose group\n  weeks_after_rose_no_rose &lt;- c(weeks_after_rose_no_rose, i)  # Track weeks after the rose\n\n}\n\n# Create a combined dataset for plotting\nelim_data &lt;- data.frame(\n  Weeks_After_Rose = c(weeks_after_rose_rose, weeks_after_rose_no_rose),\n  Cumulative_Elim_Percent = c(elim_counts_rose, elim_counts_no_rose),\n  Group = rep(c(\"Gets a date rose\", \"Doesn't get a date rose\"), each = length(elim_columns))\n)\n\n# Plot the graph\nplot &lt;- ggplot(elim_data, aes(x = Weeks_After_Rose, y = Cumulative_Elim_Percent, color = Group)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = c(\"Gets a date rose\" = \"pink\", \"Doesn't get a date rose\" = \"black\")) +\n  labs(title = \"The Date Rose Advantage\",\n       x = \"Weeks after episode\", y = \"Cumulative Chance of Being Eliminated (%)\") +\n  scale_x_continuous(breaks = seq(0, length(elim_columns), by = 1), labels = paste0(\"+\", seq(0, length(elim_columns), by = 1))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", \n        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n        axis.text.y = element_text(size = 12),\n        axis.title.x = element_text(size = 14),\n        axis.title.y = element_text(size = 14)) +\n  ylim(0, 100)  # Ensure y-axis is from 0 to 100%\n\n# Add the shorter measurement line between the two curves (shifted to the left)\nplot + \n  geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", linetype = \"dashed\", size = 1) +  # Line from x = 2 to x = 6\n  \n  # Add label for \"Rose advantage\"\n  annotate(\"text\", x = 4, y = 55, label = \"Rose advantage\", size = 6, angle = 0, hjust = 0.5)\n\nWarning in geom_segment(aes(x = 2, xend = 6, y = 50, yend = 50), color = \"grey\", : All aesthetics have length 1, but the data has 20 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nI do not know if the differences come from the original data or the approach in data anaylsis or the graph rendering."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#part-2-creating-a-table",
    "href": "presentation-exercise/presentation-exercise.html#part-2-creating-a-table",
    "title": "Presentation-exercise",
    "section": "",
    "text": "There was no table to recreate so I decided to create one that looked at and ranked the number of roses that each winner received in their respective season.\nI tried to fix my table to have the first two rows with bigger font and to have different colours but I could not get it right. ChatGPT could not help me and kept on giving me the same responses and I gave up with my editing. I will pick it up again sometime.\n\n# Create a separate copy of the dataset\nbachelorette_winner_check &lt;- bachelorette_data\n\n# Identify all elimination columns\nelim_columns &lt;- grep(\"ELIMINATION-\", names(bachelorette_winner_check), value = TRUE)\n\n# Find contestants who have \"W\" in any elimination column (winners)\nwinners &lt;- bachelorette_winner_check %&gt;%\n  filter(if_any(all_of(elim_columns), ~ . == \"W\"))\n\n# Select relevant columns\nwinner_columns &lt;- c(\"SHOW\", \"SEASON\", \"CONTESTANT\", elim_columns)\nwinners &lt;- winners %&gt;% select(all_of(winner_columns))\n\n# Identify the date columns (for checking the dates roses were given)\ndate_columns &lt;- grep(\"DATES-\", names(bachelorette_winner_check), value = TRUE)\n\n# Function to count the number of roses each winner received\ncount_roses &lt;- function(winner_row, date_columns, elim_columns) {\n  contestant &lt;- as.character(winner_row$CONTESTANT)\n  \n  date_roses &lt;- sum(sapply(date_columns, function(col) {\n    sum(grepl(contestant, as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  elim_roses &lt;- sum(sapply(elim_columns, function(col) {\n    sum(grepl(\"R\", as.character(winner_row[[col]])), na.rm = TRUE)\n  }))\n  \n  return(date_roses + elim_roses)  \n}\n\n# Create a data frame to store the winner and their respective rose counts\nrose_counts &lt;- data.frame(Winner = character(), Season = numeric(), Show_Type = character(), \n                          Roses_Received = numeric(), Roses = character(), stringsAsFactors = FALSE)\n\n# Loop over each winner and count the number of roses they received in their season\nfor (i in 1:nrow(winners)) {\n  winner_row &lt;- winners[i, ]\n  rose_count &lt;- count_roses(winner_row, date_columns, elim_columns)\n  winner_season &lt;- as.character(winner_row$SEASON)\n  winner_show &lt;- as.character(winner_row$SHOW)\n  \n  cleaned_winner_name &lt;- gsub(\"^\\\\d+_\", \"\", winner_row$CONTESTANT)\n  rose_symbol &lt;- paste(rep(\"🌹\", rose_count), collapse = \"\")  # Restored rose emoji 🌹\n  \n  rose_counts &lt;- rbind(rose_counts, data.frame(Winner = cleaned_winner_name, \n                                               Season = winner_season, \n                                               Show_Type = winner_show, \n                                               Roses_Received = rose_count, \n                                               Roses = rose_symbol, \n                                               stringsAsFactors = FALSE))\n}\n\n# Sort the table from most roses received to least\nrose_counts &lt;- rose_counts %&gt;% arrange(desc(Roses_Received))\n\n# Explanation text row\nexplanation_text &lt;- \"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette.\"\n\n# Display the table with improved formatting\nrose_counts %&gt;%\n  kable(col.names = c(\"Winner\", \"Season\", \"Show Type\", \"Roses Received\", \"Roses\"),\n        align = c(\"l\", \"c\", \"c\", \"c\", \"c\"), escape = FALSE) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", \n                bootstrap_options = c(\"striped\", \"bordered\", \"hover\")) %&gt;%\n  \n  \n  \n  # Explanation row (light red background, no gridlines)\n  add_header_above(c(\"A breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\" = 5)) %&gt;%\n  row_spec(1, background = \"#FFCCCC\", italic = TRUE, hline_after = FALSE, extra_css = \"border-bottom: none;\") %&gt;%\n  \n  # Main title row: \"Roses to the Winners\" (large font)\n  add_header_above(c(\"Roses to the Winners\" = 5)) %&gt;%\n  row_spec(0, bold = TRUE, font_size = 28) %&gt;%  # Increased font size for title\n  \n  \n  # Subheading row (Winner, Season, etc.) - Pink background\n  row_spec(2, background = \"#FFD9D9\", bold = TRUE) %&gt;%\n  \n  # Adjust column spacing for better readability\n  column_spec(2, width = \"8em\") %&gt;%\n  column_spec(3, width = \"12em\") %&gt;%\n  column_spec(4, width = \"10em\") %&gt;%\n  column_spec(5, width = \"15em\") %&gt;%\n  \n  # Footer with source information\n  footnote(general = \"Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\")\n\n\n\n\n\n\n\n\n\n\n\n\nRoses to the Winners\n\n\n\n\nA breakdown of the roses that were awarded to the winner from each of the 33 seasons of The Bachelor and Bachelorette\n\n\n\nWinner\nSeason\nShow Type\nRoses Received\nRoses\n\n\n\n\nBRYAN_A\n13\nBachelorette\n4\n🌹🌹🌹🌹 |\n\n\nJORDAN_R\n12\nBachelorette\n3\n🌹🌹🌹 |\n\n\nSHAWN_B\n11\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJEF_H\n8\nBachelorette\n3\n🌹🌹🌹 |\n\n\nJP_R\n7\nBachelorette\n3\n🌹🌹🌹 |\n\n\nNIKKI_F\n18\nBachelor\n3\n🌹🌹🌹 |\n\n\nEMILY_M\n15\nBachelor\n3\n🌹🌹🌹 |\n\n\nJOSH_M\n10\nBachelorette\n2\n🌹🌹 |\n\n\nCHRIS_S\n9\nBachelorette\n2\n🌹🌹 |\n\n\nROBERT_M\n6\nBachelorette\n2\n🌹🌹 |\n\n\nJESSE_C\n4\nBachelorette\n2\n🌹🌹 |\n\n\nVANESSA_G\n21\nBachelor\n2\n🌹🌹 |\n\n\nLAUREN_B\n20\nBachelor\n2\n🌹🌹 |\n\n\nWHITNEY_B\n19\nBachelor\n2\n🌹🌹 |\n\n\nCATHERINE_G\n17\nBachelor\n2\n🌹🌹 |\n\n\nCOURTNEY_R\n16\nBachelor\n2\n🌹🌹 |\n\n\nJENNIFER_W\n9\nBachelor\n2\n🌹🌹 |\n\n\nED_S\n5\nBachelorette\n1\n🌹 |\n\n\nVIENNA_G\n14\nBachelor\n1\n🌹 |\n\n\nMELISSA_R\n13\nBachelor\n1\n🌹 |\n\n\nSHAYNE_L\n12\nBachelor\n1\n🌹 |\n\n\nTESSA_H\n10\nBachelor\n1\n🌹 |\n\n\nSarah_S\n8\nBachelor\n1\n🌹 |\n\n\nSarah B.\n7\nBachelor\n1\n🌹 |\n\n\nJERRY_F\n3\nBachelorette\n0\n\n\n\nIAN_M\n2\nBachelorette\n0\n\n\n\nRYAN_S\n1\nBachelorette\n0\n\n\n\nMary\n6\nBachelor\n0\n\n\n\nJESSICA_B\n5\nBachelor\n0\n\n\n\nEstella\n4\nBachelor\n0\n\n\n\nJEN_X\n3\nBachelor\n0\n\n\n\nHELENE_E\n2\nBachelor\n0\n\n\n\nAMANDA_M\n1\nBachelor\n0\n\n\n\n\nNote: \n\n\n\n\n\n\n Original data retrieved from FiveThirtyEight. Original Article by: Ella Koeze and Walt Hickey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI spent an embarrasingly long time trying to recreate this image that was in the article. I spent two days going back and forth with ChatGPT and trying to get it to format the graphic to recreate the original. I even tried with DeepSeek.\nI think I used over 3000 lines of R script with my back and forths and so I am not sharing my directions to R. But I even uploaded the image multiple times and asked it to expalin the image and how everything fitted together and I corrected each element. However, no matter how much promise it showed, even with starting multiple new chats, I was never able to fully correct the image to the correct order (challenge for someone else?). Every time I tried to fix one thing, it would break everything else.\nAfter spending all of the time (I am not exagerrating when I say two days) on this I eventually realised that the authors probably used an imaging software for this since it is more of an image than a graph……..🤕😭\nNonetheless, here are two different attempts. Please feel free to try your hand.\nThis was my best attempt:\n\n# Define dataset dynamically\ncategory_counts &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Eliminated on a date\",\n    \"Won\",\n    \"Quit\",\n    \"Eliminated at an unscheduled time\",\n    \"Fired by production\"\n  ),\n  Count = c(425, 298, 75, 33, 28, 21, 6)  # Updated counts from the provided data\n)\n\n# Calculate percentages dynamically\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Compute the circle packing layout\npacking &lt;- circleProgressiveLayout(category_counts$Count, sizetype = \"area\")\ncategory_counts$x &lt;- packing$x\ncategory_counts$y &lt;- packing$y\ncategory_counts$r &lt;- packing$radius\n\n# Generate circle layout vertices for plotting\ncircle_data &lt;- circleLayoutVertices(packing, npoints = 100)\n\n# Define colors: Pink for all except \"Won\", which is red\ncolors &lt;- c(\"#FADBD8\", \"#FADBD8\", \"#FADBD8\", \"#E74C3C\", \"#FADBD8\", \"#FADBD8\", \"#FADBD8\")\n\n# Create the bubble chart\nggplot() +\n  # Draw proportional circles with specified colors\n  geom_polygon(data = circle_data, aes(x, y, group = id, fill = as.factor(id)), \n               color = \"black\", size = 0.4, alpha = 0.8) +\n  \n  # Add contestant count inside bubbles\n  geom_text(data = category_counts, aes(x = x, y = y, label = Count), \n            size = 5, fontface = \"bold\", color = \"black\") +\n  \n  # Add category names with percentages using text repulsion\n  geom_text_repel(data = category_counts, \n                  aes(x = x, y = y, label = paste0(Category, \"\\n\", Percentage)), \n                  size = 4, color = \"black\", \n                  nudge_y = 0.5, nudge_x = 0.5, \n                  box.padding = 0.4, segment.color = \"black\") +\n  \n  # Clean up the theme\n  theme_void() +\n  theme(legend.position = \"none\") +\n  \n  # Apply the pink color for all except \"Won\" (which is red)\n  scale_fill_manual(values = colors)\n\n\n\n\n\n\n\n\nFor comedy relief I will share other attempts:\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Scale bubble size\nbubble_data$BubbleSize &lt;- sqrt(bubble_data$Count) * 12  \n\n# Generate circle layout\nset.seed(42)  \npacking &lt;- circleProgressiveLayout(bubble_data$BubbleSize, sizetype = \"radius\")\nbubble_data$x &lt;- packing$x\nbubble_data$y &lt;- packing$y\nbubble_data$radius &lt;- packing$radius\n\n# Generate circle layout vertices\ncircle_vertices &lt;- circleLayoutVertices(packing, npoints = 50)\n\n# Define colors for the bubbles\ncategory_colors &lt;- c(\n  \"Eliminated in weeks 2+ rose ceremony\" = \"#fbb4ae\",\n  \"Eliminated in week 1 rose ceremony\" = \"#fbb4ae\",\n  \"Fired by production\" = \"#fbb4ae\",\n  \"Quit\" = \"#fbb4ae\",\n  \"Eliminated on a date\" = \"#fbb4ae\",\n  \"Eliminated at an unscheduled time\" = \"#fbb4ae\",\n  \"Won\" = \"#e31a1c\"\n)\n\n# ===== MANUALLY ADJUST BUBBLE POSITIONS =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    x = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -1.6,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 1.2,  \n      Category == \"Eliminated on a date\" ~ 0.2,  \n      Category == \"Quit\" ~ 1.8,  \n      Category == \"Fired by production\" ~ 2.0,  \n      Category == \"Won\" ~ -0.8,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.5,  \n      TRUE ~ 0\n    ),\n    y = case_when(\n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2,  \n      Category == \"Eliminated in week 1 rose ceremony\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.8,  \n      Category == \"Quit\" ~ -0.2,  \n      Category == \"Fired by production\" ~ 1.3,  \n      Category == \"Won\" ~ -0.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -0.9,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== ENSURING TEXT MOVES WITH BUBBLES =====\nbubble_data &lt;- bubble_data %&gt;%\n  mutate(\n    # Count label inside bubbles\n    Nudge_X = x,\n    Nudge_Y = y,\n\n    # Category text label positioning outside bubbles\n    Nudge_X_Category = case_when(\n      Category == \"Won\" ~ -1.2,  \n      Category == \"Quit\" ~ 2.2,  \n      Category == \"Eliminated on a date\" ~ 0.5,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ -2,  \n      Category == \"Eliminated at an unscheduled time\" ~ 0.6,  \n      Category == \"Fired by production\" ~ 2.1,  \n      TRUE ~ 0\n    ),\n    Nudge_Y_Category = case_when(\n      Category == \"Won\" ~ -0.8,  \n      Category == \"Quit\" ~ -0.5,  \n      Category == \"Eliminated on a date\" ~ 0.6,  \n      Category == \"Eliminated in weeks 2+ rose ceremony\" ~ 2.3,  \n      Category == \"Eliminated at an unscheduled time\" ~ -1.2,  \n      Category == \"Fired by production\" ~ 1.5,  \n      TRUE ~ 0\n    )\n  )\n\n# ===== CREATE THE BUBBLE PLOT =====\nggplot() +\n  # Draw bubbles\n  geom_polygon(data = circle_vertices, aes(x + bubble_data$x[id], y + bubble_data$y[id], \n                                           group = id, fill = bubble_data$Category[id]), \n               color = \"black\", alpha = 0.8) +  \n\n  # **Move COUNT labels inside bubbles**\n  geom_label(data = bubble_data %&gt;% filter(radius &gt; 30), \n             aes(x = x, y = y, label = Count, size = radius / 10), \n             fontface = \"bold\", fill = \"white\", alpha = 0.8,  \n             label.size = 0, hjust = 0.5, vjust = 0.5) +\n\n  # **Move COUNT labels outside small bubbles**\n  geom_label_repel(data = bubble_data %&gt;% filter(radius &lt;= 30), \n                   aes(x = x, y = y + radius * 3, label = Count, size = 5), \n                   fontface = \"bold\", fill = \"white\", color = \"black\",\n                   box.padding = 2, segment.color = \"black\", \n                   nudge_y = 20, direction = \"both\", force = 150) +\n\n  # **Move CATEGORY labels outside bubbles & allow multiple line text**\n  geom_label_repel(data = bubble_data, \n                   aes(x = x + Nudge_X_Category, y = y + Nudge_Y_Category, label = Category),  \n                   size = 4, color = \"black\", fontface = \"italic\",\n                   box.padding = 2, segment.color = \"black\", segment.size = 0.8,\n                   direction = \"both\", force = 400, label.size = 0.3, \n                   fill = \"white\", alpha = 0.9) +  \n\n  # **Move percentage labels inside bubbles**\n  geom_text(data = bubble_data, \n            aes(x = x, y = y + radius * 0.3, label = Percentage), \n            size = 3, color = \"black\", hjust = 0.5, vjust = 0.5) +\n\n  # Assign colors to the bubbles\n  scale_fill_manual(values = category_colors) +\n\n  # Remove default ggplot background\n  theme_void() +\n\n  # Adjust text styling\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n\n  # Set the title\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\nWarning: ggrepel: 3 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n# Ensure category_counts has percentages\ncategory_counts &lt;- category_counts %&gt;%\n  mutate(Percentage = paste0(round((Count / sum(Count)) * 100, 0), \"%\"))\n\n# Merge calculated percentages into bubble_data dynamically\nbubble_data &lt;- data.frame(\n  Category = c(\n    \"Eliminated in weeks 2+ rose ceremony\",\n    \"Eliminated in week 1 rose ceremony\",\n    \"Fired by production\",\n    \"Quit\",\n    \"Eliminated on a date\",\n    \"Eliminated at an unscheduled time\",\n    \"Won\"\n  )\n) %&gt;%\n  left_join(category_counts, by = \"Category\") %&gt;%\n  select(Category, Count, Percentage)\n\n# Create bubble chart without manual position adjustments\nggplot(bubble_data, aes(x = Count, y = Count, size = Count, fill = Category)) +\n  geom_point(alpha = 0.8, shape = 21, color = \"black\") +  # Use points instead of polygons\n  scale_size(range = c(5, 25)) +  # Adjust bubble scaling dynamically\n  scale_fill_viridis_d() +  # Dynamic color scaling for categories\n  geom_text_repel(aes(label = Count), size = 5, fontface = \"bold\", color = \"black\") +  # Automatic text positioning\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5)\n  ) +\n  ggtitle(\"The fate of every 'Bachelor' and 'Bachelorette' contestant\")\n\n\n\n\n\n\n\n\n~ Fin"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "fitting-exercise",
    "section": "",
    "text": "Loading packages\n\n#install packages that are missing if necessary\n\n#install.packages(\"tidymodels\")\n#install.packages(\"pander\")\n#install.packages(\"yardstick\")\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(skimr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(pander)\nlibrary(patchwork)\nlibrary(corrplot)\nlibrary(yardstick)\n\nloading and viewing a preview of the data\n\nmavoglurant_data &lt;- read.csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\")) #load data using here function\n\n\nsummary(mavoglurant_data)\n\n       ID             CMT             EVID              EVI2       \n Min.   :793.0   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:832.0   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :860.0   Median :2.000   Median :0.00000   Median :0.0000  \n Mean   :858.8   Mean   :1.926   Mean   :0.07394   Mean   :0.1613  \n 3rd Qu.:888.0   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :915.0   Max.   :2.000   Max.   :1.00000   Max.   :4.0000  \n      MDV                DV               LNDV            AMT        \n Min.   :0.00000   Min.   :   0.00   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:  23.52   1st Qu.:3.158   1st Qu.: 0.000  \n Median :0.00000   Median :  74.20   Median :4.306   Median : 0.000  \n Mean   :0.09373   Mean   : 179.93   Mean   :4.085   Mean   : 2.763  \n 3rd Qu.:0.00000   3rd Qu.: 283.00   3rd Qu.:5.645   3rd Qu.: 0.000  \n Max.   :1.00000   Max.   :1730.00   Max.   :7.456   Max.   :50.000  \n      TIME             DOSE            OCC             RATE       \n Min.   : 0.000   Min.   :25.00   Min.   :1.000   Min.   :  0.00  \n 1st Qu.: 0.583   1st Qu.:25.00   1st Qu.:1.000   1st Qu.:  0.00  \n Median : 2.250   Median :37.50   Median :1.000   Median :  0.00  \n Mean   : 5.851   Mean   :37.37   Mean   :1.378   Mean   : 16.55  \n 3rd Qu.: 6.363   3rd Qu.:50.00   3rd Qu.:2.000   3rd Qu.:  0.00  \n Max.   :48.217   Max.   :50.00   Max.   :2.000   Max.   :300.00  \n      AGE            SEX             RACE              WT        \n Min.   :18.0   Min.   :1.000   Min.   : 1.000   Min.   : 56.60  \n 1st Qu.:26.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 73.30  \n Median :31.0   Median :1.000   Median : 1.000   Median : 82.60  \n Mean   :32.9   Mean   :1.128   Mean   : 7.415   Mean   : 83.16  \n 3rd Qu.:40.0   3rd Qu.:1.000   3rd Qu.: 2.000   3rd Qu.: 90.60  \n Max.   :50.0   Max.   :2.000   Max.   :88.000   Max.   :115.30  \n       HT       \n Min.   :1.520  \n 1st Qu.:1.710  \n Median :1.780  \n Mean   :1.762  \n 3rd Qu.:1.820  \n Max.   :1.930  \n\nskim(mavoglurant_data)\n\n\nData summary\n\n\nName\nmavoglurant_data\n\n\nNumber of rows\n2678\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1\n858.81\n34.08\n793.00\n832.00\n860.00\n888.00\n915.00\n▅▆▇▇▇\n\n\nCMT\n0\n1\n1.93\n0.26\n1.00\n2.00\n2.00\n2.00\n2.00\n▁▁▁▁▇\n\n\nEVID\n0\n1\n0.07\n0.26\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nEVI2\n0\n1\n0.16\n0.70\n0.00\n0.00\n0.00\n0.00\n4.00\n▇▁▁▁▁\n\n\nMDV\n0\n1\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDV\n0\n1\n179.93\n226.28\n0.00\n23.52\n74.20\n283.00\n1730.00\n▇▂▁▁▁\n\n\nLNDV\n0\n1\n4.08\n1.88\n0.00\n3.16\n4.31\n5.64\n7.46\n▃▃▇▇▅\n\n\nAMT\n0\n1\n2.76\n10.32\n0.00\n0.00\n0.00\n0.00\n50.00\n▇▁▁▁▁\n\n\nTIME\n0\n1\n5.85\n8.91\n0.00\n0.58\n2.25\n6.36\n48.22\n▇▁▁▁▁\n\n\nDOSE\n0\n1\n37.37\n12.05\n25.00\n25.00\n37.50\n50.00\n50.00\n▇▁▁▁▇\n\n\nOCC\n0\n1\n1.38\n0.49\n1.00\n1.00\n1.00\n2.00\n2.00\n▇▁▁▁▅\n\n\nRATE\n0\n1\n16.55\n61.88\n0.00\n0.00\n0.00\n0.00\n300.00\n▇▁▁▁▁\n\n\nAGE\n0\n1\n32.90\n8.87\n18.00\n26.00\n31.00\n40.00\n50.00\n▆▇▅▅▅\n\n\nSEX\n0\n1\n1.13\n0.33\n1.00\n1.00\n1.00\n1.00\n2.00\n▇▁▁▁▁\n\n\nRACE\n0\n1\n7.41\n21.97\n1.00\n1.00\n1.00\n2.00\n88.00\n▇▁▁▁▁\n\n\nWT\n0\n1\n83.16\n12.48\n56.60\n73.30\n82.60\n90.60\n115.30\n▂▇▇▅▁\n\n\nHT\n0\n1\n1.76\n0.08\n1.52\n1.71\n1.78\n1.82\n1.93\n▁▃▆▇▃\n\n\n\n\n\nWe want to visualise the data in a plot. We want to plot DV (the outcome) as a function of time, stratified by DOSE and using ID as a grouping factor.\n\nDV_time_dose_curve &lt;- ggplot(mavoglurant_data, aes(x = TIME, y = DV, group = ID, colour = DOSE)) + geom_line(alpha = 0.7) + labs(title = \"DV vs Time by Dose\", x = \"Time\", y = \"DV\", colour = \"Dose\") \nprint(DV_time_dose_curve)\n\n\n\n\n\n\n\n\nThere is repetiton in the data. We only want to keep observations with OCC=1. We will use the filter function.\n\nmavoglurant_data_1 &lt;- filter(mavoglurant_data, OCC == 1) #make a dataset with only OCC 1 rows\n\nWe want a data frames where we have the total dose for each individual and where we exclude data where Time is zero.\n\nDV_data &lt;- mavoglurant_data_1 |&gt; filter(TIME !=0) |&gt; #removes values where TIME=0\ngroup_by(ID) |&gt;#Groups the data by ID so that we have all values for a single individual together\nsummarise(Y= sum(DV))\n\nNext we develop an additional data frames where Time is zero and join this to the previous one.\n\nmavoglurant_time_0 &lt;- mavoglurant_data_1 |&gt;filter(TIME==0) #filtering for when time is zero\n\nmerged_mavoglurant &lt;- merge(DV_data, mavoglurant_time_0, by = \"ID\") #using the merge function to join\nprint(merged_mavoglurant)\n\n     ID       Y CMT EVID EVI2 MDV DV LNDV  AMT TIME DOSE OCC RATE AGE SEX RACE\n1   793 2690.52   1    1    1   1  0    0 25.0    0 25.0   1   75  42   1    2\n2   794 2638.81   1    1    1   1  0    0 25.0    0 25.0   1  150  24   1    2\n3   795 2149.61   1    1    1   1  0    0 25.0    0 25.0   1  150  31   1    1\n4   796 1788.89   1    1    1   1  0    0 25.0    0 25.0   1  150  46   2    1\n5   797 3126.37   1    1    1   1  0    0 25.0    0 25.0   1  150  41   2    2\n6   798 2336.89   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n7   799 3007.20   1    1    1   1  0    0 25.0    0 25.0   1  150  23   1    1\n8   800 2795.65   1    1    1   1  0    0 25.0    0 25.0   1  150  20   1   88\n9   801 3865.79   1    1    1   1  0    0 25.0    0 25.0   1  150  23   1    2\n10  802 1761.62   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    1\n11  803 2548.98   1    1    1   1  0    0 25.0    0 25.0   1  150  46   1    1\n12  804 1967.61   1    1    1   1  0    0 25.0    0 25.0   1  150  22   1    1\n13  805 2352.78   1    1    1   1  0    0 37.5    0 37.5   1  225  43   2    1\n14  806 1800.79   1    1    1   1  0    0 37.5    0 37.5   1  225  50   2    1\n15  807 2009.16   1    1    1   1  0    0 37.5    0 37.5   1  225  19   1    2\n16  808 2815.26   1    1    1   1  0    0 37.5    0 37.5   1  225  26   1    2\n17  809 2008.52   1    1    1   1  0    0 37.5    0 37.5   1  225  39   1    1\n18  810 2933.99   1    1    1   1  0    0 37.5    0 37.5   1  225  46   1    1\n19  811 2748.86   1    1    1   1  0    0 37.5    0 37.5   1  225  41   1    1\n20  812 2154.56   1    1    1   1  0    0 37.5    0 37.5   1  225  30   1    1\n21  813 3462.59   1    1    1   1  0    0 37.5    0 37.5   1  225  49   1    1\n22  814 2771.69   1    1    1   1  0    0 37.5    0 37.5   1  225  28   1    1\n23  815 2423.89   1    1    1   1  0    0 37.5    0 37.5   1  225  25   1    1\n24  816 2084.87   1    1    1   1  0    0 37.5    0 37.5   1  225  37   1    1\n25  817 4984.57   1    1    1   1  0    0 50.0    0 50.0   1  300  47   1    1\n26  818 2572.45   1    1    1   1  0    0 50.0    0 50.0   1  300  27   1    1\n27  819 2667.02   1    1    1   1  0    0 50.0    0 50.0   1  300  45   2    1\n28  820 3004.21   1    1    1   1  0    0 50.0    0 50.0   1  300  28   1    1\n29  821 4834.65   1    1    1   1  0    0 50.0    0 50.0   1  300  42   2    1\n30  822 5606.58   1    1    1   1  0    0 50.0    0 50.0   1  300  29   1    1\n31  823 3408.61   1    1    1   1  0    0 50.0    0 50.0   1  300  39   1    1\n32  824 4493.01   1    1    1   1  0    0 50.0    0 50.0   1  300  38   1    1\n33  825 3513.71   1    1    1   1  0    0 50.0    0 50.0   1  300  32   1    2\n34  826 3905.93   1    1    1   1  0    0 50.0    0 50.0   1  300  47   1    2\n35  827 3644.37   1    1    1   1  0    0 50.0    0 50.0   1  300  28   1    2\n36  828 2746.20   1    1    1   1  0    0 50.0    0 50.0   1  300  40   1    2\n37  829 1424.00   1    1    1   1  0    0 25.0    0 25.0   1  150  44   2    1\n38  830 1108.17   1    1    1   1  0    0 25.0    0 25.0   1  150  48   2    2\n39  831 3104.70   1    1    1   1  0    0 50.0    0 50.0   1  300  45   1    1\n40  832 2177.20   1    1    1   1  0    0 50.0    0 50.0   1  300  31   1   88\n41  833 2193.20   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n42  834 1810.59   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n43  835 1666.10   1    1    1   1  0    0 25.0    0 25.0   1  150  45   1    2\n44  836 2027.39   1    1    1   1  0    0 25.0    0 25.0   1  150  20   1    1\n45  837 2345.50   1    1    1   1  0    0 50.0    0 50.0   1  300  31   1    1\n46  838 3310.20   1    1    1   1  0    0 50.0    0 50.0   1  300  46   1    1\n47  840 3777.20   1    1    1   1  0    0 50.0    0 50.0   1  300  37   1    1\n48  841 2063.43   1    1    1   1  0    0 25.0    0 25.0   1  150  40   1    2\n49  842 4378.37   1    1    1   1  0    0 50.0    0 50.0   1  300  18   1    2\n50  843 1853.91   1    1    1   1  0    0 25.0    0 25.0   1  150  24   1   88\n51  844 3774.00   1    1    1   1  0    0 50.0    0 50.0   1  300  35   1    2\n52  845 1625.46   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n53  846 1044.07   1    1    1   1  0    0 25.0    0 25.0   1  150  40   2    1\n54  847 1423.70   1    1    1   1  0    0 25.0    0 25.0   1  150  39   1    1\n55  848 3037.39   1    1    1   1  0    0 50.0    0 50.0   1  300  24   1    1\n56  849 2610.00   1    1    1   1  0    0 50.0    0 50.0   1  300  25   1    1\n57  850 3193.98   1    1    1   1  0    0 50.0    0 50.0   1  300  37   1    1\n58  851 1602.63   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    2\n59  852 2457.68   1    1    1   1  0    0 50.0    0 50.0   1  300  30   1    2\n60  853 1474.60   1    1    1   1  0    0 25.0    0 25.0   1  150  44   1    1\n61  854  997.89   1    1    1   1  0    0 25.0    0 25.0   1  150  43   1    1\n62  855 4451.84   1    1    1   1  0    0 50.0    0 50.0   1  300  24   1   88\n63  857 3507.10   1    1    1   1  0    0 50.0    0 50.0   1  300  22   1    1\n64  858 3332.16   1    1    1   1  0    0 50.0    0 50.0   1  300  18   1    1\n65  859 3733.10   1    1    1   1  0    0 50.0    0 50.0   1  300  34   1    2\n66  860 1886.48   1    1    1   1  0    0 25.0    0 25.0   1  150  47   1    1\n67  861 1175.69   1    1    1   1  0    0 25.0    0 25.0   1  150  33   1    1\n68  862 1517.24   1    1    1   1  0    0 25.0    0 25.0   1  150  18   1    1\n69  863 2036.20   1    1    1   1  0    0 50.0    0 50.0   1  300  18   1    2\n70  864 2532.10   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n71  865 1392.78   1    1    1   1  0    0 25.0    0 25.0   1  150  20   1    2\n72  866 2372.70   1    1    1   1  0    0 50.0    0 50.0   1  300  39   1    1\n73  867 3239.66   1    1    1   1  0    0 50.0    0 50.0   1  300  23   1    1\n74  868 1935.24   1    1    1   1  0    0 25.0    0 25.0   1  150  24   1    1\n75  869 1344.35   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    1\n76  870 1411.57   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1    1\n77  871 1712.00   1    1    1   1  0    0 25.0    0 25.0   1  150  19   1    1\n78  872 2978.20   1    1    1   1  0    0 50.0    0 50.0   1  300  49   1    1\n79  873 1948.80   1    1    1   1  0    0 50.0    0 50.0   1  300  39   1    1\n80  874 1346.62   1    1    1   1  0    0 25.0    0 25.0   1  150  41   1    2\n81  875 1380.61   1    1    1   1  0    0 25.0    0 25.0   1  150  19   1    1\n82  876 1214.97   1    1    1   1  0    0 25.0    0 25.0   1  150  49   1    1\n83  877 3622.80   1    1    1   1  0    0 50.0    0 50.0   1  300  32   1    2\n84  878 3751.90   1    1    1   1  0    0 50.0    0 50.0   1  300  42   1    1\n85  879 2092.89   1    1    1   1  0    0 25.0    0 25.0   1  150  39   2   88\n86  880 3458.43   1    1    1   1  0    0 50.0    0 50.0   1  300  39   2    1\n87  881 2789.70   1    1    1   1  0    0 50.0    0 50.0   1  300  49   2    7\n88  882 2303.58   1    1    1   1  0    0 25.0    0 25.0   1  150  28   2    1\n89  883 2030.50   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1   88\n90  884 1439.57   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n91  885 2471.60   1    1    1   1  0    0 50.0    0 50.0   1  300  45   1    2\n92  886 1097.60   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    1\n93  887 1464.29   1    1    1   1  0    0 25.0    0 25.0   1  150  25   1    1\n94  888 3243.29   1    1    1   1  0    0 50.0    0 50.0   1  300  21   1   88\n95  889 2654.70   1    1    1   1  0    0 25.0    0 25.0   1  150  37   1    2\n96  890 3609.33   1    1    1   1  0    0 50.0    0 50.0   1  300  29   1    1\n97  891 3060.70   1    1    1   1  0    0 50.0    0 50.0   1  300  25   1    2\n98  892 1374.48   1    1    1   1  0    0 25.0    0 25.0   1  150  25   1    1\n99  893 1451.50   1    1    1   1  0    0 25.0    0 25.0   1  150  36   2    1\n100 894 1503.55   1    1    1   1  0    0 25.0    0 25.0   1  150  35   2    1\n101 895 2027.60   1    1    1   1  0    0 25.0    0 25.0   1  150  28   2    2\n102 896 3046.72   1    1    1   1  0    0 50.0    0 50.0   1  300  24   1    1\n103 897 2485.00   1    1    1   1  0    0 50.0    0 50.0   1  300  29   1    1\n104 898 1731.80   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1   88\n105 899 1958.27   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1    1\n106 900 2996.40   1    1    1   1  0    0 50.0    0 50.0   1  300  26   1    1\n107 901 1288.64   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    2\n108 902 2353.40   1    1    1   1  0    0 50.0    0 50.0   1  300  37   1    2\n109 903 3016.30   1    1    1   1  0    0 50.0    0 50.0   1  300  45   1    1\n110 905 3306.15   1    1    1   1  0    0 50.0    0 50.0   1  300  38   1    1\n111 906  826.43   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n112 907 1338.20   1    1    1   1  0    0 25.0    0 25.0   1  150  40   1    1\n113 908 1490.93   1    1    1   1  0    0 25.0    0 25.0   1  150  41   1    7\n114 909 1067.56   1    1    1   1  0    0 25.0    0 25.0   1  150  37   1    1\n115 910 2472.90   1    1    1   1  0    0 50.0    0 50.0   1  300  36   1    1\n116 911 1085.93   1    1    1   1  0    0 25.0    0 25.0   1  150  41   1    1\n117 912 2278.97   1    1    1   1  0    0 50.0    0 50.0   1  300  28   1    1\n118 913 1898.00   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n119 914 1208.74   1    1    1   1  0    0 25.0    0 25.0   1  150  31   1    1\n120 915 3593.55   1    1    1   1  0    0 50.0    0 50.0   1  300  23   1    1\n       WT       HT\n1    94.3 1.769997\n2    80.4 1.759850\n3    71.8 1.809847\n4    77.4 1.649993\n5    64.3 1.560052\n6    74.1 1.829862\n7    87.9 1.850107\n8    61.9 1.730095\n9    65.3 1.649839\n10  103.5 1.840020\n11   83.0 1.779870\n12   68.7 1.700058\n13   64.4 1.560084\n14   69.8 1.640057\n15   86.1 1.910054\n16   84.5 1.770060\n17   99.1 1.809982\n18   71.2 1.669993\n19   82.6 1.830179\n20   85.4 1.860186\n21   76.1 1.700070\n22   78.3 1.719903\n23   73.3 1.690144\n24  102.1 1.809858\n25   79.5 1.749972\n26   97.5 1.849933\n27   80.7 1.659881\n28   83.2 1.740016\n29   58.0 1.580118\n30   85.7 1.770157\n31   74.2 1.749948\n32   70.4 1.640153\n33   78.9 1.740008\n34   89.3 1.689903\n35   96.8 1.900156\n36   74.8 1.650143\n37   85.4 1.640050\n38   79.5 1.620071\n39   99.1 1.809982\n40   88.3 1.759875\n41   91.6 1.740143\n42   69.2 1.810115\n43   92.0 1.690046\n44   80.5 1.879841\n45   85.9 1.719881\n46   94.5 1.779874\n47  101.8 1.829941\n48  102.7 1.879912\n49   56.6 1.740218\n50   70.7 1.780164\n51   81.6 1.650067\n52   90.6 1.789882\n53   83.8 1.680030\n54   82.1 1.810010\n55   78.7 1.790083\n56   79.8 1.859849\n57   74.9 1.760000\n58   73.2 1.729925\n59   85.2 1.809879\n60   90.1 1.779908\n61   99.8 1.800072\n62   58.4 1.660126\n63   69.4 1.820068\n64   73.1 1.749978\n65   88.0 1.840086\n66   80.3 1.699861\n67   99.3 1.799946\n68   96.0 1.909821\n69  102.7 1.809949\n70   71.4 1.679921\n71   86.0 1.680014\n72   89.3 1.719864\n73   72.4 1.840202\n74   70.4 1.740103\n75   74.3 1.800082\n76  102.5 1.909892\n77   72.7 1.710081\n78   97.3 1.800026\n79   75.7 1.780081\n80   81.0 1.749966\n81   66.2 1.800092\n82   80.9 1.810146\n83   92.2 1.799898\n84   92.9 1.760028\n85   58.2 1.619872\n86   69.8 1.520031\n87   69.6 1.580004\n88   62.3 1.750123\n89   63.6 1.719924\n90   75.1 1.850127\n91   93.8 1.780146\n92   85.9 1.870176\n93   87.1 1.839879\n94   81.9 1.770065\n95   78.6 1.740039\n96   68.8 1.810025\n97   81.0 1.780172\n98   84.6 1.710058\n99   88.2 1.710089\n100  90.0 1.679941\n101  58.9 1.580133\n102  90.1 1.799956\n103 115.3 1.930120\n104  72.2 1.700092\n105  70.5 1.679958\n106  82.1 1.780073\n107  77.5 1.819881\n108  99.1 1.779932\n109  90.0 1.730034\n110  83.4 1.819932\n111 105.1 1.879883\n112  97.3 1.860153\n113  85.8 1.789940\n114  85.4 1.820067\n115  84.4 1.730002\n116  77.1 1.589927\n117 113.2 1.909995\n118  89.0 1.829859\n119 110.8 1.869859\n120  96.3 1.820081\n\nsummary(merged_mavoglurant)\n\n       ID              Y               CMT         EVID        EVI2  \n Min.   :793.0   Min.   : 826.4   Min.   :1   Min.   :1   Min.   :1  \n 1st Qu.:822.8   1st Qu.:1700.5   1st Qu.:1   1st Qu.:1   1st Qu.:1  \n Median :853.5   Median :2349.1   Median :1   Median :1   Median :1  \n Mean   :853.7   Mean   :2445.4   Mean   :1   Mean   :1   Mean   :1  \n 3rd Qu.:884.2   3rd Qu.:3050.2   3rd Qu.:1   3rd Qu.:1   3rd Qu.:1  \n Max.   :915.0   Max.   :5606.6   Max.   :1   Max.   :1   Max.   :1  \n      MDV          DV         LNDV        AMT             TIME  \n Min.   :1   Min.   :0   Min.   :0   Min.   :25.00   Min.   :0  \n 1st Qu.:1   1st Qu.:0   1st Qu.:0   1st Qu.:25.00   1st Qu.:0  \n Median :1   Median :0   Median :0   Median :37.50   Median :0  \n Mean   :1   Mean   :0   Mean   :0   Mean   :36.46   Mean   :0  \n 3rd Qu.:1   3rd Qu.:0   3rd Qu.:0   3rd Qu.:50.00   3rd Qu.:0  \n Max.   :1   Max.   :0   Max.   :0   Max.   :50.00   Max.   :0  \n      DOSE            OCC         RATE            AGE             SEX       \n Min.   :25.00   Min.   :1   Min.   : 75.0   Min.   :18.00   Min.   :1.000  \n 1st Qu.:25.00   1st Qu.:1   1st Qu.:150.0   1st Qu.:26.00   1st Qu.:1.000  \n Median :37.50   Median :1   Median :225.0   Median :31.00   Median :1.000  \n Mean   :36.46   Mean   :1   Mean   :218.1   Mean   :33.00   Mean   :1.133  \n 3rd Qu.:50.00   3rd Qu.:1   3rd Qu.:300.0   3rd Qu.:40.25   3rd Qu.:1.000  \n Max.   :50.00   Max.   :1   Max.   :300.0   Max.   :50.00   Max.   :2.000  \n      RACE            WT               HT       \n Min.   : 1.0   Min.   : 56.60   Min.   :1.520  \n 1st Qu.: 1.0   1st Qu.: 73.17   1st Qu.:1.700  \n Median : 1.0   Median : 82.10   Median :1.770  \n Mean   : 7.2   Mean   : 82.55   Mean   :1.759  \n 3rd Qu.: 2.0   3rd Qu.: 90.10   3rd Qu.:1.813  \n Max.   :88.0   Max.   :115.30   Max.   :1.930  \n\n\nWe only kept the Time = 0 data points and so our data for time looks strange, but this is expected.\nWe want to delete unwanted columns, done with the select function, and also need to convert RACE and SEX to factors.\n\n#using the select function to delete unwanted columns\nmerged_mavoglurant_1 &lt;- merged_mavoglurant |&gt; select(-OCC , -EVID, -CMT, -EVI2, -MDV, -LNDV, -ID, -AMT, -RATE, -DV, -TIME) |&gt; mutate(RACE = as.factor(RACE), SEX = as.factor(SEX))\n\nsaveRDS(merged_mavoglurant_1, file = \"processed_mavoglurant.rds\")\n\nchecking our data and comparing two different summaries (summary vs skim) and functions for tables (kable vs pander).\n\ntable1p &lt;- pander::pander(summary(merged_mavoglurant_1))\n\ntable1k &lt;- knitr::kable(skim(merged_mavoglurant_1))\n\nprint(table1p)\n\n[1] \"\\n----------------------------------------------------------------\\n       Y              DOSE             AGE         SEX    RACE  \\n---------------- --------------- --------------- ------- -------\\n Min.  : 826.4    Min.  :25.00    Min.  :18.00    1:104   1 :74 \\n\\n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:26.00   2: 16   2 :36 \\n\\n Median :2349.1   Median :37.50   Median :31.00    NA     7 : 2 \\n\\n  Mean :2445.4     Mean :36.46     Mean :33.00     NA     88: 8 \\n\\n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:40.25    NA      NA   \\n\\n Max.  :5606.6    Max.  :50.00    Max.  :50.00     NA      NA   \\n----------------------------------------------------------------\\n\\nTable: Table continues below\\n\\n \\n--------------------------------\\n       WT              HT       \\n---------------- ---------------\\n Min.  : 56.60    Min.  :1.520  \\n\\n 1st Qu.: 73.17   1st Qu.:1.700 \\n\\n Median : 82.10   Median :1.770 \\n\\n  Mean : 82.55     Mean :1.759  \\n\\n 3rd Qu.: 90.10   3rd Qu.:1.813 \\n\\n Max.  :115.30    Max.  :1.930  \\n--------------------------------\\n\\n\"\nattr(,\"class\")\n[1] \"knit_asis\"\nattr(,\"knit_cacheable\")\n[1] NA\n\nprint(table1k)\n\n\n\n|skim_type |skim_variable | n_missing| complete_rate|factor.ordered | factor.n_unique|factor.top_counts         | numeric.mean|  numeric.sd| numeric.p0| numeric.p25| numeric.p50| numeric.p75| numeric.p100|numeric.hist |\n|:---------|:-------------|---------:|-------------:|:--------------|---------------:|:-------------------------|------------:|-----------:|----------:|-----------:|-----------:|-----------:|------------:|:------------|\n|factor    |SEX           |         0|             1|FALSE          |               2|1: 104, 2: 16             |           NA|          NA|         NA|          NA|          NA|          NA|           NA|NA           |\n|factor    |RACE          |         0|             1|FALSE          |               4|1: 74, 2: 36, 88: 8, 7: 2 |           NA|          NA|         NA|          NA|          NA|          NA|           NA|NA           |\n|numeric   |Y             |         0|             1|NA             |              NA|NA                        |   2445.40733| 961.6351038| 826.430000| 1700.525000| 2349.140000|  3050.21500|   5606.58000|▆▇▆▂▁        |\n|numeric   |DOSE          |         0|             1|NA             |              NA|NA                        |     36.45833|  11.8622315|  25.000000|   25.000000|   37.500000|    50.00000|     50.00000|▇▁▂▁▆        |\n|numeric   |AGE           |         0|             1|NA             |              NA|NA                        |     33.00000|   8.9761589|  18.000000|   26.000000|   31.000000|    40.25000|     50.00000|▅▇▃▅▅        |\n|numeric   |WT            |         0|             1|NA             |              NA|NA                        |     82.55417|  12.5211881|  56.600000|   73.175000|   82.100000|    90.10000|    115.30000|▂▇▇▅▁        |\n|numeric   |HT            |         0|             1|NA             |              NA|NA                        |      1.75901|   0.0855081|   1.520031|    1.700087|    1.770111|     1.81258|      1.93012|▁▃▆▇▃        |\n\n\nThese tables give a nice overview of the data and the small histograms for the numeric data are nice to get a very rough feel of the data.\nWe will generate a scatterplot to evaluate the total drug (Y) vs AGE.\n\nggplot(merged_mavoglurant_1, aes(x = AGE, y = Y)) +\n  geom_point(alpha = 0.6, color = \"blue\") +  # Add scatter points\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\") +  # Add trend line\n  labs(title = \"Scatterplot of Y vs Age\", x = \"Age\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSince we only kept the kept the points where time is equal to zero this scatter plot does not tell us much information about a progressing total drug vs time relationship but only shows the total drug for all participants at time zero.\nWe will repeat the scatterplot to evaluate the total drug (Y) vs dose.\n\nggplot(merged_mavoglurant_1, aes(x = DOSE, y = Y)) +\n  geom_point(alpha = 0.6, color = \"green\") +  # Add scatter points\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\") +  # Add trend line\n  labs(title = \"Scatterplot of Y vs Dose\", x = \"Dose\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: pseudoinverse used at 24.875\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: neighborhood radius 25.125\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: reciprocal condition number 2.903e-16\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: There are other near singularities as well. 631.27\n\n\n\n\n\n\n\n\n\nWe now see the pattern of increasing total drug with increasing dose (as expected).\nWe will generate a boxplot with jitter points to evaluate the total drug (Y) vs sex.\n\nggplot(merged_mavoglurant_1, aes(x = as.factor(SEX), y = Y)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7) +  # Boxplot\n  geom_jitter(alpha = 0.4, color = \"black\", width = 0.2) +  # Add jitter points\n  labs(title = \"Boxplot of Y by SEX\", x = \"SEX\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis would be more helpful if we knew which sex was reflected by which number since there are differences noted.\nWe will repeat a boxplot to evaluate the total drug (Y) vs dose.\n\nggplot(merged_mavoglurant_1, aes(x = as.factor(DOSE), y = Y)) +\n  geom_boxplot(fill = \"lightgreen\", alpha = 0.7) +  # Boxplot\n  geom_jitter(alpha = 0.4, color = \"black\", width = 0.2) +  # Add jitter points\n  labs(title = \"Boxplot of Y by Dose\", x = \"Dose\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing ChatGPT to generate code for plots of the distributions of variables\n\nvariables &lt;- c(\"Y\", \"DOSE\", \"AGE\", \"SEX\", \"RACE\", \"WT\", \"HT\")\n\n# Function to create distribution plots\nplot_distribution &lt;- function(data, var) {\n  if (is.numeric(data[[var]])) {\n    # Numeric variables: Histogram & Density Plot\n    ggplot(data, aes(x = .data[[var]])) +\n      geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n      geom_density(color = \"red\", size = 1) + \n      labs(title = paste(\"Distribution of\", var), x = var, y = \"Density\") +\n      theme_minimal()\n  } else {\n    # Categorical variables: Bar Plot\n    ggplot(data, aes(x = .data[[var]])) +\n      geom_bar(fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n      labs(title = paste(\"Distribution of\", var), x = var, y = \"Count\") +\n      theme_minimal()\n  }\n}\n\n# Plot distributions for each variable\nplots &lt;- lapply(variables, function(var) plot_distribution(merged_mavoglurant_1, var))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Display all plots together\nlibrary(patchwork)\nwrap_plots(plots, ncol = 2)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nWe get a good feel for the data for each variable here. However, because the source data is not clear we do not know what sex and race corresponds with the respective numbers. However, we can see very clear distributions favouring particular categories for these variables. The age, weight, height, and total drug (Y) data are more dispersed.\nWe want to look at a correlation plot of the data. We will use a correlation matrix:\n\n# Compute correlation matrix (select only numeric variables)\ncor_matrix &lt;- merged_mavoglurant_1 |&gt;\n  select(Y, DOSE, AGE, WT, HT) |&gt;\n  cor(use = \"pairwise.complete.obs\")  # Handle missing values properly\n\n# Plot the correlation matrix\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", \n         tl.col = \"black\", tl.cex = 0.8, addCoef.col = \"black\", number.cex = 0.7)\n\n\n\n\n\n\n\n\nThis plot provides a nice overview to look at the interactions/correlations between different variables. From this, there does not seem to be much correlation between the drug data and the biological data of participants.\n##Model Fitting\nWe need to generate some models for our tidied data. I have used chatgpt to help me to generate code.\nFirst we fit a linear model to the total drug (Y = continuous outcome) usings DOSE as our main outcome of interest.\n\n#We need to set a seed for reproducibility of our model\nset.seed(123)\n\n#We define the linear regression model\nlm_spec_dose &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Define a recipe (preprocessing steps)\nlm_recipe_dose &lt;- recipe(Y ~ DOSE, data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())  # Standardizes numeric predictors to account for different scales between vaariables \n\n# Create a workflow\nlm_workflow_dose &lt;- workflow() |&gt;\n  add_model(lm_spec_dose) |&gt;\n  add_recipe(lm_recipe_dose)\n\n#Fitting the model to the dataset\nlm_fit_dose &lt;- lm_workflow_dose |&gt;\n  fit(data = merged_mavoglurant_1)\n\n# View model summary\nlm_fit_dose |&gt; extract_fit_engine() |&gt; print() |&gt; summary()\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     2445.4        690.5  \n\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1284.91  -441.14   -97.22   325.84  2372.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2445.41      61.35   39.86   &lt;2e-16 ***\nDOSE          690.53      61.61   11.21   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 672.1 on 118 degrees of freedom\nMultiple R-squared:  0.5156,    Adjusted R-squared:  0.5115 \nF-statistic: 125.6 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\n\nAs expected, we see a positive association between the dose and the total drug (Y), with an increase with Y for increasing units of dose.\nWe will now generate a plot to visualise the model fit as well (Dose vs Total drug)\n\nggplot(merged_mavoglurant_1, aes(x = DOSE, y = Y)) +\n  geom_point(alpha = 0.5, color = \"blue\") +  # Scatterplot of actual data\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +  # Regression line\n  labs(title = \"Linear Regression: Y vs DOSE\", \n       x = \"DOSE\", \n       y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis confirms what we have seen in the data and would expect from these variables.\nWe will now compute the RMSE and R squared for this linear model looking at Total drug (Y) and Dose. The model will produce prediction values and compares these to actual values.\n\n# Generate predictions\nlm_predictions_dose &lt;- predict(lm_fit_dose, merged_mavoglurant_1) |&gt;\n  bind_cols(merged_mavoglurant_1)  # Adds actual values of Y for comparison\n\n# Compute RMSE and R²\nlm_metrics_dose &lt;- lm_predictions_dose |&gt;\n  metrics(truth = Y, estimate = .pred) |&gt;\n  filter(.metric %in% c(\"rmse\", \"rsq\"))  # Select RMSE & R²\n\n# Print results\nprint(lm_metrics_dose)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n\n\nOur model seems to perform moderately from this data.\nWe will now fit a linear model to the total drug (Y = continuous outcome) using all of the predictors. The predictors are standardised in the process to put them all on the same scale.\n\n# Define the linear regression model\nlm_spec_all_predict &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Define the recipe (preprocessing steps)\nlm_recipe_all_predict &lt;- recipe(Y ~ ., data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())  # Standardise numeric predictors \n\n# Create a workflow\nlm_workflow_all_predict &lt;- workflow() |&gt;\n  add_model(lm_spec_all_predict) |&gt;\n  add_recipe(lm_recipe_all_predict)\n\n\n# Fit the model using the entire dataset\nlm_fit_all_predict &lt;- lm_workflow_all_predict |&gt;\n  fit(data = merged_mavoglurant_1)\n\n# View model summary\nlm_fit_all_predict |&gt; extract_fit_engine() |&gt; print()|&gt; summary()\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2        RACE2        RACE7  \n    2456.92       710.96        28.32      -357.73       155.03      -405.32  \n     RACE88           WT           HT  \n     -53.51      -288.57       -64.00  \n\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1480.86  -367.81   -79.95   266.15  2431.52 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2456.92      80.58  30.490  &lt; 2e-16 ***\nDOSE          710.96      57.89  12.281  &lt; 2e-16 ***\nAGE            28.32      70.24   0.403 0.687530    \nSEX2         -357.73     216.93  -1.649 0.101957    \nRACE2         155.03     128.63   1.205 0.230650    \nRACE7        -405.32     448.19  -0.904 0.367768    \nRACE88        -53.51     244.67  -0.219 0.827296    \nWT           -288.57      80.07  -3.604 0.000471 ***\nHT            -64.00      94.40  -0.678 0.499188    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 614.3 on 111 degrees of freedom\nMultiple R-squared:  0.6193,    Adjusted R-squared:  0.5919 \nF-statistic: 22.57 on 8 and 111 DF,  p-value: &lt; 2.2e-16\n\n\nFrom these measures we once again see a strong association between dose and total drug. We now see that weight has a negative asociation with the total drug (as weight increases, Y decreases). There do not seem to be associations with age, height and race.\ncomputing R-squared and RMSE for the linear model to the total drug and all predictors.\n\n# Generate predictions\nlm_predictions_all_predict &lt;- predict(lm_fit_all_predict, merged_mavoglurant_1) |&gt;\n  bind_cols(merged_mavoglurant_1)\n\n# Compute RMSE and R-squared (and mae)\nlm_metrics_all_predict &lt;- lm_predictions_all_predict |&gt;\n  metrics(truth = Y, estimate = .pred)\n\nprint(lm_metrics_all_predict)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     591.   \n2 rsq     standard       0.619\n3 mae     standard     444.   \n\n\nOur RMSE is lower in this model which indicates slightly improved performance.\n\nggplot(lm_predictions_all_predict, aes(x = Y, y = .pred)) +\n  geom_point(alpha = 0.5, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Linear Model: Actual vs Predicted Y\", x = \"Actual Y\", y = \"Predicted Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe will now fit a logistic model for sex (a categorical/binary outcome) with DOSE as the main predictor of interest.\n\n# Define logistic regression model (SEX ~ DOSE)\nlog_spec_sex_dose &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n# Define a recipe (preprocessing steps)\nlog_recipe_sex_dose &lt;- recipe(SEX ~ DOSE, data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())\n\n# Create workflow\nlog_workflow_sex_dose &lt;- workflow() |&gt;\n  add_model(log_spec_sex_dose) |&gt;\n  add_recipe(log_recipe_sex_dose)\n\n# Fit the model\nlog_fit_sex_dose &lt;- log_workflow_sex_dose |&gt;\n  fit(data = merged_mavoglurant_1)\n\ncomputing ROC-AUC for the sex - dose logistic model and printing the results\n\n# Use augment() to get predictions (both probabilities and classes)\nlog_predictions_sex_dose &lt;- augment(log_fit_sex_dose, new_data = merged_mavoglurant_1)\n\n# Compute ROC AUC and Accuracy\nlog_metrics_sex_dose &lt;- log_predictions_sex_dose |&gt;\n  roc_auc(truth = SEX, .pred_1) |&gt;\n  bind_rows(\n    log_predictions_sex_dose |&gt;\n      accuracy(truth = SEX, estimate = .pred_class)\n  )\n\n# Print results\nprint(log_metrics_sex_dose)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc  binary         0.592\n2 accuracy binary         0.867\n\n\nThe ROC-AUC i not good and indicates that there is poor discrimination (it is basically random). Thus dose is not a good predicotr of sex (as expected). Even though we see a high accuracy, this is most likely due to the imbalance of sexes in the dataset.\n\n# Generate predictions with probabilities\nlog_predictions_sex_dose &lt;- predict(log_fit_sex_dose, merged_mavoglurant_1, type = \"prob\") |&gt;\n  bind_cols(merged_mavoglurant_1)\n\n# Plot logistic regression curve\nggplot(log_predictions_sex_dose, aes(x = DOSE, y = .pred_1)) +\n  geom_point(alpha = 0.4, color = \"blue\") +  # Scatterplot of individual predictions\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), color = \"red\") +\n  labs(title = \"Logistic Regression: Probability of SEX by DOSE\",\n       x = \"DOSE\", \n       y = \"Predicted Probability of SEX\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis curve might be misleading as it shows a positive relationship between dose and predicted probability of sex, which might be occuring due to imbalances in our data.\nWe will then fit a logistic model to sex using all predictors as we did with the linear model.\n\n# Define logistic regression model (SEX ~ all predictors)\nlog_spec_sex_predictors &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n# Define recipe\nlog_recipe_sex_predictors &lt;- recipe(SEX ~ ., data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())\n\n# Create workflow\nlog_workflow_sex_predictors &lt;- workflow() |&gt;\n  add_model(log_spec_sex_predictors) |&gt;\n  add_recipe(log_recipe_sex_predictors)\n\n# Fit the model\nlog_fit_sex_predictors &lt;- log_workflow_sex_predictors |&gt;\n  fit(data = merged_mavoglurant_1)\n\nComputing ROC-AUC for the sex - all predictors logistic model and printing the results.\n\n# Generate predictions (both probabilities & classes)\nlog_predictions_sex_predictors &lt;- augment(log_fit_sex_predictors, new_data = merged_mavoglurant_1)\n\n# Compute ROC AUC and Accuracy\nlog_metrics_sex_predictors &lt;- log_predictions_sex_predictors |&gt;\n  roc_auc(truth = SEX, .pred_1) |&gt;\n  bind_rows(\n    log_predictions_sex_predictors |&gt;\n      accuracy(truth = SEX, estimate = .pred_class)\n  )\n\n# Print results\nprint(log_metrics_sex_predictors)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc  binary         0.980\n2 accuracy binary         0.942\n\n\nOur model performed much better when looking at the other predictors as well. It had almost perfect performance at predicting sex data. Perhaps this is once again due to the skewed sex dataset and a degree of overfitting. More data points would be valauble to make sure that this is not the case and that this is based purely on the biological relationships.\n\n\n\n\n#Week 10 Assessment - Model Improvement\n##Part 1\nWe want to delete unwanted RACE column, done with the select function.\n\n#using the select function to delete unwanted Race column\nmodel_mavoglurant &lt;- merged_mavoglurant_1 |&gt; select(-RACE)\nskim(model_mavoglurant)\n\n\nData summary\n\n\nName\nmodel_mavoglurant\n\n\nNumber of rows\n120\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSEX\n0\n1\nFALSE\n2\n1: 104, 2: 16\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nY\n0\n1\n2445.41\n961.64\n826.43\n1700.53\n2349.14\n3050.22\n5606.58\n▆▇▆▂▁\n\n\nDOSE\n0\n1\n36.46\n11.86\n25.00\n25.00\n37.50\n50.00\n50.00\n▇▁▂▁▆\n\n\nAGE\n0\n1\n33.00\n8.98\n18.00\n26.00\n31.00\n40.25\n50.00\n▅▇▃▅▅\n\n\nWT\n0\n1\n82.55\n12.52\n56.60\n73.18\n82.10\n90.10\n115.30\n▂▇▇▅▁\n\n\nHT\n0\n1\n1.76\n0.09\n1.52\n1.70\n1.77\n1.81\n1.93\n▁▃▆▇▃\n\n\n\n\nsaveRDS(model_mavoglurant, file = \"cleaned_mavoglurant.rds\")\n\nSet the seed for further modeling steps:\n\nrngseed = 1234\nset.seed(rngseed)\n\nCreating a random split of the data: 75% train and 25% test set.\n\n# Split the data into training (75%) and testing (25%) sets\ndata_split &lt;- initial_split(model_mavoglurant, prop = 0.75)\n\n# Extract training and testing datasets\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Verify split\ndim(train_data)  # Check dimensions of training data\n\n[1] 90  6\n\ndim(test_data)   # Check dimensions of testing data\n\n[1] 30  6\n\n\nModel Fitting:\nWe will generate two linear models for the continuous outcome of interest (Y). The first with Dose as a predictor and the secodn with all predictors. Metric to optimise is RMSE.\nSetting up the linear regressions and running them:\n\n# Define a linear regression model specification\nmodel_lm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n#Model 1 Simple Linear Model (Only DOSE as Predictor)\nmodel_lm_simple &lt;- workflow() %&gt;%\n  add_model(model_lm_spec) %&gt;%\n  add_formula(Y ~ DOSE) %&gt;%\n  fit(data = train_data)\n\n#Model 2 Full model (All Predictors)\nmodel_lm_full &lt;- workflow() %&gt;%\n  add_model(model_lm_spec) %&gt;%\n  add_formula(Y ~ .) %&gt;%  # Includes all predictors\n  fit(data = train_data)\n\nWe will now use the models to make predictions from the training data that we assigned. We will combine this with our original data.\n\n# Compute Predictions on Training Data and combine this with the original data using bind_col\ntrain_preds_simple &lt;- predict(model_lm_simple, new_data = train_data) %&gt;%\n  bind_cols(train_data) \n\ntrain_preds_full &lt;- predict(model_lm_full, new_data = train_data) %&gt;%\n  bind_cols(train_data)\n\nWe will now compute the performance of both of the models using RMSE\n\n#Compute RMSE for Both Models\nrmse_simple_model &lt;- train_preds_simple %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\")\n\nrmse_full_model &lt;- train_preds_full %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\")\n\nWe now compute the RMSE for the null model (one that predcits the mean outcome for each observation)\n\n#Compute RMSE for null model Using tidymodels null_model function\nnull_spec &lt;- null_model() %&gt;%\n  set_engine(\"parsnip\") %&gt;% #using tidymodels parsnip engine\n  set_mode(\"regression\")\n\nnull_fit &lt;- null_spec %&gt;% fit(Y ~ 1, data = train_data) #This tells our model to use the intercept (no predictor values)\n\ntrain_preds_null &lt;- predict(null_fit, new_data = train_data) %&gt;%\n  bind_cols(train_data)\n\nrmse_null &lt;- train_preds_null %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\")\n\nWarning: A correlation computation is required, but `estimate` is constant and has 0\nstandard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nWe will display all of the calculated RMSE values to compare them\n\n#Combine and print RMSE values \nrmse_results &lt;- bind_rows(\n  rmse_null %&gt;% mutate(model = \"Null Model\"),\n  rmse_simple_model %&gt;% mutate(model = \"Model 1 (DOSE Only)\"),\n  rmse_full_model %&gt;% mutate(model = \"Model 2 (All Predictors)\")\n)\n\nprint(rmse_results)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate model                   \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                   \n1 rmse    standard        948. Null Model              \n2 rmse    standard        703. Model 1 (DOSE Only)     \n3 rmse    standard        627. Model 2 (All Predictors)\n\n\nThrough these steps we see that we get the outline values for the RMSE from the three models of 948, 702 and 627 for the null model and models 1 and 2 respectively. Thus both model one and model two seem to perform better than the the null model based on the RMSE values, with model 2 performing best.\n##Model performance assessment 2\nWe are going to do CV tests to evaluate the model performance and the recalculate the RMSE and evaluate the standard error from this. We will end with changing our set seed to see how this affects our results.\nWe will set the same seed as before to start:\n\nrngseed = 1234\nset.seed(rngseed)\n\nCreating our CV splits and showing the variable like in the tutorial.\n\n#Create 10-Fold Cross-Validation Splits\ncv_splits &lt;- vfold_cv(train_data, v = 10)  \n\ncv_splits\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10\n\n\nWe will now recreate the linear models using the CV splits. We follow a similar approach as we have used for the previous models.\n\n#Define a Linear Regression Model Specification\ncv_model_lm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")  # Linear regression model\n\n#starting with the simple model first\n\n#model 1 simple model using DOSE only\nworkflow_simple &lt;- workflow() %&gt;%\n  add_model(cv_model_lm_spec) %&gt;%\n  add_formula(Y ~ DOSE)\n\n#actually usign the cv splits we created\ncv_results_simple &lt;- workflow_simple %&gt;%\n  fit_resamples(\n    resamples = cv_splits,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE))\n\n#model 2 Full Model using ALL Predictors\nworkflow_full &lt;- workflow() %&gt;%\n  add_model(cv_model_lm_spec) %&gt;%\n  add_formula(Y ~ .)\n\n#actually using our created cv splits\ncv_results_full &lt;- workflow_full %&gt;%\n  fit_resamples(\n    resamples = cv_splits,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE))\n\nWe will first collect and compute the RMSE and SE per fold for each model for comparison.\n\n# Extract RMSE per fold for Model 1 (DOSE only)\nrmse_per_fold_simple &lt;- cv_results_simple %&gt;%\n  collect_predictions() %&gt;%  # Extract per-fold predictions\n  group_by(id) %&gt;%           # Group by fold ID\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))  # Compute RMSE per fold\n\n# Extract RMSE per fold for Model 2 (All predictors)\nrmse_per_fold_full &lt;- cv_results_full %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))\n\n# Print RMSE per fold\nprint(\"RMSE per fold for Model 1 (DOSE only):\")\n\n[1] \"RMSE per fold for Model 1 (DOSE only):\"\n\nprint(rmse_per_fold_simple)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      602.\n 2 Fold02      353.\n 3 Fold03      804.\n 4 Fold04      986.\n 5 Fold05      601.\n 6 Fold06      969.\n 7 Fold07      516.\n 8 Fold08      739.\n 9 Fold09      483.\n10 Fold10      852.\n\nprint(\"RMSE per fold for Model 2 (All predictors):\")\n\n[1] \"RMSE per fold for Model 2 (All predictors):\"\n\nprint(rmse_per_fold_full)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      745.\n 2 Fold02      480.\n 3 Fold03      617.\n 4 Fold04      971.\n 5 Fold05      403.\n 6 Fold06      765.\n 7 Fold07      454.\n 8 Fold08      522.\n 9 Fold09      540.\n10 Fold10      959.\n\n# Compute Mean RMSE and Standard Error (SE) across folds\nsummary_simple &lt;- rmse_per_fold_simple %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))  # SE = SD / sqrt(N)\n\nsummary_full &lt;- rmse_per_fold_full %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))\n\n# Print Summary Statistics\nprint(\"Summary RMSE for Model 1 (DOSE only) - manual per-pold calculation\")\n\n[1] \"Summary RMSE for Model 1 (DOSE only) - manual per-pold calculation\"\n\nprint(summary_simple)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    67.5\n\nprint(\"Summary RMSE for Model 2 (All predictors) - manual per-fold alculation\")\n\n[1] \"Summary RMSE for Model 2 (All predictors) - manual per-fold alculation\"\n\nprint(summary_full)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    64.8\n\n\nWe see substantial variation between the mean rmse for each fold. This highlights the value of checking each fold and not just the summarised mean rmse from all folds if you want to check variability.\nWe will also compute (collect) RMSE & standard error (SE) for each model automatically using tidymodels and so we are collecting the mean and SE of the overall RMSE (single value). This is calculated across all folds and not per fold.\n\n#Extract RMSE and SE values for Model 1 (DOSE only)\nrmse_cv_simple &lt;- cv_results_simple %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)  # `std_err` from tidymodels\n\n# Extract RMSE values for model 2 - All predictors\nrmse_cv_full &lt;- cv_results_full %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)\n\n\n\n#Print RMSE and standard errors \nprint(\"RMSE from 10-Fold cross-validation (DOSE only) with SE\")\n\n[1] \"RMSE from 10-Fold cross-validation (DOSE only) with SE\"\n\nprint(rmse_cv_simple)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  691.    67.5\n\nprint(\"RMSE from 10-Fold cross-validation (All predictors) with SE\")\n\n[1] \"RMSE from 10-Fold cross-validation (All predictors) with SE\"\n\nprint(rmse_cv_full)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  646.    64.8\n\n\nThis automatic approach however is preferred if you want a quick overview because we can compare the mean and SE collectively to assess each model. We can summise that the model with all variables included performs better than the simple model with only Dose as a predictor based on the decreased RMSE and SE.\nWe will now change the random seed & re-run cross-validation using the automatic approach first followed by the manual approach for completeness. But the manual per fold values might be harder to compare.\n\nset.seed(999)  #change the seed to check how RMSE variability changes\n\n#Create New CV Splits\ncv_splits_new &lt;- vfold_cv(train_data, v = 10)\n\n# Re-run CV with new random seed\n\ncv_results_simple_new &lt;- workflow_simple %&gt;%\n  fit_resamples(\n    resamples = cv_splits_new,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE)\n  )\n\ncv_results_full_new &lt;- workflow_full %&gt;%\n  fit_resamples(\n    resamples = cv_splits_new,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Extract new RMSE values for Model 1 (DOSE only)\nrmse_cv_simple_new &lt;- cv_results_simple_new %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)\n\n# Extract new RMSE values for Model 2 (All predictors)\nrmse_cv_full_new &lt;- cv_results_full_new %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)\n\n# Print RMSE values from new seed\nprint(\"RMSE from New Random Seed (DOSE only)\")\n\n[1] \"RMSE from New Random Seed (DOSE only)\"\n\nprint(rmse_cv_simple_new)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  705.    45.7\n\nprint(\"RMSE from New Random Seed (All predictors)\")\n\n[1] \"RMSE from New Random Seed (All predictors)\"\n\nprint(rmse_cv_full_new)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  648.    52.7\n\n\nWe see a marked change in the mean and SE of the RMSE from changing our seed to 999 instead of 1234.\nReruning the manual approach with the new seed:\n\n# Extract RMSE per fold for Model 1 (DOSE only) with new seed\nrmse_per_fold_simple_new &lt;- cv_results_simple_new %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))\n\n# Extract RMSE per fold for Model 2 (All predictors) with new seed\nrmse_per_fold_full_new &lt;- cv_results_full_new %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))\n\n# Print RMSE per fold from new seed\nprint(\"RMSE per fold for Model 1 (DOSE only) - New Seed:\")\n\n[1] \"RMSE per fold for Model 1 (DOSE only) - New Seed:\"\n\nprint(rmse_per_fold_simple_new)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      652.\n 2 Fold02      601.\n 3 Fold03      610.\n 4 Fold04      545.\n 5 Fold05      741.\n 6 Fold06      972.\n 7 Fold07      543.\n 8 Fold08      898.\n 9 Fold09      760.\n10 Fold10      728.\n\nprint(\"RMSE per fold for Model 2 (All predictors) - New Seed:\")\n\n[1] \"RMSE per fold for Model 2 (All predictors) - New Seed:\"\n\nprint(rmse_per_fold_full_new)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      669.\n 2 Fold02      448.\n 3 Fold03      697.\n 4 Fold04      673.\n 5 Fold05      464.\n 6 Fold06     1017.\n 7 Fold07      508.\n 8 Fold08      753.\n 9 Fold09      574.\n10 Fold10      679.\n\n# Compute Standard Error (SE) across folds (with new seed)\nsummary_simple_new &lt;- rmse_per_fold_simple_new %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))\n\nsummary_full_new &lt;- rmse_per_fold_full_new %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))\n\n# Print Summary Statistics from new seed\nprint(\"Standard Error (SE) for Model 1 (DOSE only) - New Seed:\")\n\n[1] \"Standard Error (SE) for Model 1 (DOSE only) - New Seed:\"\n\nprint(summary_simple_new)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    45.7\n\nprint(\"Standard Error (SE) for Model 2 (All predictors) - New Seed:\")\n\n[1] \"Standard Error (SE) for Model 2 (All predictors) - New Seed:\"\n\nprint(summary_full_new)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    52.7\n\n\nWe see similar differences here between he individual folds but this is harder to observe directly with the differences between folds. However, the SE clearly changed from both being in the 60s to neither of the values being in that range.\nFrom all of our checks it seems that model 2, which includes all factors, and ot just Dose, performs the best."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-fitting",
    "href": "fitting-exercise/fitting-exercise.html#model-fitting",
    "title": "fitting-exercise",
    "section": "",
    "text": "Loading packages\n\n#install packages that are missing if necessary\n\n#install.packages(\"tidymodels\")\n#install.packages(\"pander\")\n#install.packages(\"yardstick\")\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(skimr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(pander)\nlibrary(patchwork)\nlibrary(corrplot)\nlibrary(yardstick)\n\nloading and viewing a preview of the data\n\nmavoglurant_data &lt;- read.csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\")) #load data using here function\n\n\nsummary(mavoglurant_data)\n\n       ID             CMT             EVID              EVI2       \n Min.   :793.0   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:832.0   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :860.0   Median :2.000   Median :0.00000   Median :0.0000  \n Mean   :858.8   Mean   :1.926   Mean   :0.07394   Mean   :0.1613  \n 3rd Qu.:888.0   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :915.0   Max.   :2.000   Max.   :1.00000   Max.   :4.0000  \n      MDV                DV               LNDV            AMT        \n Min.   :0.00000   Min.   :   0.00   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:  23.52   1st Qu.:3.158   1st Qu.: 0.000  \n Median :0.00000   Median :  74.20   Median :4.306   Median : 0.000  \n Mean   :0.09373   Mean   : 179.93   Mean   :4.085   Mean   : 2.763  \n 3rd Qu.:0.00000   3rd Qu.: 283.00   3rd Qu.:5.645   3rd Qu.: 0.000  \n Max.   :1.00000   Max.   :1730.00   Max.   :7.456   Max.   :50.000  \n      TIME             DOSE            OCC             RATE       \n Min.   : 0.000   Min.   :25.00   Min.   :1.000   Min.   :  0.00  \n 1st Qu.: 0.583   1st Qu.:25.00   1st Qu.:1.000   1st Qu.:  0.00  \n Median : 2.250   Median :37.50   Median :1.000   Median :  0.00  \n Mean   : 5.851   Mean   :37.37   Mean   :1.378   Mean   : 16.55  \n 3rd Qu.: 6.363   3rd Qu.:50.00   3rd Qu.:2.000   3rd Qu.:  0.00  \n Max.   :48.217   Max.   :50.00   Max.   :2.000   Max.   :300.00  \n      AGE            SEX             RACE              WT        \n Min.   :18.0   Min.   :1.000   Min.   : 1.000   Min.   : 56.60  \n 1st Qu.:26.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 73.30  \n Median :31.0   Median :1.000   Median : 1.000   Median : 82.60  \n Mean   :32.9   Mean   :1.128   Mean   : 7.415   Mean   : 83.16  \n 3rd Qu.:40.0   3rd Qu.:1.000   3rd Qu.: 2.000   3rd Qu.: 90.60  \n Max.   :50.0   Max.   :2.000   Max.   :88.000   Max.   :115.30  \n       HT       \n Min.   :1.520  \n 1st Qu.:1.710  \n Median :1.780  \n Mean   :1.762  \n 3rd Qu.:1.820  \n Max.   :1.930  \n\nskim(mavoglurant_data)\n\n\nData summary\n\n\nName\nmavoglurant_data\n\n\nNumber of rows\n2678\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1\n858.81\n34.08\n793.00\n832.00\n860.00\n888.00\n915.00\n▅▆▇▇▇\n\n\nCMT\n0\n1\n1.93\n0.26\n1.00\n2.00\n2.00\n2.00\n2.00\n▁▁▁▁▇\n\n\nEVID\n0\n1\n0.07\n0.26\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nEVI2\n0\n1\n0.16\n0.70\n0.00\n0.00\n0.00\n0.00\n4.00\n▇▁▁▁▁\n\n\nMDV\n0\n1\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDV\n0\n1\n179.93\n226.28\n0.00\n23.52\n74.20\n283.00\n1730.00\n▇▂▁▁▁\n\n\nLNDV\n0\n1\n4.08\n1.88\n0.00\n3.16\n4.31\n5.64\n7.46\n▃▃▇▇▅\n\n\nAMT\n0\n1\n2.76\n10.32\n0.00\n0.00\n0.00\n0.00\n50.00\n▇▁▁▁▁\n\n\nTIME\n0\n1\n5.85\n8.91\n0.00\n0.58\n2.25\n6.36\n48.22\n▇▁▁▁▁\n\n\nDOSE\n0\n1\n37.37\n12.05\n25.00\n25.00\n37.50\n50.00\n50.00\n▇▁▁▁▇\n\n\nOCC\n0\n1\n1.38\n0.49\n1.00\n1.00\n1.00\n2.00\n2.00\n▇▁▁▁▅\n\n\nRATE\n0\n1\n16.55\n61.88\n0.00\n0.00\n0.00\n0.00\n300.00\n▇▁▁▁▁\n\n\nAGE\n0\n1\n32.90\n8.87\n18.00\n26.00\n31.00\n40.00\n50.00\n▆▇▅▅▅\n\n\nSEX\n0\n1\n1.13\n0.33\n1.00\n1.00\n1.00\n1.00\n2.00\n▇▁▁▁▁\n\n\nRACE\n0\n1\n7.41\n21.97\n1.00\n1.00\n1.00\n2.00\n88.00\n▇▁▁▁▁\n\n\nWT\n0\n1\n83.16\n12.48\n56.60\n73.30\n82.60\n90.60\n115.30\n▂▇▇▅▁\n\n\nHT\n0\n1\n1.76\n0.08\n1.52\n1.71\n1.78\n1.82\n1.93\n▁▃▆▇▃\n\n\n\n\n\nWe want to visualise the data in a plot. We want to plot DV (the outcome) as a function of time, stratified by DOSE and using ID as a grouping factor.\n\nDV_time_dose_curve &lt;- ggplot(mavoglurant_data, aes(x = TIME, y = DV, group = ID, colour = DOSE)) + geom_line(alpha = 0.7) + labs(title = \"DV vs Time by Dose\", x = \"Time\", y = \"DV\", colour = \"Dose\") \nprint(DV_time_dose_curve)\n\n\n\n\n\n\n\n\nThere is repetiton in the data. We only want to keep observations with OCC=1. We will use the filter function.\n\nmavoglurant_data_1 &lt;- filter(mavoglurant_data, OCC == 1) #make a dataset with only OCC 1 rows\n\nWe want a data frames where we have the total dose for each individual and where we exclude data where Time is zero.\n\nDV_data &lt;- mavoglurant_data_1 |&gt; filter(TIME !=0) |&gt; #removes values where TIME=0\ngroup_by(ID) |&gt;#Groups the data by ID so that we have all values for a single individual together\nsummarise(Y= sum(DV))\n\nNext we develop an additional data frames where Time is zero and join this to the previous one.\n\nmavoglurant_time_0 &lt;- mavoglurant_data_1 |&gt;filter(TIME==0) #filtering for when time is zero\n\nmerged_mavoglurant &lt;- merge(DV_data, mavoglurant_time_0, by = \"ID\") #using the merge function to join\nprint(merged_mavoglurant)\n\n     ID       Y CMT EVID EVI2 MDV DV LNDV  AMT TIME DOSE OCC RATE AGE SEX RACE\n1   793 2690.52   1    1    1   1  0    0 25.0    0 25.0   1   75  42   1    2\n2   794 2638.81   1    1    1   1  0    0 25.0    0 25.0   1  150  24   1    2\n3   795 2149.61   1    1    1   1  0    0 25.0    0 25.0   1  150  31   1    1\n4   796 1788.89   1    1    1   1  0    0 25.0    0 25.0   1  150  46   2    1\n5   797 3126.37   1    1    1   1  0    0 25.0    0 25.0   1  150  41   2    2\n6   798 2336.89   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n7   799 3007.20   1    1    1   1  0    0 25.0    0 25.0   1  150  23   1    1\n8   800 2795.65   1    1    1   1  0    0 25.0    0 25.0   1  150  20   1   88\n9   801 3865.79   1    1    1   1  0    0 25.0    0 25.0   1  150  23   1    2\n10  802 1761.62   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    1\n11  803 2548.98   1    1    1   1  0    0 25.0    0 25.0   1  150  46   1    1\n12  804 1967.61   1    1    1   1  0    0 25.0    0 25.0   1  150  22   1    1\n13  805 2352.78   1    1    1   1  0    0 37.5    0 37.5   1  225  43   2    1\n14  806 1800.79   1    1    1   1  0    0 37.5    0 37.5   1  225  50   2    1\n15  807 2009.16   1    1    1   1  0    0 37.5    0 37.5   1  225  19   1    2\n16  808 2815.26   1    1    1   1  0    0 37.5    0 37.5   1  225  26   1    2\n17  809 2008.52   1    1    1   1  0    0 37.5    0 37.5   1  225  39   1    1\n18  810 2933.99   1    1    1   1  0    0 37.5    0 37.5   1  225  46   1    1\n19  811 2748.86   1    1    1   1  0    0 37.5    0 37.5   1  225  41   1    1\n20  812 2154.56   1    1    1   1  0    0 37.5    0 37.5   1  225  30   1    1\n21  813 3462.59   1    1    1   1  0    0 37.5    0 37.5   1  225  49   1    1\n22  814 2771.69   1    1    1   1  0    0 37.5    0 37.5   1  225  28   1    1\n23  815 2423.89   1    1    1   1  0    0 37.5    0 37.5   1  225  25   1    1\n24  816 2084.87   1    1    1   1  0    0 37.5    0 37.5   1  225  37   1    1\n25  817 4984.57   1    1    1   1  0    0 50.0    0 50.0   1  300  47   1    1\n26  818 2572.45   1    1    1   1  0    0 50.0    0 50.0   1  300  27   1    1\n27  819 2667.02   1    1    1   1  0    0 50.0    0 50.0   1  300  45   2    1\n28  820 3004.21   1    1    1   1  0    0 50.0    0 50.0   1  300  28   1    1\n29  821 4834.65   1    1    1   1  0    0 50.0    0 50.0   1  300  42   2    1\n30  822 5606.58   1    1    1   1  0    0 50.0    0 50.0   1  300  29   1    1\n31  823 3408.61   1    1    1   1  0    0 50.0    0 50.0   1  300  39   1    1\n32  824 4493.01   1    1    1   1  0    0 50.0    0 50.0   1  300  38   1    1\n33  825 3513.71   1    1    1   1  0    0 50.0    0 50.0   1  300  32   1    2\n34  826 3905.93   1    1    1   1  0    0 50.0    0 50.0   1  300  47   1    2\n35  827 3644.37   1    1    1   1  0    0 50.0    0 50.0   1  300  28   1    2\n36  828 2746.20   1    1    1   1  0    0 50.0    0 50.0   1  300  40   1    2\n37  829 1424.00   1    1    1   1  0    0 25.0    0 25.0   1  150  44   2    1\n38  830 1108.17   1    1    1   1  0    0 25.0    0 25.0   1  150  48   2    2\n39  831 3104.70   1    1    1   1  0    0 50.0    0 50.0   1  300  45   1    1\n40  832 2177.20   1    1    1   1  0    0 50.0    0 50.0   1  300  31   1   88\n41  833 2193.20   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n42  834 1810.59   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n43  835 1666.10   1    1    1   1  0    0 25.0    0 25.0   1  150  45   1    2\n44  836 2027.39   1    1    1   1  0    0 25.0    0 25.0   1  150  20   1    1\n45  837 2345.50   1    1    1   1  0    0 50.0    0 50.0   1  300  31   1    1\n46  838 3310.20   1    1    1   1  0    0 50.0    0 50.0   1  300  46   1    1\n47  840 3777.20   1    1    1   1  0    0 50.0    0 50.0   1  300  37   1    1\n48  841 2063.43   1    1    1   1  0    0 25.0    0 25.0   1  150  40   1    2\n49  842 4378.37   1    1    1   1  0    0 50.0    0 50.0   1  300  18   1    2\n50  843 1853.91   1    1    1   1  0    0 25.0    0 25.0   1  150  24   1   88\n51  844 3774.00   1    1    1   1  0    0 50.0    0 50.0   1  300  35   1    2\n52  845 1625.46   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n53  846 1044.07   1    1    1   1  0    0 25.0    0 25.0   1  150  40   2    1\n54  847 1423.70   1    1    1   1  0    0 25.0    0 25.0   1  150  39   1    1\n55  848 3037.39   1    1    1   1  0    0 50.0    0 50.0   1  300  24   1    1\n56  849 2610.00   1    1    1   1  0    0 50.0    0 50.0   1  300  25   1    1\n57  850 3193.98   1    1    1   1  0    0 50.0    0 50.0   1  300  37   1    1\n58  851 1602.63   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    2\n59  852 2457.68   1    1    1   1  0    0 50.0    0 50.0   1  300  30   1    2\n60  853 1474.60   1    1    1   1  0    0 25.0    0 25.0   1  150  44   1    1\n61  854  997.89   1    1    1   1  0    0 25.0    0 25.0   1  150  43   1    1\n62  855 4451.84   1    1    1   1  0    0 50.0    0 50.0   1  300  24   1   88\n63  857 3507.10   1    1    1   1  0    0 50.0    0 50.0   1  300  22   1    1\n64  858 3332.16   1    1    1   1  0    0 50.0    0 50.0   1  300  18   1    1\n65  859 3733.10   1    1    1   1  0    0 50.0    0 50.0   1  300  34   1    2\n66  860 1886.48   1    1    1   1  0    0 25.0    0 25.0   1  150  47   1    1\n67  861 1175.69   1    1    1   1  0    0 25.0    0 25.0   1  150  33   1    1\n68  862 1517.24   1    1    1   1  0    0 25.0    0 25.0   1  150  18   1    1\n69  863 2036.20   1    1    1   1  0    0 50.0    0 50.0   1  300  18   1    2\n70  864 2532.10   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n71  865 1392.78   1    1    1   1  0    0 25.0    0 25.0   1  150  20   1    2\n72  866 2372.70   1    1    1   1  0    0 50.0    0 50.0   1  300  39   1    1\n73  867 3239.66   1    1    1   1  0    0 50.0    0 50.0   1  300  23   1    1\n74  868 1935.24   1    1    1   1  0    0 25.0    0 25.0   1  150  24   1    1\n75  869 1344.35   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    1\n76  870 1411.57   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1    1\n77  871 1712.00   1    1    1   1  0    0 25.0    0 25.0   1  150  19   1    1\n78  872 2978.20   1    1    1   1  0    0 50.0    0 50.0   1  300  49   1    1\n79  873 1948.80   1    1    1   1  0    0 50.0    0 50.0   1  300  39   1    1\n80  874 1346.62   1    1    1   1  0    0 25.0    0 25.0   1  150  41   1    2\n81  875 1380.61   1    1    1   1  0    0 25.0    0 25.0   1  150  19   1    1\n82  876 1214.97   1    1    1   1  0    0 25.0    0 25.0   1  150  49   1    1\n83  877 3622.80   1    1    1   1  0    0 50.0    0 50.0   1  300  32   1    2\n84  878 3751.90   1    1    1   1  0    0 50.0    0 50.0   1  300  42   1    1\n85  879 2092.89   1    1    1   1  0    0 25.0    0 25.0   1  150  39   2   88\n86  880 3458.43   1    1    1   1  0    0 50.0    0 50.0   1  300  39   2    1\n87  881 2789.70   1    1    1   1  0    0 50.0    0 50.0   1  300  49   2    7\n88  882 2303.58   1    1    1   1  0    0 25.0    0 25.0   1  150  28   2    1\n89  883 2030.50   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1   88\n90  884 1439.57   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n91  885 2471.60   1    1    1   1  0    0 50.0    0 50.0   1  300  45   1    2\n92  886 1097.60   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    1\n93  887 1464.29   1    1    1   1  0    0 25.0    0 25.0   1  150  25   1    1\n94  888 3243.29   1    1    1   1  0    0 50.0    0 50.0   1  300  21   1   88\n95  889 2654.70   1    1    1   1  0    0 25.0    0 25.0   1  150  37   1    2\n96  890 3609.33   1    1    1   1  0    0 50.0    0 50.0   1  300  29   1    1\n97  891 3060.70   1    1    1   1  0    0 50.0    0 50.0   1  300  25   1    2\n98  892 1374.48   1    1    1   1  0    0 25.0    0 25.0   1  150  25   1    1\n99  893 1451.50   1    1    1   1  0    0 25.0    0 25.0   1  150  36   2    1\n100 894 1503.55   1    1    1   1  0    0 25.0    0 25.0   1  150  35   2    1\n101 895 2027.60   1    1    1   1  0    0 25.0    0 25.0   1  150  28   2    2\n102 896 3046.72   1    1    1   1  0    0 50.0    0 50.0   1  300  24   1    1\n103 897 2485.00   1    1    1   1  0    0 50.0    0 50.0   1  300  29   1    1\n104 898 1731.80   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1   88\n105 899 1958.27   1    1    1   1  0    0 25.0    0 25.0   1  150  26   1    1\n106 900 2996.40   1    1    1   1  0    0 50.0    0 50.0   1  300  26   1    1\n107 901 1288.64   1    1    1   1  0    0 25.0    0 25.0   1  150  28   1    2\n108 902 2353.40   1    1    1   1  0    0 50.0    0 50.0   1  300  37   1    2\n109 903 3016.30   1    1    1   1  0    0 50.0    0 50.0   1  300  45   1    1\n110 905 3306.15   1    1    1   1  0    0 50.0    0 50.0   1  300  38   1    1\n111 906  826.43   1    1    1   1  0    0 25.0    0 25.0   1  150  30   1    2\n112 907 1338.20   1    1    1   1  0    0 25.0    0 25.0   1  150  40   1    1\n113 908 1490.93   1    1    1   1  0    0 25.0    0 25.0   1  150  41   1    7\n114 909 1067.56   1    1    1   1  0    0 25.0    0 25.0   1  150  37   1    1\n115 910 2472.90   1    1    1   1  0    0 50.0    0 50.0   1  300  36   1    1\n116 911 1085.93   1    1    1   1  0    0 25.0    0 25.0   1  150  41   1    1\n117 912 2278.97   1    1    1   1  0    0 50.0    0 50.0   1  300  28   1    1\n118 913 1898.00   1    1    1   1  0    0 25.0    0 25.0   1  150  27   1    2\n119 914 1208.74   1    1    1   1  0    0 25.0    0 25.0   1  150  31   1    1\n120 915 3593.55   1    1    1   1  0    0 50.0    0 50.0   1  300  23   1    1\n       WT       HT\n1    94.3 1.769997\n2    80.4 1.759850\n3    71.8 1.809847\n4    77.4 1.649993\n5    64.3 1.560052\n6    74.1 1.829862\n7    87.9 1.850107\n8    61.9 1.730095\n9    65.3 1.649839\n10  103.5 1.840020\n11   83.0 1.779870\n12   68.7 1.700058\n13   64.4 1.560084\n14   69.8 1.640057\n15   86.1 1.910054\n16   84.5 1.770060\n17   99.1 1.809982\n18   71.2 1.669993\n19   82.6 1.830179\n20   85.4 1.860186\n21   76.1 1.700070\n22   78.3 1.719903\n23   73.3 1.690144\n24  102.1 1.809858\n25   79.5 1.749972\n26   97.5 1.849933\n27   80.7 1.659881\n28   83.2 1.740016\n29   58.0 1.580118\n30   85.7 1.770157\n31   74.2 1.749948\n32   70.4 1.640153\n33   78.9 1.740008\n34   89.3 1.689903\n35   96.8 1.900156\n36   74.8 1.650143\n37   85.4 1.640050\n38   79.5 1.620071\n39   99.1 1.809982\n40   88.3 1.759875\n41   91.6 1.740143\n42   69.2 1.810115\n43   92.0 1.690046\n44   80.5 1.879841\n45   85.9 1.719881\n46   94.5 1.779874\n47  101.8 1.829941\n48  102.7 1.879912\n49   56.6 1.740218\n50   70.7 1.780164\n51   81.6 1.650067\n52   90.6 1.789882\n53   83.8 1.680030\n54   82.1 1.810010\n55   78.7 1.790083\n56   79.8 1.859849\n57   74.9 1.760000\n58   73.2 1.729925\n59   85.2 1.809879\n60   90.1 1.779908\n61   99.8 1.800072\n62   58.4 1.660126\n63   69.4 1.820068\n64   73.1 1.749978\n65   88.0 1.840086\n66   80.3 1.699861\n67   99.3 1.799946\n68   96.0 1.909821\n69  102.7 1.809949\n70   71.4 1.679921\n71   86.0 1.680014\n72   89.3 1.719864\n73   72.4 1.840202\n74   70.4 1.740103\n75   74.3 1.800082\n76  102.5 1.909892\n77   72.7 1.710081\n78   97.3 1.800026\n79   75.7 1.780081\n80   81.0 1.749966\n81   66.2 1.800092\n82   80.9 1.810146\n83   92.2 1.799898\n84   92.9 1.760028\n85   58.2 1.619872\n86   69.8 1.520031\n87   69.6 1.580004\n88   62.3 1.750123\n89   63.6 1.719924\n90   75.1 1.850127\n91   93.8 1.780146\n92   85.9 1.870176\n93   87.1 1.839879\n94   81.9 1.770065\n95   78.6 1.740039\n96   68.8 1.810025\n97   81.0 1.780172\n98   84.6 1.710058\n99   88.2 1.710089\n100  90.0 1.679941\n101  58.9 1.580133\n102  90.1 1.799956\n103 115.3 1.930120\n104  72.2 1.700092\n105  70.5 1.679958\n106  82.1 1.780073\n107  77.5 1.819881\n108  99.1 1.779932\n109  90.0 1.730034\n110  83.4 1.819932\n111 105.1 1.879883\n112  97.3 1.860153\n113  85.8 1.789940\n114  85.4 1.820067\n115  84.4 1.730002\n116  77.1 1.589927\n117 113.2 1.909995\n118  89.0 1.829859\n119 110.8 1.869859\n120  96.3 1.820081\n\nsummary(merged_mavoglurant)\n\n       ID              Y               CMT         EVID        EVI2  \n Min.   :793.0   Min.   : 826.4   Min.   :1   Min.   :1   Min.   :1  \n 1st Qu.:822.8   1st Qu.:1700.5   1st Qu.:1   1st Qu.:1   1st Qu.:1  \n Median :853.5   Median :2349.1   Median :1   Median :1   Median :1  \n Mean   :853.7   Mean   :2445.4   Mean   :1   Mean   :1   Mean   :1  \n 3rd Qu.:884.2   3rd Qu.:3050.2   3rd Qu.:1   3rd Qu.:1   3rd Qu.:1  \n Max.   :915.0   Max.   :5606.6   Max.   :1   Max.   :1   Max.   :1  \n      MDV          DV         LNDV        AMT             TIME  \n Min.   :1   Min.   :0   Min.   :0   Min.   :25.00   Min.   :0  \n 1st Qu.:1   1st Qu.:0   1st Qu.:0   1st Qu.:25.00   1st Qu.:0  \n Median :1   Median :0   Median :0   Median :37.50   Median :0  \n Mean   :1   Mean   :0   Mean   :0   Mean   :36.46   Mean   :0  \n 3rd Qu.:1   3rd Qu.:0   3rd Qu.:0   3rd Qu.:50.00   3rd Qu.:0  \n Max.   :1   Max.   :0   Max.   :0   Max.   :50.00   Max.   :0  \n      DOSE            OCC         RATE            AGE             SEX       \n Min.   :25.00   Min.   :1   Min.   : 75.0   Min.   :18.00   Min.   :1.000  \n 1st Qu.:25.00   1st Qu.:1   1st Qu.:150.0   1st Qu.:26.00   1st Qu.:1.000  \n Median :37.50   Median :1   Median :225.0   Median :31.00   Median :1.000  \n Mean   :36.46   Mean   :1   Mean   :218.1   Mean   :33.00   Mean   :1.133  \n 3rd Qu.:50.00   3rd Qu.:1   3rd Qu.:300.0   3rd Qu.:40.25   3rd Qu.:1.000  \n Max.   :50.00   Max.   :1   Max.   :300.0   Max.   :50.00   Max.   :2.000  \n      RACE            WT               HT       \n Min.   : 1.0   Min.   : 56.60   Min.   :1.520  \n 1st Qu.: 1.0   1st Qu.: 73.17   1st Qu.:1.700  \n Median : 1.0   Median : 82.10   Median :1.770  \n Mean   : 7.2   Mean   : 82.55   Mean   :1.759  \n 3rd Qu.: 2.0   3rd Qu.: 90.10   3rd Qu.:1.813  \n Max.   :88.0   Max.   :115.30   Max.   :1.930  \n\n\nWe only kept the Time = 0 data points and so our data for time looks strange, but this is expected.\nWe want to delete unwanted columns, done with the select function, and also need to convert RACE and SEX to factors.\n\n#using the select function to delete unwanted columns\nmerged_mavoglurant_1 &lt;- merged_mavoglurant |&gt; select(-OCC , -EVID, -CMT, -EVI2, -MDV, -LNDV, -ID, -AMT, -RATE, -DV, -TIME) |&gt; mutate(RACE = as.factor(RACE), SEX = as.factor(SEX))\n\nsaveRDS(merged_mavoglurant_1, file = \"processed_mavoglurant.rds\")\n\nchecking our data and comparing two different summaries (summary vs skim) and functions for tables (kable vs pander).\n\ntable1p &lt;- pander::pander(summary(merged_mavoglurant_1))\n\ntable1k &lt;- knitr::kable(skim(merged_mavoglurant_1))\n\nprint(table1p)\n\n[1] \"\\n----------------------------------------------------------------\\n       Y              DOSE             AGE         SEX    RACE  \\n---------------- --------------- --------------- ------- -------\\n Min.  : 826.4    Min.  :25.00    Min.  :18.00    1:104   1 :74 \\n\\n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:26.00   2: 16   2 :36 \\n\\n Median :2349.1   Median :37.50   Median :31.00    NA     7 : 2 \\n\\n  Mean :2445.4     Mean :36.46     Mean :33.00     NA     88: 8 \\n\\n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:40.25    NA      NA   \\n\\n Max.  :5606.6    Max.  :50.00    Max.  :50.00     NA      NA   \\n----------------------------------------------------------------\\n\\nTable: Table continues below\\n\\n \\n--------------------------------\\n       WT              HT       \\n---------------- ---------------\\n Min.  : 56.60    Min.  :1.520  \\n\\n 1st Qu.: 73.17   1st Qu.:1.700 \\n\\n Median : 82.10   Median :1.770 \\n\\n  Mean : 82.55     Mean :1.759  \\n\\n 3rd Qu.: 90.10   3rd Qu.:1.813 \\n\\n Max.  :115.30    Max.  :1.930  \\n--------------------------------\\n\\n\"\nattr(,\"class\")\n[1] \"knit_asis\"\nattr(,\"knit_cacheable\")\n[1] NA\n\nprint(table1k)\n\n\n\n|skim_type |skim_variable | n_missing| complete_rate|factor.ordered | factor.n_unique|factor.top_counts         | numeric.mean|  numeric.sd| numeric.p0| numeric.p25| numeric.p50| numeric.p75| numeric.p100|numeric.hist |\n|:---------|:-------------|---------:|-------------:|:--------------|---------------:|:-------------------------|------------:|-----------:|----------:|-----------:|-----------:|-----------:|------------:|:------------|\n|factor    |SEX           |         0|             1|FALSE          |               2|1: 104, 2: 16             |           NA|          NA|         NA|          NA|          NA|          NA|           NA|NA           |\n|factor    |RACE          |         0|             1|FALSE          |               4|1: 74, 2: 36, 88: 8, 7: 2 |           NA|          NA|         NA|          NA|          NA|          NA|           NA|NA           |\n|numeric   |Y             |         0|             1|NA             |              NA|NA                        |   2445.40733| 961.6351038| 826.430000| 1700.525000| 2349.140000|  3050.21500|   5606.58000|▆▇▆▂▁        |\n|numeric   |DOSE          |         0|             1|NA             |              NA|NA                        |     36.45833|  11.8622315|  25.000000|   25.000000|   37.500000|    50.00000|     50.00000|▇▁▂▁▆        |\n|numeric   |AGE           |         0|             1|NA             |              NA|NA                        |     33.00000|   8.9761589|  18.000000|   26.000000|   31.000000|    40.25000|     50.00000|▅▇▃▅▅        |\n|numeric   |WT            |         0|             1|NA             |              NA|NA                        |     82.55417|  12.5211881|  56.600000|   73.175000|   82.100000|    90.10000|    115.30000|▂▇▇▅▁        |\n|numeric   |HT            |         0|             1|NA             |              NA|NA                        |      1.75901|   0.0855081|   1.520031|    1.700087|    1.770111|     1.81258|      1.93012|▁▃▆▇▃        |\n\n\nThese tables give a nice overview of the data and the small histograms for the numeric data are nice to get a very rough feel of the data.\nWe will generate a scatterplot to evaluate the total drug (Y) vs AGE.\n\nggplot(merged_mavoglurant_1, aes(x = AGE, y = Y)) +\n  geom_point(alpha = 0.6, color = \"blue\") +  # Add scatter points\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\") +  # Add trend line\n  labs(title = \"Scatterplot of Y vs Age\", x = \"Age\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSince we only kept the kept the points where time is equal to zero this scatter plot does not tell us much information about a progressing total drug vs time relationship but only shows the total drug for all participants at time zero.\nWe will repeat the scatterplot to evaluate the total drug (Y) vs dose.\n\nggplot(merged_mavoglurant_1, aes(x = DOSE, y = Y)) +\n  geom_point(alpha = 0.6, color = \"green\") +  # Add scatter points\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\") +  # Add trend line\n  labs(title = \"Scatterplot of Y vs Dose\", x = \"Dose\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: pseudoinverse used at 24.875\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: neighborhood radius 25.125\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: reciprocal condition number 2.903e-16\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: There are other near singularities as well. 631.27\n\n\n\n\n\n\n\n\n\nWe now see the pattern of increasing total drug with increasing dose (as expected).\nWe will generate a boxplot with jitter points to evaluate the total drug (Y) vs sex.\n\nggplot(merged_mavoglurant_1, aes(x = as.factor(SEX), y = Y)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7) +  # Boxplot\n  geom_jitter(alpha = 0.4, color = \"black\", width = 0.2) +  # Add jitter points\n  labs(title = \"Boxplot of Y by SEX\", x = \"SEX\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis would be more helpful if we knew which sex was reflected by which number since there are differences noted.\nWe will repeat a boxplot to evaluate the total drug (Y) vs dose.\n\nggplot(merged_mavoglurant_1, aes(x = as.factor(DOSE), y = Y)) +\n  geom_boxplot(fill = \"lightgreen\", alpha = 0.7) +  # Boxplot\n  geom_jitter(alpha = 0.4, color = \"black\", width = 0.2) +  # Add jitter points\n  labs(title = \"Boxplot of Y by Dose\", x = \"Dose\", y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing ChatGPT to generate code for plots of the distributions of variables\n\nvariables &lt;- c(\"Y\", \"DOSE\", \"AGE\", \"SEX\", \"RACE\", \"WT\", \"HT\")\n\n# Function to create distribution plots\nplot_distribution &lt;- function(data, var) {\n  if (is.numeric(data[[var]])) {\n    # Numeric variables: Histogram & Density Plot\n    ggplot(data, aes(x = .data[[var]])) +\n      geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n      geom_density(color = \"red\", size = 1) + \n      labs(title = paste(\"Distribution of\", var), x = var, y = \"Density\") +\n      theme_minimal()\n  } else {\n    # Categorical variables: Bar Plot\n    ggplot(data, aes(x = .data[[var]])) +\n      geom_bar(fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n      labs(title = paste(\"Distribution of\", var), x = var, y = \"Count\") +\n      theme_minimal()\n  }\n}\n\n# Plot distributions for each variable\nplots &lt;- lapply(variables, function(var) plot_distribution(merged_mavoglurant_1, var))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Display all plots together\nlibrary(patchwork)\nwrap_plots(plots, ncol = 2)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nWe get a good feel for the data for each variable here. However, because the source data is not clear we do not know what sex and race corresponds with the respective numbers. However, we can see very clear distributions favouring particular categories for these variables. The age, weight, height, and total drug (Y) data are more dispersed.\nWe want to look at a correlation plot of the data. We will use a correlation matrix:\n\n# Compute correlation matrix (select only numeric variables)\ncor_matrix &lt;- merged_mavoglurant_1 |&gt;\n  select(Y, DOSE, AGE, WT, HT) |&gt;\n  cor(use = \"pairwise.complete.obs\")  # Handle missing values properly\n\n# Plot the correlation matrix\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", \n         tl.col = \"black\", tl.cex = 0.8, addCoef.col = \"black\", number.cex = 0.7)\n\n\n\n\n\n\n\n\nThis plot provides a nice overview to look at the interactions/correlations between different variables. From this, there does not seem to be much correlation between the drug data and the biological data of participants.\n##Model Fitting\nWe need to generate some models for our tidied data. I have used chatgpt to help me to generate code.\nFirst we fit a linear model to the total drug (Y = continuous outcome) usings DOSE as our main outcome of interest.\n\n#We need to set a seed for reproducibility of our model\nset.seed(123)\n\n#We define the linear regression model\nlm_spec_dose &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Define a recipe (preprocessing steps)\nlm_recipe_dose &lt;- recipe(Y ~ DOSE, data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())  # Standardizes numeric predictors to account for different scales between vaariables \n\n# Create a workflow\nlm_workflow_dose &lt;- workflow() |&gt;\n  add_model(lm_spec_dose) |&gt;\n  add_recipe(lm_recipe_dose)\n\n#Fitting the model to the dataset\nlm_fit_dose &lt;- lm_workflow_dose |&gt;\n  fit(data = merged_mavoglurant_1)\n\n# View model summary\nlm_fit_dose |&gt; extract_fit_engine() |&gt; print() |&gt; summary()\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     2445.4        690.5  \n\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1284.91  -441.14   -97.22   325.84  2372.87 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2445.41      61.35   39.86   &lt;2e-16 ***\nDOSE          690.53      61.61   11.21   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 672.1 on 118 degrees of freedom\nMultiple R-squared:  0.5156,    Adjusted R-squared:  0.5115 \nF-statistic: 125.6 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\n\nAs expected, we see a positive association between the dose and the total drug (Y), with an increase with Y for increasing units of dose.\nWe will now generate a plot to visualise the model fit as well (Dose vs Total drug)\n\nggplot(merged_mavoglurant_1, aes(x = DOSE, y = Y)) +\n  geom_point(alpha = 0.5, color = \"blue\") +  # Scatterplot of actual data\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +  # Regression line\n  labs(title = \"Linear Regression: Y vs DOSE\", \n       x = \"DOSE\", \n       y = \"Total Drug (Y)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis confirms what we have seen in the data and would expect from these variables.\nWe will now compute the RMSE and R squared for this linear model looking at Total drug (Y) and Dose. The model will produce prediction values and compares these to actual values.\n\n# Generate predictions\nlm_predictions_dose &lt;- predict(lm_fit_dose, merged_mavoglurant_1) |&gt;\n  bind_cols(merged_mavoglurant_1)  # Adds actual values of Y for comparison\n\n# Compute RMSE and R²\nlm_metrics_dose &lt;- lm_predictions_dose |&gt;\n  metrics(truth = Y, estimate = .pred) |&gt;\n  filter(.metric %in% c(\"rmse\", \"rsq\"))  # Select RMSE & R²\n\n# Print results\nprint(lm_metrics_dose)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n\n\nOur model seems to perform moderately from this data.\nWe will now fit a linear model to the total drug (Y = continuous outcome) using all of the predictors. The predictors are standardised in the process to put them all on the same scale.\n\n# Define the linear regression model\nlm_spec_all_predict &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n# Define the recipe (preprocessing steps)\nlm_recipe_all_predict &lt;- recipe(Y ~ ., data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())  # Standardise numeric predictors \n\n# Create a workflow\nlm_workflow_all_predict &lt;- workflow() |&gt;\n  add_model(lm_spec_all_predict) |&gt;\n  add_recipe(lm_recipe_all_predict)\n\n\n# Fit the model using the entire dataset\nlm_fit_all_predict &lt;- lm_workflow_all_predict |&gt;\n  fit(data = merged_mavoglurant_1)\n\n# View model summary\nlm_fit_all_predict |&gt; extract_fit_engine() |&gt; print()|&gt; summary()\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2        RACE2        RACE7  \n    2456.92       710.96        28.32      -357.73       155.03      -405.32  \n     RACE88           WT           HT  \n     -53.51      -288.57       -64.00  \n\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1480.86  -367.81   -79.95   266.15  2431.52 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2456.92      80.58  30.490  &lt; 2e-16 ***\nDOSE          710.96      57.89  12.281  &lt; 2e-16 ***\nAGE            28.32      70.24   0.403 0.687530    \nSEX2         -357.73     216.93  -1.649 0.101957    \nRACE2         155.03     128.63   1.205 0.230650    \nRACE7        -405.32     448.19  -0.904 0.367768    \nRACE88        -53.51     244.67  -0.219 0.827296    \nWT           -288.57      80.07  -3.604 0.000471 ***\nHT            -64.00      94.40  -0.678 0.499188    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 614.3 on 111 degrees of freedom\nMultiple R-squared:  0.6193,    Adjusted R-squared:  0.5919 \nF-statistic: 22.57 on 8 and 111 DF,  p-value: &lt; 2.2e-16\n\n\nFrom these measures we once again see a strong association between dose and total drug. We now see that weight has a negative asociation with the total drug (as weight increases, Y decreases). There do not seem to be associations with age, height and race.\ncomputing R-squared and RMSE for the linear model to the total drug and all predictors.\n\n# Generate predictions\nlm_predictions_all_predict &lt;- predict(lm_fit_all_predict, merged_mavoglurant_1) |&gt;\n  bind_cols(merged_mavoglurant_1)\n\n# Compute RMSE and R-squared (and mae)\nlm_metrics_all_predict &lt;- lm_predictions_all_predict |&gt;\n  metrics(truth = Y, estimate = .pred)\n\nprint(lm_metrics_all_predict)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     591.   \n2 rsq     standard       0.619\n3 mae     standard     444.   \n\n\nOur RMSE is lower in this model which indicates slightly improved performance.\n\nggplot(lm_predictions_all_predict, aes(x = Y, y = .pred)) +\n  geom_point(alpha = 0.5, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Linear Model: Actual vs Predicted Y\", x = \"Actual Y\", y = \"Predicted Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe will now fit a logistic model for sex (a categorical/binary outcome) with DOSE as the main predictor of interest.\n\n# Define logistic regression model (SEX ~ DOSE)\nlog_spec_sex_dose &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n# Define a recipe (preprocessing steps)\nlog_recipe_sex_dose &lt;- recipe(SEX ~ DOSE, data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())\n\n# Create workflow\nlog_workflow_sex_dose &lt;- workflow() |&gt;\n  add_model(log_spec_sex_dose) |&gt;\n  add_recipe(log_recipe_sex_dose)\n\n# Fit the model\nlog_fit_sex_dose &lt;- log_workflow_sex_dose |&gt;\n  fit(data = merged_mavoglurant_1)\n\ncomputing ROC-AUC for the sex - dose logistic model and printing the results\n\n# Use augment() to get predictions (both probabilities and classes)\nlog_predictions_sex_dose &lt;- augment(log_fit_sex_dose, new_data = merged_mavoglurant_1)\n\n# Compute ROC AUC and Accuracy\nlog_metrics_sex_dose &lt;- log_predictions_sex_dose |&gt;\n  roc_auc(truth = SEX, .pred_1) |&gt;\n  bind_rows(\n    log_predictions_sex_dose |&gt;\n      accuracy(truth = SEX, estimate = .pred_class)\n  )\n\n# Print results\nprint(log_metrics_sex_dose)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc  binary         0.592\n2 accuracy binary         0.867\n\n\nThe ROC-AUC i not good and indicates that there is poor discrimination (it is basically random). Thus dose is not a good predicotr of sex (as expected). Even though we see a high accuracy, this is most likely due to the imbalance of sexes in the dataset.\n\n# Generate predictions with probabilities\nlog_predictions_sex_dose &lt;- predict(log_fit_sex_dose, merged_mavoglurant_1, type = \"prob\") |&gt;\n  bind_cols(merged_mavoglurant_1)\n\n# Plot logistic regression curve\nggplot(log_predictions_sex_dose, aes(x = DOSE, y = .pred_1)) +\n  geom_point(alpha = 0.4, color = \"blue\") +  # Scatterplot of individual predictions\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), color = \"red\") +\n  labs(title = \"Logistic Regression: Probability of SEX by DOSE\",\n       x = \"DOSE\", \n       y = \"Predicted Probability of SEX\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis curve might be misleading as it shows a positive relationship between dose and predicted probability of sex, which might be occuring due to imbalances in our data.\nWe will then fit a logistic model to sex using all predictors as we did with the linear model.\n\n# Define logistic regression model (SEX ~ all predictors)\nlog_spec_sex_predictors &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n# Define recipe\nlog_recipe_sex_predictors &lt;- recipe(SEX ~ ., data = merged_mavoglurant_1) |&gt;\n  step_normalize(all_numeric_predictors())\n\n# Create workflow\nlog_workflow_sex_predictors &lt;- workflow() |&gt;\n  add_model(log_spec_sex_predictors) |&gt;\n  add_recipe(log_recipe_sex_predictors)\n\n# Fit the model\nlog_fit_sex_predictors &lt;- log_workflow_sex_predictors |&gt;\n  fit(data = merged_mavoglurant_1)\n\nComputing ROC-AUC for the sex - all predictors logistic model and printing the results.\n\n# Generate predictions (both probabilities & classes)\nlog_predictions_sex_predictors &lt;- augment(log_fit_sex_predictors, new_data = merged_mavoglurant_1)\n\n# Compute ROC AUC and Accuracy\nlog_metrics_sex_predictors &lt;- log_predictions_sex_predictors |&gt;\n  roc_auc(truth = SEX, .pred_1) |&gt;\n  bind_rows(\n    log_predictions_sex_predictors |&gt;\n      accuracy(truth = SEX, estimate = .pred_class)\n  )\n\n# Print results\nprint(log_metrics_sex_predictors)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc  binary         0.980\n2 accuracy binary         0.942\n\n\nOur model performed much better when looking at the other predictors as well. It had almost perfect performance at predicting sex data. Perhaps this is once again due to the skewed sex dataset and a degree of overfitting. More data points would be valauble to make sure that this is not the case and that this is based purely on the biological relationships.\n\n\n\n\n#Week 10 Assessment - Model Improvement\n##Part 1\nWe want to delete unwanted RACE column, done with the select function.\n\n#using the select function to delete unwanted Race column\nmodel_mavoglurant &lt;- merged_mavoglurant_1 |&gt; select(-RACE)\nskim(model_mavoglurant)\n\n\nData summary\n\n\nName\nmodel_mavoglurant\n\n\nNumber of rows\n120\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSEX\n0\n1\nFALSE\n2\n1: 104, 2: 16\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nY\n0\n1\n2445.41\n961.64\n826.43\n1700.53\n2349.14\n3050.22\n5606.58\n▆▇▆▂▁\n\n\nDOSE\n0\n1\n36.46\n11.86\n25.00\n25.00\n37.50\n50.00\n50.00\n▇▁▂▁▆\n\n\nAGE\n0\n1\n33.00\n8.98\n18.00\n26.00\n31.00\n40.25\n50.00\n▅▇▃▅▅\n\n\nWT\n0\n1\n82.55\n12.52\n56.60\n73.18\n82.10\n90.10\n115.30\n▂▇▇▅▁\n\n\nHT\n0\n1\n1.76\n0.09\n1.52\n1.70\n1.77\n1.81\n1.93\n▁▃▆▇▃\n\n\n\n\nsaveRDS(model_mavoglurant, file = \"cleaned_mavoglurant.rds\")\n\nSet the seed for further modeling steps:\n\nrngseed = 1234\nset.seed(rngseed)\n\nCreating a random split of the data: 75% train and 25% test set.\n\n# Split the data into training (75%) and testing (25%) sets\ndata_split &lt;- initial_split(model_mavoglurant, prop = 0.75)\n\n# Extract training and testing datasets\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Verify split\ndim(train_data)  # Check dimensions of training data\n\n[1] 90  6\n\ndim(test_data)   # Check dimensions of testing data\n\n[1] 30  6\n\n\nModel Fitting:\nWe will generate two linear models for the continuous outcome of interest (Y). The first with Dose as a predictor and the secodn with all predictors. Metric to optimise is RMSE.\nSetting up the linear regressions and running them:\n\n# Define a linear regression model specification\nmodel_lm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n#Model 1 Simple Linear Model (Only DOSE as Predictor)\nmodel_lm_simple &lt;- workflow() %&gt;%\n  add_model(model_lm_spec) %&gt;%\n  add_formula(Y ~ DOSE) %&gt;%\n  fit(data = train_data)\n\n#Model 2 Full model (All Predictors)\nmodel_lm_full &lt;- workflow() %&gt;%\n  add_model(model_lm_spec) %&gt;%\n  add_formula(Y ~ .) %&gt;%  # Includes all predictors\n  fit(data = train_data)\n\nWe will now use the models to make predictions from the training data that we assigned. We will combine this with our original data.\n\n# Compute Predictions on Training Data and combine this with the original data using bind_col\ntrain_preds_simple &lt;- predict(model_lm_simple, new_data = train_data) %&gt;%\n  bind_cols(train_data) \n\ntrain_preds_full &lt;- predict(model_lm_full, new_data = train_data) %&gt;%\n  bind_cols(train_data)\n\nWe will now compute the performance of both of the models using RMSE\n\n#Compute RMSE for Both Models\nrmse_simple_model &lt;- train_preds_simple %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\")\n\nrmse_full_model &lt;- train_preds_full %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\")\n\nWe now compute the RMSE for the null model (one that predcits the mean outcome for each observation)\n\n#Compute RMSE for null model Using tidymodels null_model function\nnull_spec &lt;- null_model() %&gt;%\n  set_engine(\"parsnip\") %&gt;% #using tidymodels parsnip engine\n  set_mode(\"regression\")\n\nnull_fit &lt;- null_spec %&gt;% fit(Y ~ 1, data = train_data) #This tells our model to use the intercept (no predictor values)\n\ntrain_preds_null &lt;- predict(null_fit, new_data = train_data) %&gt;%\n  bind_cols(train_data)\n\nrmse_null &lt;- train_preds_null %&gt;%\n  metrics(truth = Y, estimate = .pred) %&gt;%\n  filter(.metric == \"rmse\")\n\nWarning: A correlation computation is required, but `estimate` is constant and has 0\nstandard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nWe will display all of the calculated RMSE values to compare them\n\n#Combine and print RMSE values \nrmse_results &lt;- bind_rows(\n  rmse_null %&gt;% mutate(model = \"Null Model\"),\n  rmse_simple_model %&gt;% mutate(model = \"Model 1 (DOSE Only)\"),\n  rmse_full_model %&gt;% mutate(model = \"Model 2 (All Predictors)\")\n)\n\nprint(rmse_results)\n\n# A tibble: 3 × 4\n  .metric .estimator .estimate model                   \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                   \n1 rmse    standard        948. Null Model              \n2 rmse    standard        703. Model 1 (DOSE Only)     \n3 rmse    standard        627. Model 2 (All Predictors)\n\n\nThrough these steps we see that we get the outline values for the RMSE from the three models of 948, 702 and 627 for the null model and models 1 and 2 respectively. Thus both model one and model two seem to perform better than the the null model based on the RMSE values, with model 2 performing best.\n##Model performance assessment 2\nWe are going to do CV tests to evaluate the model performance and the recalculate the RMSE and evaluate the standard error from this. We will end with changing our set seed to see how this affects our results.\nWe will set the same seed as before to start:\n\nrngseed = 1234\nset.seed(rngseed)\n\nCreating our CV splits and showing the variable like in the tutorial.\n\n#Create 10-Fold Cross-Validation Splits\ncv_splits &lt;- vfold_cv(train_data, v = 10)  \n\ncv_splits\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10\n\n\nWe will now recreate the linear models using the CV splits. We follow a similar approach as we have used for the previous models.\n\n#Define a Linear Regression Model Specification\ncv_model_lm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")  # Linear regression model\n\n#starting with the simple model first\n\n#model 1 simple model using DOSE only\nworkflow_simple &lt;- workflow() %&gt;%\n  add_model(cv_model_lm_spec) %&gt;%\n  add_formula(Y ~ DOSE)\n\n#actually usign the cv splits we created\ncv_results_simple &lt;- workflow_simple %&gt;%\n  fit_resamples(\n    resamples = cv_splits,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE))\n\n#model 2 Full Model using ALL Predictors\nworkflow_full &lt;- workflow() %&gt;%\n  add_model(cv_model_lm_spec) %&gt;%\n  add_formula(Y ~ .)\n\n#actually using our created cv splits\ncv_results_full &lt;- workflow_full %&gt;%\n  fit_resamples(\n    resamples = cv_splits,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE))\n\nWe will first collect and compute the RMSE and SE per fold for each model for comparison.\n\n# Extract RMSE per fold for Model 1 (DOSE only)\nrmse_per_fold_simple &lt;- cv_results_simple %&gt;%\n  collect_predictions() %&gt;%  # Extract per-fold predictions\n  group_by(id) %&gt;%           # Group by fold ID\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))  # Compute RMSE per fold\n\n# Extract RMSE per fold for Model 2 (All predictors)\nrmse_per_fold_full &lt;- cv_results_full %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))\n\n# Print RMSE per fold\nprint(\"RMSE per fold for Model 1 (DOSE only):\")\n\n[1] \"RMSE per fold for Model 1 (DOSE only):\"\n\nprint(rmse_per_fold_simple)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      602.\n 2 Fold02      353.\n 3 Fold03      804.\n 4 Fold04      986.\n 5 Fold05      601.\n 6 Fold06      969.\n 7 Fold07      516.\n 8 Fold08      739.\n 9 Fold09      483.\n10 Fold10      852.\n\nprint(\"RMSE per fold for Model 2 (All predictors):\")\n\n[1] \"RMSE per fold for Model 2 (All predictors):\"\n\nprint(rmse_per_fold_full)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      745.\n 2 Fold02      480.\n 3 Fold03      617.\n 4 Fold04      971.\n 5 Fold05      403.\n 6 Fold06      765.\n 7 Fold07      454.\n 8 Fold08      522.\n 9 Fold09      540.\n10 Fold10      959.\n\n# Compute Mean RMSE and Standard Error (SE) across folds\nsummary_simple &lt;- rmse_per_fold_simple %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))  # SE = SD / sqrt(N)\n\nsummary_full &lt;- rmse_per_fold_full %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))\n\n# Print Summary Statistics\nprint(\"Summary RMSE for Model 1 (DOSE only) - manual per-pold calculation\")\n\n[1] \"Summary RMSE for Model 1 (DOSE only) - manual per-pold calculation\"\n\nprint(summary_simple)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    67.5\n\nprint(\"Summary RMSE for Model 2 (All predictors) - manual per-fold alculation\")\n\n[1] \"Summary RMSE for Model 2 (All predictors) - manual per-fold alculation\"\n\nprint(summary_full)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    64.8\n\n\nWe see substantial variation between the mean rmse for each fold. This highlights the value of checking each fold and not just the summarised mean rmse from all folds if you want to check variability.\nWe will also compute (collect) RMSE & standard error (SE) for each model automatically using tidymodels and so we are collecting the mean and SE of the overall RMSE (single value). This is calculated across all folds and not per fold.\n\n#Extract RMSE and SE values for Model 1 (DOSE only)\nrmse_cv_simple &lt;- cv_results_simple %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)  # `std_err` from tidymodels\n\n# Extract RMSE values for model 2 - All predictors\nrmse_cv_full &lt;- cv_results_full %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)\n\n\n\n#Print RMSE and standard errors \nprint(\"RMSE from 10-Fold cross-validation (DOSE only) with SE\")\n\n[1] \"RMSE from 10-Fold cross-validation (DOSE only) with SE\"\n\nprint(rmse_cv_simple)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  691.    67.5\n\nprint(\"RMSE from 10-Fold cross-validation (All predictors) with SE\")\n\n[1] \"RMSE from 10-Fold cross-validation (All predictors) with SE\"\n\nprint(rmse_cv_full)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  646.    64.8\n\n\nThis automatic approach however is preferred if you want a quick overview because we can compare the mean and SE collectively to assess each model. We can summise that the model with all variables included performs better than the simple model with only Dose as a predictor based on the decreased RMSE and SE.\nWe will now change the random seed & re-run cross-validation using the automatic approach first followed by the manual approach for completeness. But the manual per fold values might be harder to compare.\n\nset.seed(999)  #change the seed to check how RMSE variability changes\n\n#Create New CV Splits\ncv_splits_new &lt;- vfold_cv(train_data, v = 10)\n\n# Re-run CV with new random seed\n\ncv_results_simple_new &lt;- workflow_simple %&gt;%\n  fit_resamples(\n    resamples = cv_splits_new,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE)\n  )\n\ncv_results_full_new &lt;- workflow_full %&gt;%\n  fit_resamples(\n    resamples = cv_splits_new,\n    metrics = metric_set(rmse),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Extract new RMSE values for Model 1 (DOSE only)\nrmse_cv_simple_new &lt;- cv_results_simple_new %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)\n\n# Extract new RMSE values for Model 2 (All predictors)\nrmse_cv_full_new &lt;- cv_results_full_new %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, std_err)\n\n# Print RMSE values from new seed\nprint(\"RMSE from New Random Seed (DOSE only)\")\n\n[1] \"RMSE from New Random Seed (DOSE only)\"\n\nprint(rmse_cv_simple_new)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  705.    45.7\n\nprint(\"RMSE from New Random Seed (All predictors)\")\n\n[1] \"RMSE from New Random Seed (All predictors)\"\n\nprint(rmse_cv_full_new)\n\n# A tibble: 1 × 2\n   mean std_err\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  648.    52.7\n\n\nWe see a marked change in the mean and SE of the RMSE from changing our seed to 999 instead of 1234.\nReruning the manual approach with the new seed:\n\n# Extract RMSE per fold for Model 1 (DOSE only) with new seed\nrmse_per_fold_simple_new &lt;- cv_results_simple_new %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))\n\n# Extract RMSE per fold for Model 2 (All predictors) with new seed\nrmse_per_fold_full_new &lt;- cv_results_full_new %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_rmse = rmse_vec(truth = Y, estimate = .pred))\n\n# Print RMSE per fold from new seed\nprint(\"RMSE per fold for Model 1 (DOSE only) - New Seed:\")\n\n[1] \"RMSE per fold for Model 1 (DOSE only) - New Seed:\"\n\nprint(rmse_per_fold_simple_new)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      652.\n 2 Fold02      601.\n 3 Fold03      610.\n 4 Fold04      545.\n 5 Fold05      741.\n 6 Fold06      972.\n 7 Fold07      543.\n 8 Fold08      898.\n 9 Fold09      760.\n10 Fold10      728.\n\nprint(\"RMSE per fold for Model 2 (All predictors) - New Seed:\")\n\n[1] \"RMSE per fold for Model 2 (All predictors) - New Seed:\"\n\nprint(rmse_per_fold_full_new)\n\n# A tibble: 10 × 2\n   id     mean_rmse\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Fold01      669.\n 2 Fold02      448.\n 3 Fold03      697.\n 4 Fold04      673.\n 5 Fold05      464.\n 6 Fold06     1017.\n 7 Fold07      508.\n 8 Fold08      753.\n 9 Fold09      574.\n10 Fold10      679.\n\n# Compute Standard Error (SE) across folds (with new seed)\nsummary_simple_new &lt;- rmse_per_fold_simple_new %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))\n\nsummary_full_new &lt;- rmse_per_fold_full_new %&gt;%\n  summarise(se_rmse = sd(mean_rmse) / sqrt(n()))\n\n# Print Summary Statistics from new seed\nprint(\"Standard Error (SE) for Model 1 (DOSE only) - New Seed:\")\n\n[1] \"Standard Error (SE) for Model 1 (DOSE only) - New Seed:\"\n\nprint(summary_simple_new)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    45.7\n\nprint(\"Standard Error (SE) for Model 2 (All predictors) - New Seed:\")\n\n[1] \"Standard Error (SE) for Model 2 (All predictors) - New Seed:\"\n\nprint(summary_full_new)\n\n# A tibble: 1 × 1\n  se_rmse\n    &lt;dbl&gt;\n1    52.7\n\n\nWe see similar differences here between he individual folds but this is harder to observe directly with the differences between folds. However, the SE clearly changed from both being in the 60s to neither of the values being in that range.\nFrom all of our checks it seems that model 2, which includes all factors, and ot just Dose, performs the best."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "ml-models-exercise",
    "section": "",
    "text": "ML Models Exercise\nLoading packages\n\n#install packages that are missing if necessary\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(skimr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tibble)\nlibrary(glmnet)\nlibrary(ranger)\nlibrary(purrr)\n\nSet the seed.\n\nrngseed = 1234\nset.seed(rngseed)\n\nLoading the dataset and double checking.\n\nmavoglurant_cleaned_data &lt;- here::here(\"ml-models-exercise\", \"processed_mavoglurant.rds\")\nmavoglurant_cleaned &lt;- readRDS(mavoglurant_cleaned_data)\nsummary(mavoglurant_cleaned)\n\n       Y               DOSE            AGE        SEX     RACE   \n Min.   : 826.4   Min.   :25.00   Min.   :18.00   1:104   1 :74  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:26.00   2: 16   2 :36  \n Median :2349.1   Median :37.50   Median :31.00           7 : 2  \n Mean   :2445.4   Mean   :36.46   Mean   :33.00           88: 8  \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:40.25                  \n Max.   :5606.6   Max.   :50.00   Max.   :50.00                  \n       WT               HT       \n Min.   : 56.60   Min.   :1.520  \n 1st Qu.: 73.17   1st Qu.:1.700  \n Median : 82.10   Median :1.770  \n Mean   : 82.55   Mean   :1.759  \n 3rd Qu.: 90.10   3rd Qu.:1.813  \n Max.   :115.30   Max.   :1.930  \n\nhead(mavoglurant_cleaned)\n\n        Y DOSE AGE SEX RACE   WT       HT\n1 2690.52   25  42   1    2 94.3 1.769997\n2 2638.81   25  24   1    2 80.4 1.759850\n3 2149.61   25  31   1    1 71.8 1.809847\n4 1788.89   25  46   2    1 77.4 1.649993\n5 3126.37   25  41   2    2 64.3 1.560052\n6 2336.89   25  27   1    2 74.1 1.829862\n\n\nMaking a third category for race out of the ctegoris 7 and 88.\n\n#Group levels \"7\" and \"88\" into new level \"3\"\nmavoglurant_cleaned_1 &lt;- mavoglurant_cleaned %&gt;%  mutate( RACE = fct_collapse(RACE, \"3\" = c(\"7\", \"88\")))  \n\nMaking a pairwise correlation plot using the continuous variables (including age which might not always be considered continuous)\n\n#Select continuous variables (including age)\nnumeric_data &lt;- mavoglurant_cleaned_1 %&gt;% select(WT, HT, AGE, Y)  \n\n#Compute Pearson correlation matrix\ncor_matrix &lt;- round(cor(numeric_data, use = \"pairwise.complete.obs\"), 2)\n\n#Convert matrix to long format\ncor_long &lt;- as.data.frame(cor_matrix) %&gt;% rownames_to_column(\"Var1\") %&gt;% pivot_longer(-Var1, names_to = \"Var2\", values_to = \"Correlation\")\n\n#Visualize the correlation matrix as a heatmap\nggplot(cor_long, aes(x = Var1, y = Var2, fill = Correlation)) +\n  geom_tile(color = \"white\") +  # Create colored tiles for each correlation pair\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0, limit = c(-1, 1),\n                       name = \"Pearson\\nCorrelation\") +  # Color scale\n  geom_text(aes(label = Correlation), size = 4) +  # Add numeric correlation values\n  labs(title = \"Correlation Plot: WT, HT, AGE, and Y\",x = NULL, y = NULL) +\n  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n#####Feature engineering using BMI\nThe formula for BMI is BMI= Weight/(height in meters)^2. We will assume that the weight in the dataset is given in kilograms and the height is in meters based on the values in the dataset (weight ranges between 56 and 116 (kg) and height between 1.52 and 1.93 (m) which are consistent will normal ranges in average adults).\nWe will calculate a variable for BMI:\n\n#Add BMI using WT (assuming that they are in kg) and HT (assuming that they are in meters)\nmavoglurant_cleaned_1 &lt;- mavoglurant_cleaned_1 %&gt;% mutate(BMI = WT / ((HT)^2))  \n#Check the range of BMI to confirm it's plausible\nsummary(mavoglurant_cleaned_1$BMI)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.69   24.54   26.38   26.63   29.70   32.21 \n\n\nThis range for BMI between 18.69 and 32.21 is realistic and so we suspect that our assumptions are correct.\n#####Model building:\nWe will create three models, a linear model, a LASSO model and a RF (random forest) model. The first stage will be without CV values before moving to include and evaluate with the CV values. After developing all three models we will evaluate them on the dataset and compare the RMSE values.\n#######Starting with the linear model:\nCreating a modeling recipe and variable starting out with our linear model with all predictions\n\n#Create a modeling dataset \nmodel_data &lt;- mavoglurant_cleaned_1\n\n#Define the recipe: Y ~ all other predictors\nlm_recipe &lt;- recipe(Y ~ ., data = model_data)\n\n#Define the linear model using parsnip\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\") %&gt;% set_mode(\"regression\")\n\n#Combine recipe and model into a workflow\nlm_workflow &lt;- workflow() %&gt;% add_recipe(lm_recipe) %&gt;% add_model(lm_model)\n\nFitting the linear model and seeing a summary:\n\n#Fit the model to the full dataset\nlm_fit &lt;- lm_workflow %&gt;% fit(data = model_data)\n\n#Extract fitted model object\nlm_fit_model &lt;- lm_fit %&gt;% extract_fit_parsnip()\nsummary(lm_fit_model$fit)\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1474.59  -350.58   -95.58   332.52  2459.99 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32051.213  10161.419   3.154  0.00207 ** \nDOSE            59.116      4.726  12.509  &lt; 2e-16 ***\nAGE              4.533      7.471   0.607  0.54525    \nSEX2          -434.999    209.974  -2.072  0.04061 *  \nRACE2          157.942    124.315   1.271  0.20656    \nRACE3         -257.500    214.859  -1.198  0.23329    \nWT             145.476     59.484   2.446  0.01603 *  \nHT          -16839.750   5727.944  -2.940  0.00400 ** \nBMI           -535.654    187.911  -2.851  0.00520 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 594.3 on 111 degrees of freedom\nMultiple R-squared:  0.6437,    Adjusted R-squared:  0.618 \nF-statistic: 25.07 on 8 and 111 DF,  p-value: &lt; 2.2e-16\n\n\nWe see an R squared of 0.64 which indicates that this is a statistically strong model. We previously assessed the model and found its performance to be good although we did identify some limitations.\nWe will move on to creating a LASSO regression model without any CV values.\n#######LASSO model:\nWe need to specify a new recipe for the LASSO model because LASSO does not work with variabes that are not numeric, and so we need to generate dummy categorical predictors to replace our categorical variables (Sex and Race) for our model. We include code to remove any constant predictors that might exist (we don’t seem to have any but this is good practice when assigning dummy predictors).\n\n#Provide dummy encoding for categorical variables in recipe\nlasso_recipe &lt;- recipe(Y ~ ., data = model_data) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%   \n  step_zv(all_predictors())                  #Remove zero-variance (constant) predictors if present\n\nWe specify the LASSO model and create the workflow with the parameters outlined\n\n#Define the LASSO model with penalty = 0.1 and mixture = 1 (LASSO)\nlasso_model &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;% set_engine(\"glmnet\") %&gt;% set_mode(\"regression\")\n\n\nlasso_workflow &lt;- workflow() %&gt;% add_recipe(lasso_recipe) %&gt;% add_model(lasso_model)\n\nMove on to fitting and viewing the coefficients of the model\n\n#fit the model\nlasso_fit &lt;- lasso_workflow %&gt;% fit(data = model_data)\n\n#View coefficients at penalty = 0.1\nlasso_fit_model &lt;- lasso_fit %&gt;% pull_workflow_fit()\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n\n#Extract coefficients (at lambda = 0.1)\ncoef(lasso_fit_model$fit, s = 0.1)\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  30579.025889\nDOSE            59.152528\nAGE              4.428095\nWT             136.796507\nHT          -16008.945383\nBMI           -508.305547\nSEX_X2        -430.708626\nRACE_X2        157.552133\nRACE_X3       -250.959842\n\n\n#######Random Forest (RF) model:\nWe will now work on the random forest model, starting with setting the recipe with dummy values encoded.\n\n# Recipe with dummy encoding and zero-variance removal\nrf_recipe &lt;- recipe(Y ~ ., data = model_data) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nWe need to set the seed again, using the same rngseed value.\n\nrngseed = 1234\nset.seed(rngseed)\n\nSetting the RF model and workflow.\n\n# Specify the random forest model using ranger\nrf_model &lt;- rand_forest() %&gt;% set_engine(\"ranger\", seed = rngseed) %&gt;% set_mode(\"regression\")\n\nrf_workflow &lt;- workflow() %&gt;% add_recipe(rf_recipe) %&gt;% add_model(rf_model)\n\nContinuing to fit the model and viewing a RF summary of this:\n\nrf_fit &lt;- rf_workflow %&gt;% fit(data = model_data)\n\n\n# Extract the fitted model\nrf_fit_model &lt;- rf_fit %&gt;% extract_fit_parsnip()\n\n# View model summary\nrf_fit_model$fit  # This is the underlying ranger object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, seed = ~rngseed,      num.threads = 1, verbose = FALSE) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      120 \nNumber of independent variables:  8 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       496631.8 \nR squared (OOB):                  0.462951 \n\n# View variable importance (default: impurity)\nrf_fit_model$fit$variable.importance\n\nNULL\n\n\nFirst we will make predictions, calculate the RMSE and produce an observed vs predicted plot for each model, thereafter we will compare the RMSE values between all of the models.\nModel 1 (LM) prediction and RMSE:\n\nlm_preds &lt;- predict(lm_fit, model_data) %&gt;% bind_cols(model_data %&gt;% select(Y))\n\nlm_rmse &lt;- rmse(lm_preds, truth = Y, estimate = .pred)\nlm_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        572.\n\n\nObserved vs Predicted plot for the LM model:\n\nggplot(lm_preds, aes(x = .pred, y = Y)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Linear Model: Observed vs. Predicted\",\n       x = \"Predicted Y\", y = \"Observed Y\")\n\n\n\n\n\n\n\n\nWe have previously plotted and evaluated the LM model but we see a good association of the observed and predicted values in this model.\nModel 2 (LASSO) prediction and RMSE:\n\nlasso_preds &lt;- predict(lasso_fit, model_data) %&gt;% bind_cols(model_data %&gt;% select(Y))\n\nlasso_rmse &lt;- rmse(lasso_preds, truth = Y, estimate = .pred)\nlasso_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        572.\n\n\nWe will plot the LASSO model as observed vs predicted values:\n\nggplot(lasso_preds, aes(x = .pred, y = Y)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"LASSO Model: Observed vs. Predicted\",\n       x = \"Predicted Y\", y = \"Observed Y\")\n\n\n\n\n\n\n\n\nThe plot from the LASSO is almost identical to the LM model. This could be because our penalty is quite small, but also because the linear model showed that many of our points performed well and so there might not be much for the LASSO model to penalise and remove and so the results are similar to the linear model.\nModel 3 (RF) prediction and RMSE:\n\nrf_preds &lt;- predict(rf_fit, model_data) %&gt;% bind_cols(model_data %&gt;% select(Y))\n\nrf_rmse &lt;- rmse(rf_preds, truth = Y, estimate = .pred)\nrf_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        382.\n\n\nObserving the plot for the RF model of observed vs predicted values.\n\nggplot(rf_preds, aes(x = .pred, y = Y)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Random Forest: Observed vs. Predicted\",\n       x = \"Predicted Y\", y = \"Observed Y\")\n\n\n\n\n\n\n\n\nThe RF plot creates an even tighter fit and grouping to the 45 degree line of the observed vs predicted plot.\nGenerating a summary table of the RMSE values for comparison:\n\ntibble(Model = c(\"Linear\", \"LASSO\", \"Random Forest\"), RMSE = c(lm_rmse$.estimate, lasso_rmse$.estimate, rf_rmse$.estimate))\n\n# A tibble: 3 × 2\n  Model          RMSE\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Linear         572.\n2 LASSO          572.\n3 Random Forest  382.\n\n\nThe random forest had the best performance by far, with an RMSE of 381.5. The RMSE of the linear and LASSO mode were nearly identical as mentioned before. This could indicate that there are non-linear relationships that exist between the variables that are missed by the linear model.\n####Tuning the models\nTuning the LASSO model and evaluating the model performance:\nCreating a tuning grid, updating the LASSO model for tuning and creating the tuning workflow\n\n#Ensure outcome is numeric\nmodel_data &lt;- model_data %&gt;% mutate(Y = as.numeric(Y))\n\n#Define LASSO recipe with safety steps\nlasso_recipe &lt;- recipe(Y ~ ., data = model_data) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n#Create a log-scaled penalty grid using dials::penalty()\npenalty_range &lt;- penalty(range = c(-5, 2), trans = log10_trans())  # defines log10 scale\nlasso_grid &lt;- grid_regular(penalty_range, levels = 50)  # creates 50 log-scaled penalty values\n\n# Update the model to tune the penalty\nlasso_tune_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% set_engine(\"glmnet\") %&gt;% set_mode(\"regression\")\n\n#reuse the recipe from before with the dummy variables\nlasso_tune_workflow &lt;- workflow() %&gt;%\n  add_recipe(lasso_recipe) %&gt;%       # uses step_dummy() and step_zv()\n  add_model(lasso_tune_model)\n\n\n#Use all the data (no hold-out) as resampling input\nlasso_tune_res &lt;- tune_grid(\n  object = lasso_tune_workflow,\n  resamples = apparent(model_data),\n  grid = lasso_grid,\n  metrics = metric_set(rmse))\n\n#Manually extract RMSE values from each config\nlasso_results &lt;- map_dfr(lasso_tune_res$.metrics,~ .x, .id = \"config_id\")\n\nlasso_rmse_results_clean &lt;- lasso_results %&gt;% filter(.metric == \"rmse\") %&gt;% rename(rmse = .estimate)\n\n#Arrange and extract best penalty\nbest_lasso_row &lt;- lasso_rmse_results_clean %&gt;% arrange(rmse) %&gt;% slice(1)\n\n#print the results\nprint(best_lasso_row)\n\n# A tibble: 1 × 6\n  config_id penalty .metric .estimator  rmse .config              \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                \n1 1         0.00001 rmse    standard    572. Preprocessor1_Model01\n\n#Plot penalty vs RMSE manually\nggplot(lasso_rmse_results_clean, aes(x = penalty, y = rmse)) +\n  geom_line() +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"LASSO Tuning (Apparent): RMSE by Penalty\",\n    x = \"Penalty (lambda)\", y = \"RMSE\")\n\n\n\n\n\n\n\n\nWe see that the RMSE is improved with lower values of the penalty and a dramatic shift with a reduction in the performance (increased RMSE values) when we reach high penalty (lamda) values around 1e+005 and over 1e+01. The penalty/lamda is applied to values more strictly as this value increases, which means that more values are penalised (set to zero). Thus the model ends up losing more and more values and datapoints which ends up simplifying the model and it loses value which is reflected by the decrease in RMSE. Increasing the penalty can therefore cause the model to underfit because it becomes simpler and the prediction error increases. It cannot drop below the linear model (perform better than the linear model) because the model performs the same as the linear model before penalties are applied and so this is the minimum\nRandom Forest model tuning without CV\n\n# This prevents issues with non-numeric outcome (Y)\nmodel_data &lt;- model_data %&gt;%\n  mutate(Y = as.numeric(Y))\n\nrf_recipe &lt;- recipe(Y ~ ., data = model_data) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%         # Convert categorical vars to dummy vars\n  step_zv(all_predictors()) %&gt;%                    # Remove predictors with zero variance\n  step_naomit(all_predictors(), all_outcomes())    # Drop rows with NA values\n\nrf_tune_model &lt;- rand_forest(mtry = tune(),      # number of predictors at each split\n  min_n = tune(),     # minimum node size\n  trees = 300) %&gt;%         # fixed number of trees\n  set_engine(\"ranger\") %&gt;% set_mode(\"regression\")\n\nrf_tune_workflow &lt;- workflow() %&gt;%\n  add_recipe(rf_recipe) %&gt;%\n  add_model(rf_tune_model)\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7)  # 7 levels for each → 7 x 7 = 49 combinations\n\n\nrf_tune_res &lt;- tune_grid(\n  rf_tune_workflow,\n  resamples = apparent(model_data),\n  grid = rf_grid,\n  metrics = metric_set(rmse))\n\n#Manually extract metrics \nrf_results &lt;- map_dfr(rf_tune_res$.metrics,~ .x, .id = \"config_id\")\n\n#Filter to just RMSE results, and rename .estimate to rmse for clarity\n#Keep only rmse and drop original mtry/min_n before the join\nrf_rmse_results &lt;- rf_results %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  rename(rmse = .estimate) %&gt;%\n  mutate(row_id = row_number()) %&gt;%\n  select(row_id, rmse) %&gt;%\n  left_join(rf_grid %&gt;% mutate(row_id = row_number()), by = \"row_id\") %&gt;%\n  select(-row_id)\n\nPlotting a tuning grid for the RF model\n\n#Visualize RMSE across tuning grid\n#Heatmap of RMSE by mtry and min_n\nggplot(rf_rmse_results, aes(x = factor(mtry), y = factor(min_n), fill = rmse)) +\n  geom_tile() + scale_fill_viridis_c(option = \"plasma\", name = \"RMSE\") +\n  labs(title = \"Random Forest Tuning (Apparent Resampling)\",\n    x = \"mtry (predictors tried at each split)\",\n    y = \"min_n (minimum node size)\") +  theme_minimal()\n\n\n\n\n\n\n\n\nThis is harder to visualise the the LASSO plot. But we see that the RMSE decreases (better performance) with decreasing min_n and increasing mtry.\n####Tuning with CV\nSetting the seed again, as before\n\nset.seed(rngseed)\n\nSetting the CV folds for the models\n\ncv_folds &lt;- vfold_cv(model_data, v = 5, repeats = 5)\n\nRunning our LASSO and RF tuning models but including the CV resampling steps\n\n# Run LASSO tuning using CV resamples\nlasso_cv_res &lt;- tune_grid(\n  lasso_tune_workflow,\n  resamples = cv_folds,\n  grid = lasso_grid,\n  metrics = metric_set(rmse))\n\n\n# Run RF tuning using CV resamples\nrf_cv_res &lt;- tune_grid(\n  rf_tune_workflow,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(rmse))\n\nPlotting the RF and LASSO models with the CV values\n\nautoplot(lasso_cv_res)  \n\n\n\n\n\n\n\nautoplot(rf_cv_res)\n\n\n\n\n\n\n\n\nOur LASSO plot is very similar to before and so I will not expand on this again.\nThe RF plot is different as the RMSE values are much higher than before adding the CV. The RF plot indicates that as the minimal node size increases the rmse value decreases, with the best results seen with a minimal node size of 21. The number of randomly selected parameters also strongly impact the RMSE results, with the strong results seen when we have 4 randomly selected predictors and more.\nSeeing which parameters give us the best rmse values from the models to compare to top rmse values.\n\nlasso_cv_res %&gt;% show_best(metric = \"rmse\", n = 5)\n\n# A tibble: 5 × 7\n    penalty .metric .estimator  mean     n std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00001   rmse    standard    610.    25    21.0 Preprocessor1_Model01\n2 0.0000139 rmse    standard    610.    25    21.0 Preprocessor1_Model02\n3 0.0000193 rmse    standard    610.    25    21.0 Preprocessor1_Model03\n4 0.0000268 rmse    standard    610.    25    21.0 Preprocessor1_Model04\n5 0.0000373 rmse    standard    610.    25    21.0 Preprocessor1_Model05\n\nrf_cv_res %&gt;% show_best(metric = \"rmse\", n = 5)\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     5    21 rmse    standard    671.    25    21.7 Preprocessor1_Model47\n2     6    21 rmse    standard    671.    25    21.1 Preprocessor1_Model48\n3     5    17 rmse    standard    672.    25    21.2 Preprocessor1_Model40\n4     7    17 rmse    standard    672.    25    21.9 Preprocessor1_Model42\n5     7    21 rmse    standard    672.    25    21.5 Preprocessor1_Model49\n\n\nWe see that the best performing LASSO model has an RMSE of around 610 and the best performing RF model has an RMSE of around 670, which is much highler than the initial tests.\nWe see that when including CV the LASSO model performs the best. This is probably demosntrative of overfitting with the RF model in our initial evaluations. This highlights the need for including CV and also for performing test/trains splits with datasets. The LASSO model is simpler and provides more predictable/generalizable performance as seen through the curve, and there seems to be less risk of overfitting. Thus this is likely the best performing and preferred model."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#quarto",
    "href": "ml-models-exercise/ml-models-exercise.html#quarto",
    "title": "ml-models-exercise",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#running-code",
    "href": "ml-models-exercise/ml-models-exercise.html#running-code",
    "title": "ml-models-exercise",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  }
]